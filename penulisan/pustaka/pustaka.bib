@article{abhishekIntegratedHadoopCloud2017,
  title = {Integrated {{Hadoop Cloud Framework}} ({{IHCF}})},
  author = {Abhishek, Kumar and {Department of CSE, NIT Patna, Ashok Rajpath, Mahendru, Patna - 800005, Bihar, India} and Kumar Verma, Manish and {Department of CSE, NIT Patna, Ashok Rajpath, Mahendru, Patna - 800005, Bihar, India} and Shivam, Kumar and {Department of CSE, NIT Patna, Ashok Rajpath, Mahendru, Patna - 800005, Bihar, India} and Kumar, Vinit and {Department of CSE, NIT Patna, Ashok Rajpath, Mahendru, Patna - 800005, Bihar, India} and Mohan, Adarsh and {Department of CSE, NIT Patna, Ashok Rajpath, Mahendru, Patna - 800005, Bihar, India}},
  year = {2017},
  month = feb,
  journal = {Indian Journal of Science and Technology},
  volume = {10},
  number = {10},
  pages = {1--8},
  issn = {0974-5645, 0974-6846},
  doi = {10.17485/ijst/2017/v10i10/107943},
  urldate = {2023-11-10},
  abstract = {Objective: The paper proposes the design of an intermediate Hadoop framework which will not only make Hadoop user friendly but also it is open source and free to use. Methods: The framework has been called Integrated Hadoop Cloud Framework (IHCF) and it also supports various Hadoop based frameworks like Hive, Pig as cloud services. It can be accessed from outside the Hadoop cluster too. Various experiment results has been included which show the efficient working of IHCF. Findings: The IHCF contains modules like Setup, Client and Cloud which are working in sync with one another. The setup module controls automated creation of cluster and client module provides users access to the cluster. The cloud module handles Hadoop based frameworks and ensures that user/client can use frameworks as cloud services. Improvement: The IHCF can be customized further to ensure optimized use of clusters and prevent over/under utilization of resources in cluster.},
  file = {/Users/diwahsap/Zotero/storage/J5Z2BKX4/Abhishek et al. - 2017 - Integrated Hadoop Cloud Framework (IHCF).pdf}
}

@inproceedings{adrianExpertReviewBig2018,
  title = {Expert {{Review}} on {{Big Data Analytics Implementation Model}} in {{Data-driven Decision-Making}}},
  booktitle = {2018 {{Fourth International Conference}} on {{Information Retrieval}} and {{Knowledge Management}} ({{CAMP}})},
  author = {Adrian, Cecilia and Abdullah, Rusli and Atan, Rodziah and Jusoh, Yusmadi Yah},
  year = {2018},
  month = mar,
  pages = {1--5},
  publisher = {IEEE},
  address = {Kota Kinabalu, Malaysia},
  doi = {10.1109/INFRKM.2018.8464770},
  urldate = {2024-01-17},
  isbn = {978-1-5386-3812-5},
  file = {/Users/diwahsap/Downloads/adrian2018.pdf}
}

@article{ahmadvandGapproxUsingGallup2019,
  title = {Gapprox: Using {{Gallup}} Approach for Approximation in {{Big Data}} Processing},
  shorttitle = {Gapprox},
  author = {Ahmadvand, Hossein and Goudarzi, Maziar and Foroutan, Fouzhan},
  year = {2019},
  month = feb,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {20},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0185-4},
  urldate = {2023-09-29},
  abstract = {As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various parts of data to have different impact on the final result. To address this problem, we develop a data variety aware approximation approach called Gapprox. Our idea is to use a kind of cluster sampling to improve the accuracy of estimation. Our approach can decrease the amount of data to be processed to achieve the desired Quality of Result with acceptable error bound and confidence interval. We divide the input data into some blocks considering the intra/inter cluster variance. The size of the block and the sample size are determined in such a way that by processing small amount of input data, an acceptable confidence interval and error bound is achieved. We compared our work with two well-known state of the art. The experimental results show that our result surpasses the state of the art and improve processing time up to 17{\texttimes} compared to ApproxHadoop and 8{\texttimes} compared to Sapprox when the user can tolerate an error of 5\% with 95\% confidence.},
  keywords = {Approximation,Cluster sampling,Data variety,Quality of Result},
  file = {/Users/diwahsap/Zotero/storage/XEUXFUIQ/Ahmadvand et al. - 2019 - Gapprox using Gallup approach for approximation i.pdf;/Users/diwahsap/Zotero/storage/J5W3YNWV/s40537-019-0185-4.html}
}

@article{ahmedComprehensivePerformanceAnalysis2020,
  title = {A Comprehensive Performance Analysis of {{Apache Hadoop}} and {{Apache Spark}} for Large Scale Data Sets Using {{HiBench}}},
  author = {Ahmed, N. and Barczak, Andre L. C. and Susnjak, Teo and Rashid, Mohammed A.},
  year = {2020},
  month = dec,
  journal = {Journal of Big Data},
  volume = {7},
  number = {1},
  pages = {110},
  issn = {2196-1115},
  doi = {10.1186/s40537-020-00388-5},
  urldate = {2023-11-23},
  abstract = {Abstract             Big Data analytics for storing, processing, and analyzing large-scale datasets has become an essential tool for the industry. The advent of distributed computing frameworks such as Hadoop and Spark offers efficient solutions to analyze vast amounts of data. Due to the application programming interface (API) availability and its performance, Spark becomes very popular, even more popular than the MapReduce framework. Both these frameworks have more than 150 parameters, and the combination of these parameters has a massive impact on cluster performance. The default system parameters help the system administrator deploy their system applications without much effort, and they can measure their specific cluster performance with factory-set parameters. However, an open question remains: can new parameter selection improve cluster performance for large datasets? In this regard, this study investigates the most impacting parameters, under resource utilization, input splits, and shuffle, to compare the performance between Hadoop and Spark, using an implemented cluster in our laboratory. We used a trial-and-error approach for tuning these parameters based on a large number of experiments. In order to evaluate the frameworks of comparative analysis, we select two workloads: WordCount and TeraSort. The performance metrics are carried out based on three criteria: execution time, throughput, and speedup. Our experimental results revealed that both system performances heavily depends on input data size and correct parameter selection. The analysis of the results shows that Spark has better performance as compared to Hadoop when data sets are small, achieving up to two times speedup in WordCount workloads and up to 14 times in TeraSort workloads when default parameter values are reconfigured.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/5VMUQMVI/Ahmed et al. - 2020 - A comprehensive performance analysis of Apache Had.pdf}
}

@article{ahnPerformanceStudySpark2018,
  title = {Performance {{Study}} of {{Spark}} on {{YARN Cluster Using HiBench}}},
  author = {Ahn, HooYoung and Kim, Hyunjae and You, Woongshik},
  year = {2018},
  journal = {IEEE International Conference on Consumer Electronics},
  abstract = {Recently, various kinds of Internet-of-Things (IoT) solutions and services are provided such as smart industry, smart city, smart factory, smart agriculture and etc. Those solutions and services generate large amount of data from various devices which are connected through networks while they communicate with each other. However, it is a difficult problem to process the fast and massively produced data efficiently. To solve the problems in the framework level, there are many open-source big data processing and analysis frameworks. To process large-scale data in a fast manner, those frameworks use a cluster consisting of multiple computing machines. However, to set the framework running on large-scale cluster properly is not simple and it is difficult to verify its performance in the distributed environment. In this paper, we evaluate the performance of Apache Spark which is one of the most popular big data processing and analysis frameworks. Especially, we conduct experiments by using a representative benchmark tool, called HiBench, and large-scale data in the cluster environment. From the experimental results, we can conclude that Spark is highly scalable for distributed machine learning as well as big data processing.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/LPBF89UW/Ahn et al. - 2018 - Performance Study of Spark on YARN Cluster Using H.pdf}
}

@inproceedings{almansouriHadoopDistributedFile2019,
  title = {Hadoop {{Distributed File System}} for {{Big}} Data Analysis},
  booktitle = {2019 4th {{World Conference}} on {{Complex Systems}} ({{WCCS}})},
  author = {Almansouri, Hatim Talal and Masmoudi, Youssef},
  year = {2019},
  month = apr,
  pages = {1--5},
  publisher = {IEEE},
  address = {Ouarzazate, Morocco},
  doi = {10.1109/ICoCS.2019.8930804},
  urldate = {2023-11-08},
  isbn = {978-1-72811-232-9},
  file = {/Users/diwahsap/Zotero/storage/WN3AGZUX/Almansouri and Masmoudi - 2019 - Hadoop Distributed File System for Big data analys.pdf}
}

@article{aminudinPengukuranPerformaApache2019,
  title = {{Pengukuran Performa Apache Spark dengan Library H2O Menggunakan Benchmark Hibench Berbasis Cloud Computing}},
  author = {Aminudin, Aminudin and Cahyono, Eko Budi},
  year = {2019},
  month = oct,
  journal = {Jurnal Teknologi Informasi dan Ilmu Komputer},
  volume = {6},
  number = {5},
  pages = {519--526},
  issn = {2528-6579},
  doi = {10.25126/jtiik.2019651520},
  urldate = {2023-09-21},
  abstract = {Apache Spark merupakan platform yang dapat digunakan untuk memproses data dengan ukuran data yang relatif~ besar (big data) dengan kemampuan untuk membagi data tersebut ke masing-masing cluster yang telah ditentukan konsep ini disebut dengan parallel komputing. Apache Spark mempunyai kelebihan dibandingkan dengan framework lain yang serupa misalnya Apache Hadoop dll, di mana Apache Spark mampu memproses data secara streaming artinya data yang masuk ke dalam lingkungan Apache Spark dapat langsung diproses tanpa menunggu data lain terkumpul. Agar di dalam Apache Spark mampu melakukan proses machine learning, maka di dalam paper ini akan dilakukan eksperimen yaitu dengan mengintegrasikan Apache Spark yang bertindak sebagai lingkungan pemrosesan data yang besar dan konsep parallel komputing akan dikombinasikan dengan library H2O yang khusus untuk menangani pemrosesan data menggunakan algoritme machine learning. Berdasarkan hasil pengujian Apache Spark di dalam lingkungan cloud computing, Apache Spark mampu memproses data cuaca yang didapatkan dari arsip data cuaca terbesar yaitu yaitu data NCDC dengan ukuran data sampai dengan 6GB. Data tersebut diproses menggunakan salah satu model machine learning yaitu deep learning dengan membagi beberapa node yang telah terbentuk di lingkungan cloud computing dengan memanfaatkan library H2O. Keberhasilan tersebut dapat dilihat dari parameter pengujian yang telah diujikan meliputi nilai running time, throughput, Avarege Memory dan Average CPU yang didapatkan dari Benchmark Hibench. Semua nilai tersebut ~dipengaruhi oleh banyaknya data dan jumlah node.~AbstractApache Spark is a platform that can be used to process data with relatively large data sizes (big data) with the ability to divide the data into each cluster that has been determined. This concept is called parallel computing. Apache Spark has advantages compared to other similar frameworks such as Apache Hadoop, etc., where Apache Spark is able to process data in streaming, meaning that the data entered into the Apache Spark environment can be directly processed without waiting for other data to be collected. In order for Apache Spark to be able to do machine learning processes, in this paper an experiment will be conducted that integrates Apache Spark which acts as a large data processing environment and the concept of parallel computing will be combined with H2O libraries specifically for handling data processing using machine learning algorithms . Based on the results of testing Apache Spark in a cloud computing environment, Apache Spark is able to process weather data obtained from the largest weather data archive, namely NCDC data with data sizes up to 6GB. The data is processed using one of the machine learning models namely deep learning by dividing several nodes that have been formed in the cloud computing environment by utilizing the H2O library. The success can be seen from the test parameters that have been tested including the value of running time, throughput, Avarege Memory and CPU Average obtained from the Hibench Benchmark. All these values are influenced by the amount of data and number of nodes.},
  copyright = {Hak Cipta (c) 2019 Jurnal Teknologi Informasi dan Ilmu Komputer},
  langid = {indonesian},
  file = {/Users/diwahsap/Zotero/storage/WJ5EJWQA/Aminudin and Cahyono - 2019 - Pengukuran Performa Apache Spark dengan Library H2.pdf}
}

@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2023-11-08},
  howpublished = {https://hadoop.apache.org/},
  file = {/Users/diwahsap/Zotero/storage/ZKR8J5ML/hadoop.apache.org.html}
}

@misc{ApacheHadoopApache,
  title = {Apache {{Hadoop}} 3.3.6 -- {{Apache Hadoop YARN}}},
  urldate = {2023-12-28},
  howpublished = {https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/YARN.html},
  file = {/Users/diwahsap/Zotero/storage/6BC7S95D/YARN.html}
}

@misc{ApacheHadoopHDFS,
  title = {Apache {{Hadoop}} 3.3.6 -- {{HDFS Architecture}}},
  urldate = {2023-11-10},
  howpublished = {https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html},
  file = {/Users/diwahsap/Zotero/storage/WDW46Q5Q/HdfsDesign.html}
}

@misc{ApacheSparkIntroduction,
  title = {Apache {{Spark}} - {{Introduction}}},
  urldate = {2023-12-30},
  abstract = {Apache Spark Introduction - Industries are using Hadoop extensively to analyze their data sets. The reason is that Hadoop framework is based on a simple programming model (MapReduce) and it enables a computing solution that is scalable, flexible, fault-tolerant and cost effective. Here, the main concern is to maintain speed in},
  howpublished = {https://www.tutorialspoint.com/apache\_spark.htm},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/S23BXF2C/apache_spark_introduction.html}
}

@misc{ApacheSparkRDD,
  title = {Apache {{Spark}} - {{RDD}}},
  urldate = {2024-01-15},
  abstract = {Apache Spark RDD - Resilient Distributed Datasets (RDD) is a fundamental data structure of Spark. It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster. RDDs can contain any type of Python, Java, or Scala ob},
  howpublished = {https://www.tutorialspoint.com/apache\_spark/apache\_spark\_rdd.htm},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/8E95KL3B/apache_spark_rdd.html}
}

@misc{ApacheSparkUnified,
  title = {Apache {{Spark}}â„¢ - {{Unified Engine}} for Large-Scale Data Analytics},
  urldate = {2023-11-10},
  howpublished = {https://spark.apache.org/},
  file = {/Users/diwahsap/Zotero/storage/94YYI3F2/spark.apache.org.html}
}

@article{azhirPerformanceEvaluationQuery2022,
  title = {Performance {{Evaluation}} of {{Query Plan Recommendation}} with {{Apache Hadoop}} and {{Apache Spark}}},
  author = {Azhir, Elham and Hosseinzadeh, Mehdi and Khan, Faheem and Mosavi, Amir},
  year = {2022},
  month = jan,
  journal = {Mathematics},
  volume = {10},
  number = {19},
  pages = {3517},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7390},
  doi = {10.3390/math10193517},
  urldate = {2023-11-04},
  abstract = {Access plan recommendation is a query optimization approach that executes new queries using prior created query execution plans (QEPs). The query optimizer divides the query space into clusters in the mentioned method. However, traditional clustering algorithms take a significant amount of execution time for clustering such large datasets. The MapReduce distributed computing model provides efficient solutions for storing and processing vast quantities of data. Apache Spark and Apache Hadoop frameworks are used in the present investigation to cluster different sizes of query datasets in the MapReduce-based access plan recommendation method. The performance evaluation is performed based on execution time. The results of the experiments demonstrated the effectiveness of parallel query clustering in achieving high scalability. Furthermore, Apache Spark achieved better performance than Apache Hadoop, reaching an average speedup of 2x.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {access plan recommendation,Apache Hadoop,Apache Spark,artificial intelligence,big data,cloud computing,data science,MapReduce,parallel processing,soft computing},
  file = {/Users/diwahsap/Zotero/storage/Q5UZ7XLP/Azhir et al. - 2022 - Performance Evaluation of Query Plan Recommendatio.pdf}
}

@misc{baeldungGuideCpuBoundBound2021,
  title = {Guide to the ``{{Cpu-Bound}}'' and ``{{I}}/{{O Bound}}'' {{Terms}} {\textbar} {{Baeldung}} on {{Computer Science}}},
  author = {{baeldung}},
  year = {2021},
  month = dec,
  urldate = {2024-05-21},
  abstract = {Learn about the CPU-bound and I/O bound operations.},
  howpublished = {https://www.baeldung.com/cs/cpu-io-bound},
  langid = {american},
  file = {/Users/diwahsap/Zotero/storage/A7FVS3YU/cpu-io-bound.html}
}

@article{bakratsasHadoopMapReducePerformance2018,
  title = {Hadoop {{MapReduce Performance}} on {{SSDs}} for {{Analyzing Social Networks}}},
  author = {Bakratsas, M. and Basaras, P. and Katsaros, D. and Tassiulas, L.},
  year = {2018},
  month = mar,
  journal = {Big Data Research},
  volume = {11},
  pages = {1--10},
  issn = {22145796},
  doi = {10.1016/j.bdr.2017.06.001},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/LTEKWTGT/Bakratsas et al. - 2018 - Hadoop MapReduce Performance on SSDs for Analyzing.pdf}
}

@book{barosenAnalysisComparisonInterfacing2018,
  title = {Analysis and Comparison of Interfacing, Data Generation and Workload Implementation in {{BigDataBench}} 4.0 and {{Intel HiBench}} 7.0},
  author = {Barosen, Alexander and Dalin, Sadok},
  year = {2018},
  urldate = {2023-12-28},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/GKKJIVTD/Barosen and Dalin - 2018 - Analysis and comparison of interfacing, data gener.pdf}
}

@article{bhattacharyaEvaluatingDistributedComputing2021,
  title = {Evaluating {{Distributed Computing Infrastructures}}: {{An Empirical Study Comparing Hadoop Deployments}} on {{Cloud}} and {{Local Systems}}},
  shorttitle = {Evaluating {{Distributed Computing Infrastructures}}},
  author = {Bhattacharya, Devipsita and Currim, Faiz and Ram, Sudha},
  year = {2021},
  month = jul,
  journal = {IEEE Transactions on Cloud Computing},
  volume = {9},
  number = {3},
  pages = {1075--1088},
  issn = {2168-7161},
  doi = {10.1109/TCC.2019.2902377},
  urldate = {2023-10-13},
  abstract = {The popularity of distributed computing platforms (e.g., Hadoop) is largely due to their ability to address scalability issues that arise from data storage and processing limitations of standard computing systems. However, the decision to dedicate organizational resources and capital for such systems needs a careful consideration of several factors including evaluation of cloud-based distributed computing options. We propose a framework of metrics which we used to conduct an in-depth performance and cost benefit analysis of two standard Hadoop infrastructural choices, i.e., a Platform as a Service (PaaS) on-demand cloud setup and a local organizational setup. We evaluated the framework by means of an exploratory data analysis use-case for a large-scale graph processing research problem. Our analysis considered highly granular aspects of distributed computing performance and studied how utilization rates and infrastructure amortization times affect break-even times. We identified that virtual memory management adversely affects the performance of a cloud cluster during the reduce phase with the magnitude of degradation dependent on the type of MapReduce operation. Our study is intended not only as an evaluation of infrastructural choices but also a development of a metric framework that can serve as a baseline for researchers examining distributed infrastructures.},
  file = {/Users/diwahsap/Zotero/storage/HJ98ZIFQ/Bhattacharya et al. - 2021 - Evaluating Distributed Computing Infrastructures .pdf;/Users/diwahsap/Zotero/storage/HT9ID6T6/8656524.html}
}

@inproceedings{ceesayPlugPlayBench2017,
  title = {Plug and Play Bench: {{Simplifying}} Big Data Benchmarking Using Containers},
  shorttitle = {Plug and Play Bench},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Ceesay, Sheriffo and Barker, Adam and Varghese, Blesson},
  year = {2017},
  month = dec,
  pages = {2821--2828},
  publisher = {IEEE},
  address = {Boston, MA},
  doi = {10.1109/BigData.2017.8258249},
  urldate = {2023-10-27},
  abstract = {The recent boom of big data, coupled with the challenges of its processing and storage gave rise to the development of distributed data processing and storage paradigms like MapReduce, Spark, and NoSQL databases. With the advent of cloud computing, processing and storing such massive datasets on clusters of machines is now feasible with ease. However, there are limited tools and approaches, which users can rely on to gauge and comprehend the performance of their big data applications deployed locally on clusters, or in the cloud. Researchers have started exploring this area by providing benchmarking suites suitable for big data applications. However, many of these tools are fragmented, complex to deploy and manage, and do not provide transparency with respect to the monetary cost of benchmarking an application.},
  isbn = {978-1-5386-2715-0},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/8GQ8E9P2/Ceesay et al. - 2017 - Plug and play bench Simplifying big data benchmar.pdf}
}

@inproceedings{chebbiComparisonBigRemote2018,
  title = {A Comparison of Big Remote Sensing Data Processing with {{Hadoop MapReduce}} and {{Spark}}},
  booktitle = {2018 4th {{International Conference}} on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  author = {Chebbi, I. and Boulila, W. and Mellouli, N. and Lamolle, M. and Farah, I.R.},
  year = {2018},
  month = mar,
  pages = {1--4},
  publisher = {IEEE},
  address = {Sousse},
  doi = {10.1109/ATSIP.2018.8364497},
  urldate = {2023-11-08},
  isbn = {978-1-5386-5239-8},
  file = {/Users/diwahsap/Zotero/storage/DGFLNQLR/Chebbi et al. - 2018 - A comparison of big remote sensing data processing.pdf}
}

@article{cTaskFailureResilience2020,
  title = {Task Failure Resilience Technique for Improving the Performance of {{MapReduce}} in {{Hadoop}}},
  author = {C, Kavitha and X, Anita},
  year = {2020},
  journal = {ETRI Journal},
  volume = {42},
  number = {5},
  pages = {748--760},
  issn = {2233-7326},
  doi = {10.4218/etrij.2018-0265},
  urldate = {2023-11-08},
  abstract = {MapReduce is a framework that can process huge datasets in parallel and distributed computing environments. However, a single machine failure during the runtime of MapReduce tasks can increase completion time by 50\%. MapReduce handles task failures by restarting the failed task and re-computing all input data from scratch, regardless of how much data had already been processed. To solve this issue, we need the computed key-value pairs to persist in a storage system to avoid re-computing them during the restarting process. In this paper, the task failure resilience (TFR) technique is proposed, which allows the execution of a failed task to continue from the point it was interrupted without having to redo all the work. Amazon ElastiCache for Redis is used as a non-volatile cache for the key-value pairs. We measured the performance of TFR by running different Hadoop benchmarking suites. TFR was implemented using the Hadoop software framework, and the experimental results showed significant performance improvements when compared with the performance of the default Hadoop implementation.},
  copyright = {{\copyright} 2020 ETRI},
  langid = {english},
  keywords = {Hadoop,in-memory,key-value pair,MapReduce,recovery,Redis cache,resilience,task failure},
  file = {/Users/diwahsap/Zotero/storage/XD6GCPMV/C and X - 2020 - Task failure resilience technique for improving th.pdf}
}

@inproceedings{dabasAnalysisCommentsYoutube2019,
  title = {Analysis of {{Comments}} on {{Youtube Videos}} Using {{Hadoop}}},
  booktitle = {2019 {{Fifth International Conference}} on {{Image Information Processing}} ({{ICIIP}})},
  author = {Dabas, Chetna and Kaur, Parmeet and Gulati, Nimisha and Tilak, Manan},
  year = {2019},
  month = nov,
  pages = {353--358},
  issn = {2640-074X},
  doi = {10.1109/ICIIP47207.2019.8985907},
  urldate = {2023-11-08},
  abstract = {YouTube is a popular video sharing portal of today's time which also allows users to provide their feedback in form of comments, likes and subscriptions. Analysis of this feedback plays a crucial role in enhancement of video content to suit the need of the time. The comments, in specific, can highly benefit a new user in forming an informed and decisive opinion about a particular video. The work presents a system to perform a detailed analysis and classification of the comments on YouTube videos. The comments corresponding to videos have been ingested into the Hadoop Distributed File System (HDFS) and queried with the Hadoop analytical software, Hive. Further, sentiment analysis of these comments has been performed using Python. The proposed system has been evaluated by executing multiple self-designed queries on the YouTube data. Results of execution times of these queries have been tabulated and presented in the form of graphs. Further, results of sentiment analysis of comments that determine the reaction to the video, which are encouraging.},
  file = {/Users/diwahsap/Zotero/storage/9KY8F2PC/Dabas et al. - 2019 - Analysis of Comments on Youtube Videos using Hadoo.pdf}
}

@inproceedings{deanMapReduceSimplifiedData2004,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  booktitle = {Proceedings of the 6th Conference on {{Symposium}} on {{Operating Systems Design}} \& {{Implementation}} - {{Volume}} 6},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2004},
  month = dec,
  series = {{{OSDI}}'04},
  pages = {10},
  publisher = {USENIX Association},
  address = {USA},
  urldate = {2023-09-28},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782},
  doi = {10.1145/1327452.1327492},
  urldate = {2023-12-30},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the underlying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make efficient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
  file = {/Users/diwahsap/Zotero/storage/UPIMGYT2/Dean and Ghemawat - 2008 - MapReduce simplified data processing on large clu.pdf}
}

@misc{dhamechaMapReduceFoundationBig2018,
  type = {{{SSRN Scholarly Paper}}},
  title = {{{MapReduce Foundation}} of {{Big Data With Hadoop Environment}}},
  author = {Dhamecha, Maulik and Patalia, Dr Tejas},
  year = {2018},
  month = nov,
  number = {3276536},
  address = {Rochester, NY},
  doi = {10.2139/ssrn.3276536},
  urldate = {2023-11-08},
  abstract = {Hadoop Distributed File System that is also known as HDFS and Hadoop MapReduce are good enough to point out about big data's consequences in market. This is very positive hints for data scientists. The opposite hints are that person actually be a digger to be able to find the particular from these results. For more accurate result we have to find more possibilities and produce larger number of pair to find greater accuracy. From our past and because of our great future hand, modules which are open source and also company programmers into the whole market have been created and try that modules to increase the adaptability and use of Hadoop environment.  And that's why for this hadoop environment, the adaptability of this map and reduce functionality is in larger cap or we can say that we can get more accurate result with this map and reduce functionality. In Apache project, many developers ware practicing on signals of the hadoop ecosystem. This constant development in working in sequel of static and incremental leads to handle the hadoop ecosystem ahead in a secure and controlled environment. In this paper we demonstrate all these fundamental criterion with example and prove the basic importance of mapreduce foundation with hadoop environment in big data.},
  langid = {english},
  keywords = {Hadoop,Hadoop Distributed File System (HDFS),Mapreduce},
  file = {/Users/diwahsap/Zotero/storage/K4T3CQIP/Dhamecha and Patalia - 2018 - MapReduce Foundation of Big Data With Hadoop Envir.pdf}
}

@misc{DigitalOcean,
  title = {About {\textbar} {{DigitalOcean}}},
  urldate = {2024-05-07},
  abstract = {Helping millions of developers easily build, test, manage, and scale applications of any size - faster than ever before.},
  howpublished = {https://www.digitalocean.com/about},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/KN9B2A6W/about.html}
}

@article{elkawkagyHighPerformanceHadoop2020,
  title = {High {{Performance Hadoop Distributed File System}}},
  author = {Elkawkagy, Mohamed and Elbeh, Heba},
  year = {2020},
  month = may,
  journal = {International Journal of Networked and Distributed Computing},
  volume = {8},
  number = {3},
  pages = {119--123},
  publisher = {Atlantis Press},
  issn = {2211-7946},
  doi = {10.2991/ijndc.k.200515.007},
  urldate = {2023-11-08},
  abstract = {Although by the end of 2020, most of companies will be running 1000 node Hadoop in the system, the Hadoop implementation is still accompanied by many challenges like security, fault tolerance, flexibility. Hadoop is a software paradigm that handles big data, and it has a distributed file systems so-called Hadoop Distributed File System (HDFS). HDFS...},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/NYZ8L3LZ/Elkawkagy and Elbeh - 2020 - High Performance Hadoop Distributed File System.pdf}
}

@article{enesBigDataOrientedPaaS2018,
  title = {Big {{Data-Oriented PaaS Architecture}} with {{Disk-as-a-Resource Capability}} and {{Container-Based Virtualization}}},
  author = {Enes, Jonatan and Cacheiro, Javier L{\'o}pez and Exp{\'o}sito, Roberto R. and Touri{\~n}o, Juan},
  year = {2018},
  month = dec,
  journal = {Journal of Grid Computing},
  volume = {16},
  number = {4},
  pages = {587--605},
  issn = {1570-7873, 1572-9184},
  doi = {10.1007/s10723-018-9460-4},
  urldate = {2023-09-08},
  langid = {english},
  file = {/Users/dimasws/Downloads/enes2018.pdf}
}

@article{fernandoUtilizationBigData2020,
  title = {Utilization of {{Big Data In E-Commerce Business}}},
  author = {Fernando, Nico and Mery, Mery and Jessica, Jessica and Andry, Johanes},
  year = {2020},
  month = nov,
  journal = {Conference Series},
  volume = {3},
  pages = {62--67},
  doi = {10.34306/conferenceseries.v3i1.383},
  abstract = {In this day and age with the internet brings revolution to the way all fields work, especially in the field of business. With the internet technology data became big and known as "Big Data". The development of big data has increased significantly so that it can be utilized in various fields, especially in business areas that have been implemented with internet technology. This electronic buying and selling media have a wide range such as from small to large stores that can utilize media or with a site. This makes users always need this technology, so an E-commerce can also be said to be the largest data-producing media. This study addresses the problem of the extent to which big data generated from E-commerce can affect business and provide benefits for business organizations such as expanding the scope of transactions, supporting decision-making, and others. The research method used in compiling this research is to collect data and information and then conduct processing and analysis of the data. So it is expected from the results of utilization of big data in this E-commerce business that has been processed can provide support especially in making decisions with cluster results that have been obtained for example such as to know the most sales patterns in order to be able to add stock to certain goods and determine promotions based on future sales. The study concluded that based on gift shop sales data, the average item purchased by the store's customers is found in items that are included in cluster group 0 so that the gift shop can increase the stock of items contained in cluster group 0.},
  file = {/Users/diwahsap/Zotero/storage/NIBECCLK/Fernando et al. - 2020 - Utilization of Big Data In E-Commerce Business.pdf}
}

@incollection{furhtIntroductionBigData2016,
  title = {Introduction to {{Big Data}}},
  author = {Furht, Borko and Villanustre, Flavio},
  year = {2016},
  month = sep,
  pages = {3--11},
  doi = {10.1007/978-3-319-44550-2_1},
  abstract = {In this chapter we present the basic terms and concepts in Big Data computing. Big data is a large and complex collection of data sets, which is difficult to process using on-hand database management tools and traditional data processing applications. Big Data topics include the following activities:},
  isbn = {978-3-319-44548-9},
  file = {/Users/diwahsap/Downloads/furht2016.pdf}
}

@article{gandomiHybSMRPHybridScheduling2019,
  title = {{{HybSMRP}}: A Hybrid Scheduling Algorithm in {{Hadoop MapReduce}} Framework},
  shorttitle = {{{HybSMRP}}},
  author = {Gandomi, Abolfazl and Reshadi, Midia and Movaghar, Ali and Khademzadeh, Ahmad},
  year = {2019},
  month = nov,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {106},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0253-9},
  urldate = {2023-11-08},
  abstract = {Due to the advent of new technologies, devices, and communication tools such as social networking sites, the amount of data produced by mankind is growing rapidly every year. Big data is a collection of large datasets that cannot be processed using traditional computing techniques. MapReduce has been introduced to solve large-data computational problems. It is specifically designed to run on commodity hardware, and it depends on dividing and conquering principles. Nowadays, the focus of researchers has shifted towards Hadoop MapReduce. One of the most outstanding characteristics of MapReduce is data locality-aware scheduling. Data locality-aware scheduler is a further efficient solution to optimize one or a set of performance metrics such as data locality, energy consumption and job completion time. Similar to all situations, time and scheduling are the most important aspects of the MapReduce framework. Therefore, many scheduling algorithms have been proposed in the past decades. The main ideas of these algorithms are increasing data locality rate and decreasing the response and completion time. In this paper, a new hybrid scheduling algorithm has been proposed, which uses dynamic priority and localization ID techniques and focuses on increasing data locality rate and decreasing completion time. The proposed algorithm was evaluated and compared with Hadoop default schedulers (FIFO, Fair), by running concurrent workloads consisting of Wordcount and Terasort benchmarks. The experimental results show that the proposed algorithm is faster than FIFO and Fair scheduling, achieves higher data locality rate and avoids wasting resources.},
  keywords = {Data Locality,Dynamic priority,Hybrid algorithm,MapReduce,Scheduling},
  file = {/Users/diwahsap/Zotero/storage/V4NKM67T/Gandomi et al. - 2019 - HybSMRP a hybrid scheduling algorithm in Hadoop M.pdf}
}

@article{gaoBigDataBenchBigData2013,
  title = {{{BigDataBench}}: A {{Big Data Benchmark Suite}} from {{Web Search Engines}}},
  shorttitle = {{{BigDataBench}}},
  author = {Gao, Wanling and Zhu, Yuqing and Jia, Zhen and Luo, Chunjie and Wang, Lei and Li, Zhiguo and Zhan, Jianfeng and Qi, Yong and He, Yongqiang and Gong, Shiming and Li, Xiaona and Zhang, Shujie and Qiu, Bizhu},
  year = {2013},
  month = jul,
  abstract = {This paper presents our joint research efforts on big data benchmarking with several industrial partners. Considering the complexity, diversity, workload churns, and rapid evolution of big data systems, we take an incremental approach in big data benchmarking. For the first step, we pay attention to search engines, which are the most important domain in Internet services in terms of the number of page views and daily visitors. However, search engine service providers treat data, applications, and web access logs as business confidentiality, which prevents us from building benchmarks. To overcome those difficulties, with several industry partners, we widely investigated the open source solutions in search engines, and obtained the permission of using anonymous Web access logs. Moreover, with two years' great efforts, we created a sematic search engine named ProfSearch (available from http://prof.ict.ac.cn). These efforts pave the path for our big data benchmark suite from search engines---BigDataBench, which is released on the web page (http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search engine workloads, and present our benchmarking methodology. An innovative data generation methodology and tool are proposed to generate scalable volumes of big data from a small seed of real data, preserving semantics and locality of data. Also, we preliminarily report two case studies using BigDataBench for both system and architecture researches.},
  file = {/Users/diwahsap/Zotero/storage/765S9IN3/Gao et al. - 2013 - BigDataBench a Big Data Benchmark Suite from Web .pdf}
}

@inproceedings{gohilPerformanceAnalysisMapReduce2014,
  title = {A Performance Analysis of {{MapReduce}} Applications on Big Data in Cloud Based {{Hadoop}}},
  booktitle = {International {{Conference}} on {{Information Communication}} and {{Embedded Systems}} ({{ICICES2014}})},
  author = {Gohil, Parth and Garg, Dweepna and Panchal, Bakul},
  year = {2014},
  month = feb,
  pages = {1--6},
  doi = {10.1109/ICICES.2014.7033791},
  urldate = {2024-05-07},
  abstract = {MapReduce is one of the most popular programming model for big data analysis in Distributed and Parallel Computing Environment. It is used for implementing parallel applications. With the growing development of mobile Internet and cloud computing, the issues related to big data have been a matter of concern in both industry and academy. There are several platforms for users to develop their applications based on MapReduce framework such as Hadoop. Hadoop is a free, Java-based programming framework that supports the processing of large data sets in a distributed computing environment. This paper discusses various MapReduce applications like Wordcount, Pi, TeraSort, Grep in Cloud based Hadoop. We have shown experimental results of these applications on Amazon EC2 using two types of Ubuntu instances. In this paper, performance of above application has been shown with respect to execution time and number of nodes. We find in our research study that as the number of nodes increases the execution time decreases and performance increases.},
  keywords = {Amazon EC2,Big data,Big Data,Cloud computing,Cloud Computing,Educational institutions,File systems,Hadoop,Hardware,HDFS,MapReduce,Programming,Servers},
  file = {/Users/diwahsap/Zotero/storage/T9GDXVJ8/Gohil et al. - 2014 - A performance analysis of MapReduce applications o.pdf;/Users/diwahsap/Zotero/storage/ZY99B45W/7033791.html}
}

@article{gopalaniComparingApacheSpark2015,
  title = {Comparing {{Apache Spark}} and {{Map Reduce}} with {{Performance Analysis}} Using {{K-Means}}},
  author = {Gopalani, Satish and Arora, Rohan},
  year = {2015},
  month = mar,
  journal = {International Journal of Computer Applications},
  volume = {113},
  number = {1},
  pages = {8--11},
  issn = {09758887},
  doi = {10.5120/19788-0531},
  urldate = {2023-10-13},
  abstract = {Data has long been the topic of fascination for Computer Science enthusiasts around the world, and has gained even more prominence in the recent times with the continuous explosion of data resulting from the likes of social media and the quest for tech giants to gain access to deeper analysis of their data. This paper discusses two of the comparison of - Hadoop Map Reduce and the recently introduced Apache Spark - both of which provide a processing model for analyzing big data. Although both of these options are based on the concept of Big Data, their performance varies significantly based on the use case under implementation. This is what makes these two options worthy of analysis with respect to their variability and variety in the dynamic field of Big Data. In this paper we compare these two frameworks along with providing the performance analysis using a standard machine learning algorithm for clustering (K- Means).},
  file = {/Users/diwahsap/Zotero/storage/PVDDSTVN/Gopalani and Arora - 2015 - Comparing Apache Spark and Map Reduce with Perform.pdf}
}

@article{guoScientificBigData2014,
  title = {Scientific Big Data and {{Digital Earth}}},
  author = {Guo, Huadong and Wang, Lizhe and Chen, Fang and Liang, Dong},
  year = {2014},
  month = dec,
  journal = {Chinese Science Bulletin (Chinese Version)},
  volume = {59},
  pages = {1047},
  doi = {10.1360/972013-1054},
  abstract = {Big data has been a focus of research in science, technology, economics, and social studies. Many countries have already incorporated big data research into their national strategies. This paper elaborates upon the origin, connotation, and development of big data from both a spatial and temporal perspective. It proposes that scientific big data will become a new solution in scientific research as the paradigm changes from being model-driven to data-driven. This paper defines the concept of "scientific big data'' and proposes strategies for solving "big data problems''. Theoretical frameworks and data systems for Digital Earth are discussed with a clear conclusion that scientific big data is a prominent feature of Digital Earth. As an example, spatial cognition of the formation mechanism of China's Heihe-Tengchong Line-a geo-demographic demarcation line dividing China into two parts-is discussed within the context of big data computation and analysis for Digital Earth.}
}

@misc{HadoopMarketSize,
  title = {Hadoop {{Market Size}}, {{Share}} \& {{Competition Analysis}}, 2021-2027},
  journal = {KBV Research},
  urldate = {2024-05-15},
  abstract = {The Global Hadoop Market size is expected to reach \$341.4 billion by 2027, rising at a market growth of 38.2\% CAGR during the forecast period. Hadoop is a},
  howpublished = {https://www.kbvresearch.com/hadoop-market/},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/ZLCUZBWD/hadoop-market.html}
}

@misc{herodotouHadoopPerformanceModels2011,
  title = {Hadoop {{Performance Models}}},
  author = {Herodotou, Herodotos},
  year = {2011},
  month = jun,
  number = {arXiv:1106.0940},
  eprint = {1106.0940},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1106.0940},
  urldate = {2023-11-08},
  abstract = {Hadoop MapReduce is now a popular choice for performing large-scale data analytics. This technical report describes a detailed set of mathematical performance models for describing the execution of a MapReduce job on Hadoop. The models describe dataflow and cost information at the fine granularity of phases within the map and reduce tasks of a job execution. The models can be used to estimate the performance of MapReduce jobs as well as to find the optimal configuration settings to use when running the jobs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/Users/diwahsap/Zotero/storage/3LCQ7U89/Herodotou - 2011 - Hadoop Performance Models.pdf;/Users/diwahsap/Zotero/storage/N924Q9MA/1106.html}
}

@article{huangHiBenchBenchmarkSuitea,
  title = {The {{HiBench Benchmark Suite}}: {{Characterization}} of the {{MapReduce-Based Data Analysis}}},
  author = {Huang, Shengsheng and Huang, Jie and Dai, Jinquan and Xie, Tao and Huang, Bo},
  abstract = {The MapReduce model is becoming prominent for the large-scale data analysis in the cloud. In this paper, we present the benchmarking, evaluation and characterization of Hadoop, an open-source implementation of MapReduce. We first introduce HiBench, a new benchmark suite for Hadoop. It consists of a set of Hadoop programs, including both synthetic micro-benchmarks and real-world Hadoop applications. We then evaluate and characterize the Hadoop framework using HiBench, in terms of speed (i.e., job running time), throughput (i.e., the number of tasks completed per minute), HDFS bandwidth, system resource (e.g., CPU, memory and I/O) utilizations, and data access patterns.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/PVZNPQJD/Huang et al. - The HiBench Benchmark Suite Characterization of t.pdf}
}

@misc{IntelbigdataHiBench2023,
  title = {Intel-Bigdata/{{HiBench}}},
  year = {2023},
  month = dec,
  urldate = {2023-12-30},
  abstract = {HiBench is a big data benchmark suite.},
  howpublished = {Intel-bigdata}
}

@misc{IntroductoryStatisticsScienceDirect,
  title = {Introductory {{Statistics}} {\textbar} {{ScienceDirect}}},
  urldate = {2024-05-22},
  howpublished = {https://www.sciencedirect.com/book/9780128043172/introductory-statistics}
}

@book{johnDataLakeEnterprises2017,
  title = {Data {{Lake}} for Enterprises: Leveraging {{Lambda}} Architecture for Building {{Enterprise Data Lake}}},
  shorttitle = {Data {{Lake}} for Enterprises},
  author = {John, Tomcy and Misra, Pankaj},
  year = {2017},
  publisher = {Packt Publishing},
  address = {Birmingham, UK, Mumbai},
  isbn = {978-1-78728-134-9},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/I3UYPSIA/John and Misra - 2017 - Data Lake for enterprises leveraging Lambda archi.pdf}
}

@article{jonesWhatWeTalk2019,
  title = {What We Talk about When We Talk about (Big) Data},
  author = {Jones, Matthew},
  year = {2019},
  month = mar,
  journal = {The Journal of Strategic Information Systems},
  volume = {28},
  number = {1},
  pages = {3--16},
  issn = {0963-8687},
  doi = {10.1016/j.jsis.2018.10.005},
  urldate = {2023-12-28},
  abstract = {In common with much contemporary discourse around big data, recent discussion of datafication in the Journal of Strategic Information Systems has focused on its effects on individuals, organisations and society. Generally missing from such analysis, however, is any consideration of data themselves. What is it that is having these effects? In this Viewpoint article I therefore present a critical analysis of a number of widely-held assumptions about data in general and big data in particular. Rather than being a referential, natural, foundational, objective and equal representation of the world, it will be argued, data are partial and contingent and are brought into being through situated practices of conceptualization, recording and use. Big data are also not as revolutionary voluminous, universal or exhaustive as they are often presented. Some initial implications of this reconceptualization of data are explored. A distinction is made between ``data in principle'' as they are recorded, and the ``data in practice'' as they are used. It is only the latter, typically a small and not necessarily representative subset of the former, that will contribute directly to the effects of datafication.},
  file = {/Users/diwahsap/Zotero/storage/YNS54JPD/S0963868718302622.html}
}

@article{kaliaAnalysisHadoopMapReduce2021,
  title = {Analysis of Hadoop {{MapReduce}} Scheduling in Heterogeneous Environment},
  author = {Kalia, Khushboo and Gupta, Neeraj},
  year = {2021},
  month = mar,
  journal = {Ain Shams Engineering Journal},
  volume = {12},
  number = {1},
  pages = {1101--1110},
  issn = {20904479},
  doi = {10.1016/j.asej.2020.06.009},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/MFT5BUGG/Kalia and Gupta - 2021 - Analysis of hadoop MapReduce scheduling in heterog.pdf}
}

@incollection{khataiImplementationTextMining2021,
  title = {An {{Implementation}} of {{Text Mining Decision Feedback Model Using Hadoop MapReduce}}},
  booktitle = {Trends of {{Data Science}} and {{Applications}}: {{Theory}} and {{Practices}}},
  author = {Khatai, Swagat and Rautaray, Siddharth Swarup and Sahoo, Swetaleena and Pandey, Manjusha},
  editor = {Rautaray, Siddharth Swarup and Pemmaraju, Phani and Mohanty, Hrushikesha},
  year = {2021},
  series = {Studies in {{Computational Intelligence}}},
  pages = {273--306},
  publisher = {Springer},
  address = {Singapore},
  doi = {10.1007/978-981-33-6815-6_14},
  urldate = {2023-11-09},
  abstract = {A very large amount of unstructured text data is generated everyday on the Internet as well as in real life. Text mining has dramatically lifted the commercial value of these data by pulling out the unknown comprehensive potential patterns from these data. Text mining uses the algorithms of data mining, statistics, machine learning, and natural language processing for hidden knowledge discovery from the unstructured text data. This paper hosts the extensive research done on text mining in recent years. Then, the overall process of text mining is discussed with some high-end applications. The entire process is classified into different modules which are test parsing, text filtering, transformation, clustering, and predictive analytics. A more efficient and more sophisticated text mining model is also proposed with a decision feedback perception in which it is a way advanced than the conventional models providing a better accuracy and attending broader objectives. The text filtering module is discussed in detail with the implementation of word stemming algorithms like Lovins stemmer and Porter stemmer using MapReduce. The implementation set up has been done on a single node Hadoop cluster operating in pseudo-distributed mode. An enhanced implementation technique has been also proposed which is Porter stemmer with partitioner (PSP). Then, a comparative analysis using MapReduce has been done considering above three algorithms where the PSP provides a better stemming performance than Lovins stemmer and Porter stemmer. Experimental result shows that PSP provides 20--25\% more stemming capacity than Lovins stemmer and 3--15\% more stemming capacity then Porter stemmer algorithm.},
  isbn = {978-981-336-815-6},
  langid = {english},
  keywords = {Big data,Decision feedback,Hadoop,MapReduce,PSP,Word stemming}
}

@article{kianDetectionFraudBanking2021,
  title = {Detection of Fraud in Banking Transactions Using Big Data Clustering Technique {{Customer Behavior Indicators}}},
  author = {Kian, Ramez and Obaid, Hadeel S},
  year = {2021},
  month = nov,
  journal = {Journal of Applied Research on Industrial Engineering},
  number = {Online First},
  doi = {10.22105/jarie.2021.307635.1387},
  urldate = {2024-01-18},
  abstract = {Human life today is intertwined with abundant trade and economic exchanges, and life would not be possible without trade and commerce. One of the main pillars of financial exchanges are banks and financial and credit institutions, which, as the vital arteries of the economy, are responsible for transferring funds and keeping the economy alive. In the world of economic competition between organizations, profitability and proper performance for stakeholders are the basic principles of the organization's survival. To increase profitability, banks must take measures that, in addition to reducing costs, increase the level of service and customer satisfaction. The best way to do this is to use new technologies and orient the bank's policies to provide services in person and independent of time and place. The use of new technologies in the banking system sometimes leads to customers' distrust and distrust of the bank. Therefore, solutions to detect fraud in banking transactions should be provided. This article aims to discover a model for face-to-face transactions and to establish a system to block fraudulently issued transactions. Therefore, a big data clustering method is designed to timely identify bribery in banking transactions. The results show that using the big data clustering method in the fastest time can detect and stop possible fraud in customers' banking transactions.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/WXZE8VRV/Kian and Obaid - 2021 - Detection of fraud in banking transactions using b.pdf}
}

@article{kimDesignImplementationCloud2022,
  title = {Design and {{Implementation}} of {{Cloud Docker Application Architecture Based}} on {{Machine Learning}} in {{Container Management}} for {{Smart Manufacturing}}},
  author = {Kim, Byoung Soo and Lee, Sang Hyeop and Lee, Ye Rim and Park, Yong Hyun and Jeong, Jongpil},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {13},
  pages = {6737},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app12136737},
  urldate = {2023-09-08},
  abstract = {Manufacturers are expanding their business-process innovation and customized manufacturing to reduce their information technology costs and increase their operational efficiency. Large companies are building enterprise-wide hybrid cloud platforms to further accelerate their digital transformation. Many companies are also introducing container virtualization technology to maximize their cloud transition and cloud benefits. However, small- and mid-sized manufacturers are struggling with their digital transformation owing to technological barriers. Herein, for small- and medium-sized manufacturing enterprises transitioning onto the cloud, we introduce a Docker Container application architecture, a customized container-based defect inspection machine-learning model for the AWS cloud environment developed for use in small manufacturing plants. By linking with open-source software, the development was improved and a datadog-based container monitoring system, built to enable real-time anomaly detection, was implemented.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cloud docker,container management,docker container,machine learning,monitoring,smart manufacturing},
  file = {/Users/diwahsap/Zotero/storage/3U7ES6WF/Kim et al. - 2022 - Design and Implementation of Cloud Docker Applicat.pdf}
}

@article{kozlovRealtimeDataStream,
  title = {Real-Time {{Data Stream Processing System}}},
  author = {Kozlov, Vitalij},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/KUFJH34Q/Kozlov - Real-time Data Stream Processing System.pdf}
}

@inproceedings{linLargescaleDataSet2020,
  title = {A {{Large-scale Data Set}} and an {{Empirical Study}} of {{Docker Images Hosted}} on {{Docker Hub}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Lin, Changyuan and Nadi, Sarah and Khazaei, Hamzeh},
  year = {2020},
  month = sep,
  pages = {371--381},
  publisher = {IEEE},
  address = {Adelaide, SA,  Australia},
  doi = {10.1109/ICSME46990.2020.00043},
  urldate = {2023-09-13},
  abstract = {Docker is currently one of the most popular containerization solutions. Previous work investigated various characteristics of the Docker ecosystem, but has mainly focused on Dockerfiles from GitHub, limiting the type of questions that can be asked, and did not investigate evolution aspects. In this paper, we create a recent and more comprehensive data set by collecting data from Docker Hub, GitHub, and Bitbucket. Our data set contains information about 3,364,529 Docker images and 378,615 git repositories behind them. Using this data set, we conduct a large-scale empirical study with four research questions where we reproduce previously explored characteristics (e.g., popular languages and base images), investigate new characteristics such as image tagging practices, and study evolution trends. Our results demonstrate the maturity of the Docker ecosystem: we find more reliance on ready-to-use language and application base images as opposed to yet-to-be-configured OS images, a downward trend of Docker image sizes demonstrating the adoption of best practices of keeping images small, and a declining trend in the number of smells in Dockerfiles suggesting a general improvement in quality. On the downside, we find an upward trend in using obsolete OS base images, posing security risks, and find problematic usages of the latest tag, including version lagging. Overall, our results bring good news such as more developers following best practices, but they also indicate the need to build tools and infrastructure embracing new trends and addressing potential issues.},
  isbn = {978-1-72815-619-4},
  langid = {english},
  file = {/Users/dimasws/Downloads/lin2020.pdf}
}

@inproceedings{maneasEvolutionHadoopDistributed2018,
  title = {The {{Evolution}} of the {{Hadoop Distributed File System}}},
  booktitle = {2018 32nd {{International Conference}} on {{Advanced Information Networking}} and {{Applications Workshops}} ({{WAINA}})},
  author = {Maneas, Stathis and Schroeder, Bianca},
  year = {2018},
  month = may,
  pages = {67--74},
  doi = {10.1109/WAINA.2018.00065},
  urldate = {2023-11-08},
  abstract = {Frameworks for large-scale distributed data processing, such as the Hadoop ecosystem, are at the core of the big data revolution we have experienced over the last decade. In this paper, we conduct an extensive study of the Hadoop Distributed File System (HDFS)'s code evolution. Our study is based on the reports and patch files (patches) available from the official Apache issue tracker (JIRA) and our goal was to make complete use of the entire history of HDFS at the time and the richness of the available data. The purpose of our study is to assist developers in improving the design of similar systems and implementing more solid systems in general. In contrast to prior work, our study covers all reports that have been submitted over HDFS's lifetime, rather than a sampled subset. Additionally, we include all associated patch files that have been verified by the developers of the system and classify the root causes of issues at a finer granularity than prior work, by manually inspecting all 3302 reports over the first nine years, based on a two-level classification scheme that we developed. This allows us to present a different perspective of HDFS, including a focus on the system's evolution over time, as well as a detailed analysis of characteristics that have not been previously studied in detail. These include, for example, the scope and complexity of issues in terms of the size of the patch that fixes it and number of files it affects, the time it takes before an issue is exposed, the time it takes to resolve an issue and how these vary over time. Our results indicate that bug reports constitute the most dominant type, having a continuously increasing rate over time. Moreover, the overall scope and complexity of reports and patch files remain surprisingly stable throughout HDFS' lifetime, despite the significant growth the code base experiences over time. Finally, as part of our work, we created a detailed database that includes all reports and patches, along with the key characteristics we extracted.},
  file = {/Users/diwahsap/Zotero/storage/GYPNSFB3/Maneas and Schroeder - 2018 - The Evolution of the Hadoop Distributed File Syste.pdf;/Users/diwahsap/Zotero/storage/RCADNN6Z/8418050.html}
}

@misc{MapReduceDistributedComputing,
  title = {{{MapReduce}} - {{Distributed Computing}} in {{Java}} 9 [{{Book}}]},
  urldate = {2023-11-08},
  abstract = {MapReduce MapReduce is a programming pattern used by Apache Hadoop. Hadoop MapReduce works in providing the systems that can store, process, and mine huge data with parallel multi node clusters {\dots} - Selection from Distributed Computing in Java 9 [Book]},
  howpublished = {https://www.oreilly.com/library/view/distributed-computing-in/9781787126992/5fef6ce5-20d7-4d7c-93eb-7e669d48c2b4.xhtml},
  isbn = {9781787126992},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/25BHVL4L/5fef6ce5-20d7-4d7c-93eb-7e669d48c2b4.html}
}

@misc{MapReduceTutorial,
  title = {{{MapReduce Tutorial}}},
  urldate = {2023-11-08},
  howpublished = {https://hadoop.apache.org/docs/r1.2.1/mapred\_tutorial.html},
  file = {/Users/diwahsap/Zotero/storage/WKBYLL97/mapred_tutorial.html}
}

@article{mavridisLogFileAnalysis,
  title = {Log {{File Analysis}} in {{Cloud}} with {{Apache Hadoop}} and {{Apache Spark}}},
  author = {Mavridis, Ilias and Karatza, Eleni},
  urldate = {2023-11-23},
  file = {/Users/diwahsap/Zotero/storage/JZ8AQD24/30277103.html}
}

@article{meenaReducedTimeCompression2019,
  title = {Reduced {{Time Compression}} in {{Big Data Using MapReduce Approach}} and {{Hadoop}}},
  author = {Meena, K. and Sujatha, J.},
  year = {2019},
  month = jun,
  journal = {Journal of Medical Systems},
  volume = {43},
  number = {8},
  pages = {239},
  issn = {1573-689X},
  doi = {10.1007/s10916-019-1369-3},
  urldate = {2023-11-08},
  abstract = {An exponential rise has been observed in the data volume over the time when considering a real time environment. A phenomenal feature termed as `Predictability' helps in predicting and portraying related data to the user according to their needs. Moreover, classification of Big Data is usually a tedious and lengthy task. The technique of MapReduce Framework performs the data processing that being paralleled by data distribution in small chunks through the clusters. This Map Reduce technique is being proposed which is employed to process heterogeneous data items. Few issues that are being targeted in the existing paper include associating climatologically and meteorological information with large variety of farming decisions. Using the well-known MapReduce framework the above issues and challenges can be resolved. The existing paper proposes empirical techniques of climate classification and prediction by adopting Co-EANFS (Co-Effective and Adaptive Neuro-Fuzzy System) approach for data handling. Furthermore, the paper examines association rule mining too, which is being implemented for examining the best crop production by relying upon the soil and weather condition. Lastly, a technique is proposed for managing various levels such as preprocessing, clustering, classification and prediction. First, the weather dataset is being collected which undergoes processing; thereafter the proposed model is implemented which results in formation of cluster data sets linked to each season. For evaluating the performance, accuracy predictions generated by Co-EANFS is used which being formulated with varying no: of inputs and variables. The proposed framework acquires least execution time.},
  langid = {english},
  keywords = {Association rule mining,Big data,Classification,Clustering,Co-effective and adaptive neuro-fuzzy system (Co-EANFS),Hadoop,MapReduce,Prediction}
}

@inproceedings{moravcikOverviewDockerContainer2020,
  title = {Overview of {{Docker}} Container Orchestration Tools},
  booktitle = {2020 18th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  author = {Moravcik, Marek and Kontsek, Martin},
  year = {2020},
  month = nov,
  pages = {475--480},
  publisher = {IEEE},
  address = {Ko{\v s}ice, Slovenia},
  doi = {10.1109/ICETA51985.2020.9379236},
  urldate = {2023-09-13},
  abstract = {The main goal of this paper is to analyze current options of Docker container orchestration. Currently, there are three widely used orchestrators - Docker Swarm, Kubernetes and OpenShift. In the paper, we analyzed all three of them, we highlighted their advantages and disadvantages and compared them.},
  isbn = {978-1-66542-226-0},
  langid = {english},
  file = {/Users/dimasws/Downloads/moravcik2020.pdf}
}

@article{mousaviScalableStreamProcessing,
  title = {Scalable {{Stream Processing}} and {{Management}} for {{Time Series Data}}},
  author = {Mousavi, Bamdad},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/6CHBA5HA/Mousavi - Scalable Stream Processing and Management for Time.pdf}
}

@article{muthiahPerformanceEvaluationHadoop,
  title = {Performance {{Evaluation}} of {{Hadoop}} Based {{Big Data Applications}} with {{HiBench Benchmarking}} Tool on {{IaaS Cloud Platforms}}},
  author = {Muthiah, Karthika},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/XAKHHG4D/Muthiah - Performance Evaluation of Hadoop based Big Data Ap.pdf}
}

@inproceedings{naikDockerContainerbasedBig2017,
  title = {Docker Container-Based Big Data Processing System in Multiple Clouds for Everyone},
  booktitle = {2017 {{IEEE International Systems Engineering Symposium}} ({{ISSE}})},
  author = {Naik, Nitin},
  year = {2017},
  month = oct,
  pages = {1--7},
  publisher = {IEEE},
  address = {Vienna, Austria},
  doi = {10.1109/SysEng.2017.8088294},
  urldate = {2023-09-08},
  abstract = {Big data processing is progressively becoming essential for everyone to extract the meaningful information from their large volume of data irrespective of types of users and their application areas. Big data processing is a broad term and includes several operations such as the storage, cleaning, organization, modelling, analysis and presentation of data at a scale and efficiency. For ordinary users, the significant challenges are the requirement of the powerful data processing system and its provisioning, installation of complex big data analytics and difficulty in their usage. Docker is a container-based virtualization technology and it has recently introduced Docker Swarm for the development of various types of multi-cloud distributed systems, which can be helpful in solving all above problems for ordinary users. However, Docker is predominantly used in the software development industry, and less focus is given to the data processing aspect of this container-based technology. Therefore, this paper proposes the Docker container-based big data processing system in multiple clouds for everyone, which explores another potential dimension of Docker for big data analysis. This Docker container-based system is an inexpensive and user-friendly framework for everyone who has the knowledge of basic IT skills. Additionally, it can be easily developed on a single machine, multiple machines or multiple clouds. This paper demonstrates the architectural design and simulated development of the proposed Docker container-based big data processing system in multiple clouds. Subsequently, it illustrates the automated provisioning of big data clusters using two popular big data analytics, Hadoop and Pachyderm (without Hadoop) including the Web-based GUI interface Hue for easy data processing in Hadoop.},
  isbn = {978-1-5386-3403-5},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/6XLSEFN6/Naik - 2017 - Docker container-based big data processing system .pdf}
}

@book{newhamLearningBashShell2005,
  title = {Learning the Bash Shell: {{Unix}} Shell Programming},
  shorttitle = {Learning the Bash Shell},
  author = {Newham, Cameron and Rosenblatt, Bill and Rosenblatt, Bill},
  year = {2005},
  series = {{{UNIX Shell}} Programming},
  edition = {3. ed},
  publisher = {O'Reilly},
  address = {Beijing K{\"o}ln},
  isbn = {978-0-596-00965-6},
  langid = {english}
}

@misc{OpenStaxFreeTextbooks,
  title = {{{OpenStax}} {\textbar} {{Free Textbooks Online}} with {{No Catch}}},
  urldate = {2024-05-22},
  abstract = {OpenStax offers free college textbooks for all types of students, making education accessible \& affordable for everyone. Browse our list of available subjects!},
  howpublished = {https://openstax.org/},
  langid = {american}
}

@article{oussousBigDataTechnologies2018,
  title = {Big {{Data}} Technologies: {{A}} Survey},
  shorttitle = {Big {{Data}} Technologies},
  author = {Oussous, Ahmed and Benjelloun, Fatima-Zahra and Ait Lahcen, Ayoub and Belfkih, Samir},
  year = {2018},
  month = oct,
  journal = {Journal of King Saud University - Computer and Information Sciences},
  volume = {30},
  number = {4},
  pages = {431--448},
  issn = {1319-1578},
  doi = {10.1016/j.jksuci.2017.06.001},
  urldate = {2023-12-28},
  abstract = {Developing Big Data applications has become increasingly important in the last few years. In fact, several organizations from different sectors depend increasingly on knowledge extracted from huge volumes of data. However, in Big Data context, traditional data techniques and platforms are less efficient. They show a slow responsiveness and lack of scalability, performance and accuracy. To face the complex Big Data challenges, much work has been carried out. As a result, various types of distributions and technologies have been developed. This paper is a review that survey recent technologies developed for Big Data. It aims to help to select and adopt the right combination of different Big Data technologies according to their technological needs and specific applications' requirements. It provides not only a global view of main Big Data technologies but also comparisons according to different system layers such as Data Storage Layer, Data Processing Layer, Data Querying Layer, Data Access Layer and Management Layer. It categorizes and discusses main technologies features, advantages, limits and usages.},
  keywords = {Big Data,Big Data analytics,Big Data distributions,Hadoop,Machine learning,NoSQL}
}

@misc{ozdilExperimentalComparativeBenchmark2021,
  title = {An {{Experimental}} and {{Comparative Benchmark Study Examining Resource Utilization}} in {{Managed Hadoop Context}}},
  author = {Ozdil, Uluer Emre and Ayvaz, Serkan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10134},
  eprint = {2112.10134},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-27},
  abstract = {Transitioning cloud-based Hadoop from IaaS to PaaS, which are commercially conceptualized as pay-as-you-go or pay-per-use, often reduces the associated system costs. However, managed Hadoop systems do present a black-box behavior to the end-users who cannot be clear on the inner performance dynamics, hence, on the benefits of leveraging them. In the study, we aimed to understand managed Hadoop context in terms of resource utilization. We utilized three experimental Hadoopon-PaaS proposals as they come out-of-the-box and conducted Hadoopspecific workloads of the HiBench Benchmark Suite. During the benchmark executions, we collected system resource utilization data on the worker nodes. The results indicated that the same property specifications among cloud services do not guarantee nearby performance outputs, nor consistent results within themselves. We assume that the managed systems' architectures and pre-configurations play a significant role in the performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Distributed Parallel and Cluster Computing},
  file = {/Users/diwahsap/Zotero/storage/ZGH4JFBT/Ozdil and Ayvaz - 2021 - An Experimental and Comparative Benchmark Study Ex.pdf}
}

@inproceedings{pavloComparisonApproachesLargescale2009,
  title = {A Comparison of Approaches to Large-Scale Data Analysis},
  booktitle = {Proceedings of the 2009 {{ACM SIGMOD International Conference}} on {{Management}} of Data},
  author = {Pavlo, Andrew and Paulson, Erik and Rasin, Alexander and Abadi, Daniel J. and DeWitt, David J. and Madden, Samuel and Stonebraker, Michael},
  year = {2009},
  month = jun,
  series = {{{SIGMOD}} '09},
  pages = {165--178},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1559845.1559865},
  urldate = {2023-11-06},
  abstract = {There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.},
  isbn = {978-1-60558-551-2},
  keywords = {benchmarks,mapreduce,parallel database},
  file = {/Users/diwahsap/Zotero/storage/E6VLEXR3/Pavlo et al. - 2009 - A comparison of approaches to large-scale data ana.pdf}
}

@misc{PenjelasanApaItu2023,
  title = {Penjelasan {{Apa}} Itu {{Bandwidth}}, {{Throughput}}, Dan {{Latency}}},
  year = {2023},
  month = jan,
  urldate = {2024-05-21},
  abstract = {Saat kita belajar mengenai jaringan komputer dan internet, ada beberapa istilah yang kerap muncul dan mesti Anda pahami. Istilah-istilah tersebut adalah},
  chapter = {Server Projects},
  howpublished = {https://www.initialboard.com/penjelasan-apa-itu-bandwidth-throughput-dan-latency},
  langid = {american},
  file = {/Users/diwahsap/Zotero/storage/VUME4UI3/penjelasan-apa-itu-bandwidth-throughput-dan-latency.html}
}

@article{qaiserTextMiningUse2018,
  title = {Text {{Mining}}: {{Use}} of {{TF-IDF}} to {{Examine}} the {{Relevance}} of {{Words}} to {{Documents}}},
  shorttitle = {Text {{Mining}}},
  author = {Qaiser, Shahzad and Ali, Ramsha},
  year = {2018},
  month = jul,
  journal = {International Journal of Computer Applications},
  volume = {181},
  number = {1},
  pages = {25--29},
  issn = {09758887},
  doi = {10.5120/ijca2018917395},
  urldate = {2024-05-16},
  abstract = {In this paper, the use of TF-IDF stands for (term frequencyinverse document frequency) is discussed in examining the relevance of key-words to documents in corpus. The study is focused on how the algorithm can be applied on number of documents. First, the working principle and steps which should be followed for implementation of TF-IDF are elaborated. Secondly, in order to verify the findings from executing the algorithm, results are presented, then strengths and weaknesses of TD-IDF algorithm are compared. This paper also talked about how such weaknesses can be tackled. Finally, the work is summarized and the future research directions are discussed.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/GHNJXVJN/Qaiser and Ali - 2018 - Text Mining Use of TF-IDF to Examine the Relevanc.pdf}
}

@misc{RedHot20212021,
  title = {Red {{Hot}}: {{The}} 2021 {{Machine Learning}}, {{AI}} and {{Data}} ({{MAD}}) {{Landscape}}},
  shorttitle = {Red {{Hot}}},
  year = {2021},
  month = sep,
  journal = {Matt Turck},
  urldate = {2023-12-28},
  abstract = {Full resolution version of the landscape image here It's been a hot, hot year in the world of data, machine learning and AI.  Just when you thought it couldn't grow any more explosively, the data/AI landscape just did: rapid pace of company creation, exciting new product and project launch},
  chapter = {AI},
  howpublished = {https://mattturck.com/data2021/},
  langid = {american},
  file = {/Users/diwahsap/Zotero/storage/M8EC3DZQ/data2021.html}
}

@article{reinselDigitizationWorldEdge2018,
  title = {The {{Digitization}} of the {{World}} from {{Edge}} to {{Core}}},
  author = {Reinsel, David and Gantz, John and Rydning, John},
  year = {2018},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/84TI8EGQ/Reinsel et al. - 2018 - The Digitization of the World from Edge to Core.pdf}
}

@misc{ResilientDistributedComputing,
  title = {Resilient Distributed Computing Platforms for Big Data Analysis Using {{Spark}} and {{Hadoop}} {\textbar} {{IEEE Conference Publication}} {\textbar} {{IEEE Xplore}}},
  urldate = {2023-10-13},
  howpublished = {https://xplorestaging.ieee.org/document/7539859/;jsessionid=PRAnIaN-qIUI9KFpsvdeQ4sQLkwBcXV9bFIxgU2qADV5InHStw-J!-1449844779},
  file = {/Users/diwahsap/Zotero/storage/P78XIXB3/;jsessionid=PRAnIaN-qIUI9KFpsvdeQ4sQLkwBcXV9bFIxgU2qADV5InHStw-J!-1449844779.html}
}

@book{rossIntroductoryStatistics2017,
  title = {Introductory Statistics},
  author = {Ross, Sheldon M.},
  year = {2017},
  edition = {Fourth edition},
  publisher = {Elsevier Academic Press},
  address = {Amsterdam Boston Heidelberg London New York Oxford Paris San Diego San Francisco Singapore Sydney Tokyo},
  isbn = {978-0-12-804317-2 978-0-12-374388-6},
  langid = {english}
}

@article{ruijunLightweightExperimentalPlatform2020,
  title = {A {{Lightweight Experimental Platform}} for {{Big Data Based}} on {{Docker Containers}}},
  author = {Ruijun, Gu},
  year = {2020},
  month = jan,
  journal = {Journal of Physics: Conference Series},
  volume = {1437},
  number = {1},
  pages = {012104},
  publisher = {IOP Publishing},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1437/1/012104},
  urldate = {2023-09-08},
  abstract = {In recent years, many colleges and universities have set up the major of big data. The biggest problem in teaching is that there is no supporting basic experimental environment, and it is difficult to deploy and configure the big data environment at the same time. In addition, the lack of experimental data, experimental teaching plans and experimental manuals in the experimental process makes it difficult to carry out relevant teaching. In order to reduce the cost of laboratory construction and the difficulty of learning big data, a lightweight big data experimental platform was constructed based on virtualized container technology. Through this platform, we can create a big data cluster, provide various suitable experimental environments, focus on the technology itself, and greatly improve the learning efficiency.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/WLGIH7LQ/Ruijun - 2020 - A Lightweight Experimental Platform for Big Data B.pdf}
}

@article{saadoonFaultToleranceBig2022,
  title = {Fault Tolerance in Big Data Storage and Processing Systems: {{A}} Review on Challenges and Solutions},
  shorttitle = {Fault Tolerance in Big Data Storage and Processing Systems},
  author = {Saadoon, Muntadher and Ab. Hamid, Siti Hafizah and Sofian, Hazrina and Altarturi, Hamza H. M. and Azizul, Zati Hakim and Nasuha, Nur},
  year = {2022},
  month = mar,
  journal = {Ain Shams Engineering Journal},
  volume = {13},
  number = {2},
  pages = {101538},
  issn = {2090-4479},
  doi = {10.1016/j.asej.2021.06.024},
  urldate = {2024-01-18},
  abstract = {Big data systems are sufficiently stable to store and process a massive volume of rapidly changing data. However, big data systems are composed of large-scale hardware resources that make their subspecies easily fail. Fault tolerance is the main property of such systems because it maintains availability, reliability, and constant performance during faults. Achieving an efficient fault tolerance solution in a big data system is challenging because fault tolerance must meet some constraints related to the system performance and resource consumption. This study aims to provide a consistent understanding of fault tolerance in big data systems and highlights common challenges that hinder the improvement in fault tolerance efficiency. The fault tolerance solutions applied by previous studies intended to address the identified challenges are reviewed. The paper also presents a perceptive discussion of the findings derived from previous studies and proposes a list of future directions to address the fault tolerance challenges.},
  keywords = {Big data processing,Big data storage,Fault detection,Fault recovery,Fault tolerance}
}

@inproceedings{samadiComparativeStudyHadoop2016,
  title = {Comparative Study between {{Hadoop}} and {{Spark}} Based on {{Hibench}} Benchmarks},
  booktitle = {2016 2nd {{International Conference}} on {{Cloud Computing Technologies}} and {{Applications}} ({{CloudTech}})},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  year = {2016},
  month = may,
  pages = {267--275},
  publisher = {IEEE},
  address = {Marrakech, Morocco},
  doi = {10.1109/CloudTech.2016.7847709},
  urldate = {2023-09-24},
  abstract = {Big Data is currently a hot topic for companies and scientists around the world, due to the emergence of new technologies, devices and communication means like social network sites, which led to a noticeable increase of the amount of data produced every year, even every day. In addition, traditional algorithms and technologies are inefficient to process, analyze and store this vast amount of data. So, to solve this problem, Big Data frameworks are needed. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks. Hadoop and Spark, which are used to efficiently process vast amount of data in parallel and distributed mode on a large clusters. Hibench benchmark suite is used to compare the performance of these two frameworks based on the criteria as execution time, throughput and speedup. Our experimental results show that Spark is more efficient than Hadoop to deal with large amount of data. However, spark requires higher memory allocation, since it loads processes into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  isbn = {978-1-4673-8894-8},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/VVTPYL4Z/Samadi et al. - 2016 - Comparative study between Hadoop and Spark based o.pdf}
}

@article{samadiPerformanceComparisonHadoop2018,
  title = {Performance Comparison between {{Hadoop}} and {{Spark}} Frameworks Using {{HiBench}} Benchmarks},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  year = {2018},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {30},
  number = {12},
  pages = {e4367},
  issn = {1532-0634},
  doi = {10.1002/cpe.4367},
  urldate = {2023-09-21},
  abstract = {Big Data has become one of the major areas of research for cloud service providers due to a large amount of data produced every day and the inefficiency of traditional algorithms and technologies to handle these large amounts of data. Big Data with its characteristics such as volume, variety, and veracity (3V) requires efficient technologies to process in real time. To solve this problem and to process and analyze this vast amount of data, there are many powerful tools like Hadoop and Spark, which are mainly used in the context of Big Data. They work following the principles of parallel computing. The challenge is to specify which Big Data's tool is better depending on the processing context. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks deployed on virtual machines. Hadoop MapReduce and Apache Spark are used to efficiently process a vast amount of data in parallel and distributed mode on large clusters, and both of them suit for Big Data processing. We also present the execution results of Apache Hadoop in Amazon EC2, a major cloud computing environment. To compare the performance of these two frameworks, we use HiBench benchmark suite, which is an experimental approach for measuring the effectiveness of any computer system. The comparison is made based on three criteria: execution time, throughput, and speedup. We test Wordcount workload with different data sizes for more accurate results. Our experimental results show that the performance of these frameworks varies significantly based on the use case implementation. Furthermore, from our results we draw the conclusion that Spark is more efficient than Hadoop to deal with a large amount of data in major cases. However, Spark requires higher memory allocation, since it loads the data to be processed into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  copyright = {Copyright {\copyright} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Amazon EC2,Big Data,cloud computing,Hadoop,HiBench,parallel and distributed processing,Spark},
  file = {/Users/diwahsap/Zotero/storage/XGWSZMA2/Samadi et al. - 2018 - Performance comparison between Hadoop and Spark fr.pdf;/Users/diwahsap/Zotero/storage/J7R8LXTP/cpe.html}
}

@article{sandhuBigDataCloud2022,
  title = {Big Data with Cloud Computing: {{Discussions}} and Challenges},
  shorttitle = {Big Data with Cloud Computing},
  author = {Sandhu, Amanpreet Kaur},
  year = {2022},
  month = mar,
  journal = {Big Data Mining and Analytics},
  volume = {5},
  number = {1},
  pages = {32--40},
  issn = {2096-0654},
  doi = {10.26599/BDMA.2021.9020016},
  urldate = {2023-12-28},
  abstract = {With the recent advancements in computer technologies, the amount of data available is increasing day by day. However, excessive amounts of data create great challenges for users. Meanwhile, cloud computing services provide a powerful environment to store large volumes of data. They eliminate various requirements, such as dedicated space and maintenance of expensive computer hardware and software. Handling big data is a time-consuming task that requires large computational clusters to ensure successful data storage and processing. In this work, the definition, classification, and characteristics of big data are discussed, along with various cloud services, such as Microsoft Azure, Google Cloud, Amazon Web Services, International Business Machine cloud, Hortonworks, and MapR. A comparative analysis of various cloud-based big data frameworks is also performed. Various research challenges are defined in terms of distributed database storage, data security, heterogeneity, and data visualization.},
  file = {/Users/diwahsap/Zotero/storage/G6U2523Z/Sandhu - 2022 - Big data with cloud computing Discussions and cha.pdf}
}

@article{saputroPerbandinganKinerjaKomputasi2020,
  title = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}} : {{Storm Event Database}})},
  shorttitle = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}}},
  author = {Saputro, Rendiyono and Aminuddin, Aminuddin and Munarko, Yuda},
  year = {2020},
  month = mar,
  journal = {Jurnal Repositor},
  volume = {2},
  pages = {463},
  doi = {10.22219/repositor.v2i4.93},
  abstract = {Perkembangan teknologi telah mengakibatkan pertumbuhan data yang semakin cepat dan besar setiap waktunya. Hal tersebut disebabkan oleh banyaknya sumber data seperti mesin pencari, RFID, catatan transaksi digital, arsip video dan foto, user generated content, internet of things, penelitian ilmiah di berbagai bidang seperti genomika, meteorologi, astronomi, fisika, dll. Selain itu, data - data tersebut memiliki karakteristik yang unik antara satu dengan lainnya, hal ini yang menyebabkan tidak dapat diproses oleh teknologi basis data konvensional. Oleh karena itu, dikembangkan beragam framework komputasi terdistribusi seperti Apache Hadoop dan Apache Spark yang memungkinkan untuk memproses data secara terdistribusi dengan menggunakan gugus komputer.Adanya ragam framework komputasi terdistribusi, sehingga diperlukan sebuah pengujian untuk mengetahui kinerja komputasi keduanya. Pengujian dilakukan dengan memproses dataset dengan beragam ukuran dan dalam gugus komputer dengan jumlah node yang berbeda. Dari semua hasil pengujian, Apache Hadoop memerlukan waktu yang lebih sedikit dibandingkan dengan Apache Spark. Hal tersebut terjadi karena nilai throughput dan throughput/node Apache Hadoop lebih tinggi daripada Apache Spark.},
  file = {/Users/diwahsap/Zotero/storage/JYJE3NEW/Saputro et al. - 2020 - Perbandingan Kinerja Komputasi Hadoop dan Spark un.pdf}
}

@article{shakerFeatureExtractionBased2022,
  title = {Feature {{Extraction}} Based {{Text Classification}}: {{A}} Review},
  shorttitle = {Feature {{Extraction}} Based {{Text Classification}}},
  author = {Shaker, Saif and Alhajim, Dhafer and {Al-khazaali}, Ahmed and Hussein, Hussein and Athab, Ali},
  year = {2022},
  month = may,
  journal = {Journal of Algebraic Statistics},
  volume = {13},
  pages = {646--653},
  abstract = {This article reviews and discusses feature extraction techniques used for text classification as well as natural language processing in biomedical applications.This researchaims to analyze the similarities of techniques used as technology and algorithms that have become more sophisticated to optimize feature extraction. In feature extraction, a specific of words is taken out from text data. After that, they transform into a feature set to be usable by a classifier. Several algorithms have been identified for classification,including but not limited to SVM, deep neural networks as well as Na{\"i}ve Bayes algorithms. Next, the natural language is processed and achieves better performance results as indicated by certain metrics like execution time, specificity, accuracy, specificity,and sensitivity.},
  file = {/Users/diwahsap/Zotero/storage/KFBSU3LC/Shaker et al. - 2022 - Feature Extraction based Text Classification A re.pdf}
}

@article{shiClashTitansMapReduce2015,
  title = {Clash of the Titans: {{MapReduce}} vs. {{Spark}} for Large Scale Data Analytics},
  shorttitle = {Clash of the Titans},
  author = {Shi, Juwei and Qiu, Yunjie and Minhas, Umar Farooq and Jiao, Limei and Wang, Chen and Reinwald, Berthold and {\"O}zcan, Fatma},
  year = {2015},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {13},
  pages = {2110--2121},
  issn = {2150-8097},
  doi = {10.14778/2831360.2831365},
  urldate = {2023-09-28},
  abstract = {MapReduce and Spark are two very popular open source cluster computing frameworks for large scale data analytics. These frameworks hide the complexity of task parallelism and fault-tolerance, by exposing a simple programming API to users. In this paper, we evaluate the major architectural components in MapReduce and Spark frameworks including: shuffle, execution model, and caching, by using a set of important analytic workloads. To conduct a detailed analysis, we developed two profiling tools: (1) We correlate the task execution plan with the resource utilization for both MapReduce and Spark, and visually present this correlation; (2) We provide a break-down of the task execution time for in-depth analysis. Through detailed experiments, we quantify the performance differences between MapReduce and Spark. Furthermore, we attribute these performance differences to different components which are architected differently in the two frameworks. We further expose the source of these performance differences by using a set of micro-benchmark experiments. Overall, our experiments show that Spark is about 2.5x, 5x, and 5x faster than MapReduce, for Word Count, k-means, and PageRank, respectively. The main causes of these speedups are the efficiency of the hash-based aggregation component for combine, as well as reduced CPU and disk overheads due to RDD caching in Spark. An exception to this is the Sort workload, for which MapReduce is 2x faster than Spark. We show that MapReduce's execution model is more efficient for shuffling data than Spark, thus making Sort run faster on MapReduce.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/MALM7QFA/Shi et al. - 2015 - Clash of the titans MapReduce vs. Spark for large.pdf}
}

@inproceedings{shrivastavaProductRecommendationsUsing2019,
  title = {Product {{Recommendations Using Textual Similarity Based Learning Models}}},
  booktitle = {2019 {{International Conference}} on {{Computer Communication}} and {{Informatics}} ({{ICCCI}})},
  author = {Shrivastava, Rahul and Sisodia, Dilip Singh},
  year = {2019},
  month = jan,
  pages = {1--7},
  issn = {2329-7190},
  doi = {10.1109/ICCCI.2019.8821893},
  urldate = {2024-05-16},
  abstract = {Recommendation systems are achieving great success in e-Commerce applications, during a live interaction with a customer; recommendation system may apply different techniques to solve the problem of making a correct and relevant product recommendation. The main objective of this research is to perform product recommendation using textual similarity based Learning model. In this research data acquired through Amazon product advertising API after Data cleaning and text preprocessing the content based product recommendation have been performed using Bag of Words(BOW) and Term Frequency-Inverse Document Frequency(TF-IDF) based text vectorization techniques. Textual Description of the product converted into n-dimensional vector, and later the Euclidean similarity can be measured between the ndimensional vector of the queried product and other products. Text-based product similarity through text vectorization technique is very useful in performing content-based product recommendation and recommending the similar item to the user it can be used in various E-Commerce applications since these applications are heavily populated with the textual description of the product. Bag of words and TF-IDF generates an n-dimensional vector of a different textual description of the product which in turn leads to a better recommendation of the product. Experimental results and analysis section clearly describes how the proposed model for text-based product find the similarity of products with queried product and display as output the best-recommended product.},
  keywords = {Advertising,Bag of Words,Cleaning,Convolutional neural networks,Data acquisition,Euclidean distance,Euclidean Distance,Image color analysis,Informatics,TF-IDF},
  file = {/Users/diwahsap/Zotero/storage/8SW2AK5P/8821893.html}
}

@misc{simpsonLargeAnnotatedMedical2019,
  title = {A Large Annotated Medical Image Dataset for the Development and Evaluation of Segmentation Algorithms},
  author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {van Ginneken}, Bram and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc and {Golia-Pernicka}, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2019},
  month = feb,
  number = {arXiv:1902.09063},
  eprint = {1902.09063},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1902.09063},
  urldate = {2023-11-07},
  abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/diwahsap/Zotero/storage/6EMYBMJA/Simpson et al. - 2019 - A large annotated medical image dataset for the de.pdf;/Users/diwahsap/Zotero/storage/Q5XCYJXC/1902.html}
}

@article{singhPerformanceOptimizationMapReducebased2018,
  title = {Performance Optimization of {{MapReduce-based Apriori}} Algorithm on {{Hadoop}} Cluster},
  author = {Singh, Sudhakar and Garg, Rakhi and Mishra, P K},
  year = {2018},
  month = apr,
  journal = {Computers \& Electrical Engineering},
  volume = {67},
  pages = {348--364},
  issn = {00457906},
  doi = {10.1016/j.compeleceng.2017.10.008},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/L6NAY2FL/Singh et al. - 2018 - Performance optimization of MapReduce-based Aprior.pdf}
}

@article{sulaimanLITERATUREREVIEWPENERAPAN2023,
  title = {{LITERATURE REVIEW: PENERAPAN BIG DATA DALAM KESEHATAN MASYARAKAT}},
  author = {Sulaiman, Thirsya Widya and Fitriansyah, Raka Bagas and Alaudin, Ahmad Rafif and Ratsanjani, M Hasyim},
  year = {2023},
  volume = {1},
  abstract = {This research is conducted to determine the correlation and implementation of big data towards public health, which can then become a solution in Indonesia that is currently facing the triple burden. This study will review previous literature to find and gather information that can fulfill the research objectives. The results of this research show that big data combined with other disciplines such as AI, IoT, and machine learning have their own benefits and challenges. The challenges include data privacy, accuracy of results, and validation of information spread. On the other hand, the benefits are used to assist in public service activities and government in achieving goals and solving problems regarding public health, through disease surveillance and signal detection, predicting public health risks leading to opportunities to implement prevention interventions, identifying and understanding more about diseases, developing more accurate drugs, and providing more precise care with existing drugs.},
  langid = {indonesian},
  file = {/Users/diwahsap/Zotero/storage/3EVDG37Q/Sulaiman et al. - 2023 - LITERATURE REVIEW PENERAPAN BIG DATA DALAM KESEHA.pdf}
}

@article{syahputraPendeteksianFraudPeran2020,
  title = {{Pendeteksian Fraud: Peran Big Data dan Audit Forensik}},
  shorttitle = {{Pendeteksian Fraud}},
  author = {Syahputra, Briyan Efflin and Afnan, Akhmad},
  year = {2020},
  month = dec,
  journal = {Jurnal ASET (Akuntansi Riset)},
  volume = {12},
  number = {2},
  pages = {301--316},
  issn = {2541-0342},
  doi = {10.17509/jaset.v12i2.28939},
  urldate = {2024-01-18},
  abstract = {The increasing number of fraud cases in Indonesia resulted in significant losses incurred by this country. Therefore, finding an effective method to detect fraud is the focus of many parties, especially the government. This research investigates the influence of big data on forensic audit. A part from that, the influence of big data and forensic audits as mediating variable on fraud detection is also examined. This research is quantitative research with survey method by distributing questionnaires. The respondents in this research were 221 auditors who worked at Badan Pemeriksa Keuangan (BPK) and Badan Pengawasan Keuangan dan Pembangunan (BPKP) Republic of Indonesia. The statistical test in this research was the structural equation modelling (SEM) of SmartPLS.~ The result showed that big data had a significant and positive effect on forensic audit. This research also proved that big data and forensic audits have a significant and positive influence on fraud detection. In addition, forensic audits have been proven to partially mediate the relationship between big data and fraud detection.},
  copyright = {Copyright (c) 2020 Jurnal ASET (Akuntansi Riset)},
  langid = {indonesian},
  keywords = {big data,forensic audit,fraud detection},
  file = {/Users/diwahsap/Zotero/storage/VPVDFLM9/Syahputra and Afnan - 2020 - Pendeteksian Fraud Peran Big Data dan Audit Foren.pdf}
}

@misc{thailappanUnderstandInternalWorking2021,
  title = {Understand {{The Internal Working}} of {{Apache Spark}}},
  author = {Thailappan, Dhanya},
  year = {2021},
  month = aug,
  journal = {Analytics Vidhya},
  urldate = {2024-05-08},
  abstract = {Apache Spark is an open-source distributed big data processing engine. In this article, we will understand internal working of apache spark.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/387H54RR/understand-the-internal-working-of-apache-spark.html}
}

@inproceedings{vermaBigDataManagement2016,
  title = {Big Data Management Processing with {{Hadoop MapReduce}} and Spark Technology: {{A}} Comparison},
  shorttitle = {Big Data Management Processing with {{Hadoop MapReduce}} and Spark Technology},
  booktitle = {2016 {{Symposium}} on {{Colossal Data Analysis}} and {{Networking}} ({{CDAN}})},
  author = {Verma, Ankush and Mansuri, Ashik Hussain and Jain, Neelesh},
  year = {2016},
  month = mar,
  pages = {1--4},
  publisher = {IEEE},
  address = {Indore, Madhya Pradesh, India},
  doi = {10.1109/CDAN.2016.7570891},
  urldate = {2023-10-13},
  abstract = {Hadoop MapReduce is processed for analysis large volume of data through multiple nodes in parallel. However MapReduce has two function Map and Reduce, large data is stored through HDFS. Lack of facility involve in MapReduce so Spark is designed to run for real time stream data and for fast queries. Spark jobs perform work on Resilient Distributed Datasets and directed acyclic graph execution engine. In this paper, we extend Hadoop MapReduce working and Spark architecture with supporting kind of operation to perform. We also show the differences between Hadoop MapReduce and Spark through Map and Reduce phase individually.},
  isbn = {978-1-5090-0669-4},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/V9G87JKX/Verma et al. - 2016 - Big data management processing with Hadoop MapRedu.pdf}
}

@article{vijayakumarFusionBasedFeature2021,
  title = {Fusion Based {{Feature Extraction Analysis}} of {{ECG Signal Interpretation}} - {{A Systematic Approach}}},
  author = {Vijayakumar, Dr T. and Vinothkanna, Mr R. and Duraipandian, Dr M.},
  year = {2021},
  month = mar,
  journal = {Journal of Artificial Intelligence and Capsule Networks},
  volume = {3},
  number = {1},
  pages = {1--16},
  issn = {2582-2012},
  urldate = {2024-05-16},
  langid = {english}
}

@article{wibawaKOMPARASIKECEPATANHADOOP2022,
  title = {{{KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SPARK DALAM MENGOLAH DATA TEKS}}},
  author = {Wibawa, Condro and Wirawan, Setia and Mustikasari, Metty and Anggraeni, Dessy},
  year = {2022},
  month = apr,
  journal = {Jurnal Ilmiah Matrik},
  volume = {24},
  pages = {10--20},
  doi = {10.33557/jurnalmatrik.v24i1.1649},
  abstract = {Istilah Big Data saat ini bukanlah hal yang baru lagi. Salah satu komponen Big Data adalah jumlah data yang masif, yang membuat data tidak bisa diproses dengan cara-cara tradicional. Untuk menyelesaikan masalah ini, dikembangkanlah metode Map Reduce. Map Reduce adalah metode pengolahan data dengan memecah data menjadi bagian-bagian kecil (mapping) dan kemudian hasilnya dijadikan satu kembali (reducing). Framework Map Reduce yang banyak digunakan adalah Hadoop MapReduce dan Apache Spark. Konsep kedua framework ini sama akan tetapi berbeda dalam pengelolaan sumber data. Hadoop MapReduce menggunakan pendekatan HDFS (disk), sedangkan Apache Spark menggunakan RDD (in-memory). Penggunaan RDD pada Apache Spark membuat kinerja framework ini lebih cepat dibandingkan Hadoop MapReduce. Hal ini dibutktikan dalam penelitian ini, dimana untuk mengolah data teks yang sama, kecepatan rata-rata Apache Spark adalah 4,99 kali lebih cepat dibandingkan Hadoop MapReduce.},
  file = {/Users/diwahsap/Zotero/storage/K7H5FS3W/Wibawa et al. - 2022 - KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SP.pdf}
}

@article{xuDeployingResearchingHadoop,
  title = {Deploying and {{Researching Hadoop}} in {{Virtual Machines}}},
  author = {Xu, Guanghui and Xu, Feng and Ma, Hongxu},
  abstract = {Hadoop's emerging and the maturity of virtualization make it feasible to combine them together to process immense data set. To do research on Hadoop in virtual environment, an experimental environment is needed. This paper firstly introduces some technologies used such as CloudStack, MapReduce and Hadoop. Based on that, a method to deploy CloudStack is given. Then we discuss how to deploy Hadoop in virtual machines which can be obtained from CloudStack by some means, then an algorithm to solve the problem that all the virtual machines which are created by CloudStack using same template have a same hostname. After that we run some Hadoop programs under the virtual cluster, which shows that it is feasible to deploying Hadoop in this way. Then some methods to optimize Hadoop in virtual machines are discussed. From this paper, readers can follow it to set up their own Hadoop experimental environment and capture the current status and trend of optimizing Hadoop in virtual environment.},
  langid = {english},
  file = {/Users/diwahsap/Zotero/storage/6EBEWYQQ/Xu et al. - Deploying and Researching Hadoop in Virtual Machin.pdf}
}

@article{yudistiraPeranBigData2021,
  title = {{Peran Big Data dan Deep Learning untuk Menyelesaikan Permasalahan Secara Komprehensif}},
  author = {Yudistira, Novanto},
  year = {2021},
  month = dec,
  journal = {EXPERT: Jurnal Manajemen Sistem Informasi dan Teknologi},
  volume = {11},
  number = {2},
  pages = {78},
  issn = {2745-7265, 2088-5555},
  doi = {10.36448/expert.v11i2.2063},
  urldate = {2024-01-18},
  langid = {indonesian},
  file = {/Users/diwahsap/Zotero/storage/76XL6EAD/Yudistira - 2021 - Peran Big Data dan Deep Learning untuk Menyelesaik.pdf}
}

@inproceedings{zahariaSparkClusterComputing2010,
  title = {Spark: Cluster Computing with Working Sets},
  shorttitle = {Spark},
  booktitle = {Proceedings of the 2nd {{USENIX}} Conference on {{Hot}} Topics in Cloud Computing},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2010},
  month = jun,
  series = {{{HotCloud}}'10},
  pages = {10},
  publisher = {USENIX Association},
  address = {USA},
  urldate = {2023-09-28},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}
