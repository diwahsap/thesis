\chapter{LANDASAN TEORI}

\section{Tinjauan Pustaka}
Penelitian ini menggunakan beberapa teori dasar supaya memperjelas proses penelitian dan memberikan pemahaman lebih lanjut. Penelitian terdahulu mengenai evaluasi performa Hadoop dan Spark dapat dilihat pada Tabel \ref{table:penelitian-dulu}.

%\begin{table}[h]
%\caption{Penelitian Terdahulu}
%\label{table:penelitian-dulu}
%\scriptsize
%\begin{tabularx}{\textwidth}{c*{5}{>{\raggedright\arraybackslash}X}}
%\toprule
%\textbf{Tahun} & \textbf{Judul} & \textbf{Penulis} & \textbf{Metode} 
%  & \textbf{Deskripsi Penelitian} \\
%\midrule
%2020 & \textit{A comprehensive performance analysis of Apache Hadoop and Apache Spark for large scale data sets using HiBench \cite{ahmedComprehensivePerformanceAnalysis2020}} 
%  & N. Ahmed, Andre L. C. Barczak, Teo Susnjak, Mohammed A. Rashid
%  & Penelitian ini menyelidiki parameter-parameter yang paling berdampak, yaitu \textit{input splits}, dan \textit{shuffle}, untuk membandingkan kinerja antara Hadoop dan Spark, dengan menggunakan klaster yang diimplementasikan di laboratorium. Guna mengevaluasi kinerja, dua beban kerja dipilih, yakni \textit{Word Count} dan  \textit{Tera Sort}. Metrik kinerja diukur berdasarkan tiga kriteria: waktu eksekusi, \textit{throughput}, dan \textit{speedup}. 
%  & Kinerja kedua sistem sangat bergantung pada ukuran data masukan dan pemilihan parameter yang tepat. Analisis hasil menunjukkan bahwa Spark memiliki kinerja yang lebih baik dibandingkan dengan Hadoop ketika set data kecil, mencapai peningkatan kecepatan hingga dua kali lipat dalam beban kerja \textit{Word Count} dan hingga 14 kali lipat dalam beban kerja \textit{Tera Sort} ketika nilai parameter \textit{default} dikonfigurasi ulang. \\
%2018 & \textit{Performance comparison between Hadoop and Spark frameworks using HiBench benchmarks \cite{samadiPerformanceComparisonHadoop2018}} 
%  & Yassir Samadi, Mostapha Zbakh, Claude Tadonki 
%  & Perbandingan kinerja diimplementasikan pada mesin virtual (VM). Untuk membandingkannya, digunakan HiBench. Perbandingan dilakukan berdasarkan tiga kriteria: waktu eksekusi, \textit{throughput}, dan speedup. Beban kerja \textit{Word Count} diuji dengan ukuran data yang berbeda.
%  & Spark lebih efisien dibandingkan Hadoop dalam menangani jumlah data yang besar. Namun, Spark memerlukan alokasi memori yang lebih tinggi, karena memuat data yang akan diproses ke dalam memori dan menyimpannya dalam cache untuk sementara. \\
% 2015 & \textit{Comparing Apache Spark and Map Reduce with Performance Analysis using K-Means \cite{gopalaniComparingApacheSpark2015}} 
%  & Satish Gopalani, Rohan Arora
%  & Hadoop dan Spark dibandingkan menggunakan algoritma pembelajaran mesin (K-Means). Ukuran data yang digunakan adalah sebesar 64MB, 1240MB dengan satu \textit{node}, dan 1240MB dengan dua \textit{node}.  
%  & Hasil-hasil penelitian dengan jelas menunjukkan bahwa kinerja Spark jauh lebih tinggi dari segi waktu, dimana setiap ukuran dataset mengakibatkan penurunan waktu pemrosesan hingga tiga kali lipat dibandingkan dengan Hadoop. \\
%\bottomrule
%\end{tabularx}
%\end{table}


\begin{table}[h]
\caption{Penelitian Terdahulu}
\label{table:penelitian-dulu}
\begin{tabularx}{\textwidth}{c*{5}{>{\centering\arraybackslash}X}}
\toprule
\textbf{No} & \textbf{Nama Peneliti} & \textbf{Metode yang Digunakan} & \textbf{Hasil Penelitian} 
  & \textbf{Tahun} \\
\midrule
1 & C. Wibawa, S. Wirawan, M. Mustikasari, dan D. Anggraeni \cite{wibawaKOMPARASIKECEPATANHADOOP2022} & Konfigurasi empat \textit{node}. Tiga skenario beban kerja & Spark memiliki kinerja empat kali lebih cepat dibandingkan Hadoop & 2022 \\ 
2 & N. Ahmed, A. L. C. Barczak, T. Susnjak, dan M. A. Rashid \cite{ahmedComprehensivePerformanceAnalysis2020} & HiBench. Konfigurasi sembilan \textit{node}. \textit{Word Count} and \textit{TeraSort} & Spark dua kali lebih cepat pada beban kerja Word Count, 14 kali pada TeraSort & 2020 \\
3 & Yassir Samadi, Mostapha Zbakh, Claude Tadonki \cite{samadiPerformanceComparisonHadoop2018} & HiBench. Konfigurasi \textit{Pseudo Distributed}. Sembilan beban kerja & Spark memiliki kinerja empat kali lebih cepat dibandingkan Hadoop & 2018 \\
4 & Satish Gopalani, Rohan Arora \cite{gopalaniComparingApacheSpark2015} & K-Means. Konfigurasi dua \textit{node} & Spark memiliki kinerja dua kali lebih cepat dibandingkan dengan Hadoop & 2015 \\
\bottomrule
\end{tabularx}
\end{table}

\section{Konsep \textit{Big Data}}
\textit{Big data} sering didefinisikan bersama dengan kompleksitas suatu data \cite{barosenAnalysisComparisonInterfacing2018}. Berbeda dengan tradisional data, \textit{big data} merujuk pada pertumbuhan data dalam berbagai format, baik dari struktur, semi-terstruktur, dan tidak terstruktur \cite{oussousBigDataTechnologies2018}. \textit{Big data} memiliki banyak jenis sehingga membutuhkan teknologi yang lebih bertenaga serta algoritma yang lebih canggih. Pendekatan teknologi yang sering digunakan oleh \textit{business intelligence} biasanya tidak dapat lagi efisien jika digunakan.

\textit{Big data} didefinisikan menjadi tiga karakteristik (3V), yaitu \textit{Volume, Velocity}, dan \textit{Variety} \cite{furhtIntroductionBigData2016}. \textit{Volume} berkaitan dengan jumlah data yang terbentuk atau dibuat secara terus menerus oleh beragam  perangkat, seperti telepon genggam dan aplikasi (sosial media, sensor, IoT). Selanjutnya, \textit{velocity} memberikan makna bahwa data bertumbuh secara cepat dan harus diproses secara cepat juga untuk memberikan informasi yang berguna \cite{sandhuBigDataCloud2022}. YouTube adalah ilustrasi yang tepat untuk menggambarkan bagaimana pertumbuhan data begitu cepat. Terakhir, \textit{variety} berkaitan dengan variasi sumber dan format data. 

Penerapan dari \textit{big data} tidak hanya terbatas pada pengumpulan dan penyimpanan data, tetapi juga meliputi analisis dan pengolahan data tersebut untuk menghasilkan wawasan yang berguna. Beberapa sektor yang telah menerapkan \textit{big data} secara luas meliputi kesehatan, keuangan, ritel, dan pemerintahan \cite{oussousBigDataTechnologies2018}. Dalam sektor kesehatan, \textit{big data} digunakan untuk menganalisis informasi pasien secara massal guna meningkatkan kualitas perawatan dan menemukan pola-pola penyakit. Sementara itu, di sektor keuangan, big data membantu dalam analisis risiko, deteksi penipuan, dan personalisasi layanan untuk pelanggan.

\section{Statistika Deskriptif}
Statistika deskriptif merupakan cabang statistika yang fokus pada pengorganisasian, penyajian, dan pengikhtisaran data \cite{rossIntroductoryStatistics2017}. Informasi yang diperoleh dari statistika deskriptif berguna untuk mendapatkan gambaran umum mengenai data dan untuk memahaminya secara lebih mendalam. Beberapa ukuran statistika deskriptif yang umum digunakan antara lain,

\begin{enumerate}
\item Maksimum (Max), merupakan nilai tertinggi dalam suatu kumpulan data. 

\item Minimum (Min), merupakan nilai terendah dalam suatu kumpulan data. 

\item Rata-rata (Mean), merupakan nilai tengah dari suatu kumpulan data. 

\begin{equation}
\bar{x} = \dfrac{\sum_{i=1}^{n} x_i}{n}
\end{equation}

Keterangan:
\begin{itemize}
\item $\bar{x}$ adalah mean
\item $x_i$ adalah nilai data ke-i
\item $n$ adalah jumlah total data
\end{itemize}

\newpage
\item Standar Deviasi ($\sigma$), merupakan persebaran data pada suatu sampel untuk melihat seberapa jauh atau seberapa dekat nilai data dengan rata-ratanya. Semakin besar standar deviasi, semakin tersebar data dari mean. 

\begin{equation}
\text{$\sigma$} = \sqrt{\cfrac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n}}
\end{equation}

Keterangan:
\begin{itemize}
\item $\sigma$ adalah standar deviasi
\item $x_i$ adalah nilai data ke-i
\item $\bar{x}$ adalah mean
\item $n$ adalah jumlah total data
\end{itemize}

\end{enumerate}

\section{Ekstraksi Fitur Teks (\textit{Text Feature Extraction})}
Penerapan algoritma \textit{sort} dan \textit{word count} dapat dilihat pada ekstraksi fitur teks. Ekstraksi fitur teks bertujuan untuk mengurangi kompleksitas data (atau yang sering disebut juga \textit{Data Dimensionality}) namun tetap menyimpan sebanyak mungkin informasi yang paling relevan. Data mentah yang mengandung banyak fitur tidak relevan menyulitkan algoritma untuk memproses data secara akurat. Dengan melakukan ekstraksi fitur, fitur yang relevan dipisahkan dari fitur yang tidak relevan \cite{shakerFeatureExtractionBased2022}. Dengan lebih sedikit fitur yang harus diproses, kumpulan data menjadi lebih sederhana dan akurasi serta efisiensi analisis meningkat.

\subsection{\textit{Bag of Words} (BoW)}
BoW adalah teknik sederhana yang mengabaikan urutan dan struktur gramatikal kalimat, dan hanya berfokus pada frekuensi kemunculan kata dalam dokumen. Prosesnya melibatkan langkah-langkah berikut:

\begin{enumerate}
    \item Tokenisasi, yaitu teks dipecah menjadi kata-kata individual (\textit{token}).
    \item Pembuatan kosakata yang berisi daftar unik dari semua token yang ada dalam seluruh kumpulan dokumen dibuat.
    \item Penghitungan Kata (\textit{Word Count}), yaitu menghitung frekuensi kemunculan setiap kata dalam dokumen.
    \item Representasi Vektor, yaitu setiap elemen vektor mewakili frekuensi kata tertentu dalam kosakata.
\end{enumerate}

BoW mudah diimplementasikan dan efisien, namun kelemahannya adalah kehilangan informasi kontekstual dan semantik. Kata-kata dengan frekuensi tinggi, meskipun kurang informatif, dapat mendominasi representasi vektor.

\subsection{\textit{Term Frequency-Inverse Document Frequency} (TF-IDF)}

TF-IDF mengatasi beberapa kelemahan BoW dengan mempertimbangkan pentingnya kata dalam dokumen dan koleksi dokumen \cite{qaiserTextMiningUse2018}. TF-IDF terdiri dari dua komponen:

\begin{enumerate}
    \item \textit{Term Frequency} (TF), yaitu mengukur seberapa sering suatu kata muncul dalam dokumen.
    \item \textit{Inverse Document Frequency} (IDF), yaitu mengukur seberapa penting suatu kata dalam seluruh koleksi dokumen. Kata-kata yang muncul di banyak dokumen (seperti kata "yang") memiliki IDF rendah, sementara kata-kata yang jarang muncul (dan kemungkinan lebih informatif) memiliki IDF tinggi.
\end{enumerate}

Nilai TF-IDF akan mencerminkan pentingnya kata dalam dokumen dan koleksi dokumen. Rumus umum untuk TF-IDF sebagai berikut, 

\begin{equation}
w_{ij} = tf_{ij} \times \log \left(\frac{N}{df_i}\right)
\end{equation}

Keterangan:

\begin{itemize}
    \item $w_{ij}$ adalah bobot tf-idf untuk kata $i$ dalam dokumen $j$;
    \item $tf_{ij}$ adalah frekuensi kemunculan kata $i$ dalam dokumen $j$ dibagi dengan total jumlah kata dalam dokumen $j$;
    \item $N$ adalah total jumlah dokumen dalam korpus;
    \item $df_i$ adalah jumlah dokumen dalam korpus yang mengandung kata $i$.
\end{itemize}

Sebagai contoh, terdapat kata \textit{British} yang muncul lima kali dalam sebuah dokumen yang berisi seratus kata. Dengan korpus yang berisi empat dokumen, dimana dua dokumen menyebutkan kata \textit{British}, TF-IDF dapat dihitung sebagai berikut,

\begin{equation*}
w_{British} = \frac{5}{100} \times \log \left(\frac{4}{2}\right) = 0.015
\end{equation*}

TF-IDF meningkat seiring dengan peningkatan frekuensi kemunculan kata dalam dokumen, tetapi menurun seiring dengan peningkatan jumlah dokumen lain dalam korpus yang juga mengandung kata tersebut. Variasi dari skema pembobotan TF-IDF sering digunakan oleh mesin pencari sebagai alat utama dalam menilai dan mengurutkan relevansi dokumen terhadap \textit{query} pengguna.

\section{Komputasi Awan \textit{(Cloud Computing)}}
Komputasi awan didefinisikan sebagai sistem informasi yang memungkinkan akses mudah ke sumber daya komputasi atau layanan komputasi sesuai permintaan (\textit{on demand}), misalnya  aplikasi Google Mail, Microsft One Drive, dan Siakad Itera.

Sistem komputasi awan saat ini menyediakan tiga layanan utama:
\begin{enumerate}
	\item \textit{Infrastructure as a service} (IaaS), adalah layanan awan yang menawarkan kepada pengguna untuk mengatur dan mengonfigurasikan sumber daya yang dibutuhkan untuk menjalankan aplikasi dan sistem IT. Jenis IaaS biasanya berbentuk komputasi, penyimpanan, dan sumber daya jaringan yang dibuat sebagai layanan.
	\item \textit{Platform as a service} (PaaS), adalah layanan awan yang memungkinkan pengguna untuk mengembangkan, mengelola, dan menjalankan aplikasi di lingkungan yang dikontrol oleh penyedia layanan, tanpa harus khawatir dengan infrastruktur yang mendasarinya. 
	\item \textit{Software as a service} (SaaS), adalah layanan awan yang mengacu pada aplikasi yang berjalan pada infrastruktur awan yang di-\textit{hosting} oleh vendor atau penyedia layanan dan tersedia untuk pengguna akhir melalui browser web. 
\end{enumerate}
Komputasi awan menjadi salah satu aspek terpenting dalam menjalankan komputasi yang kompleks, misalnya Hadoop atau Spark. Salah satu komputasi awan yang dapat diandalkan adalah DigitalOcean. DigitalOcean didirikan pada tahun 2012 untuk memenuhi kebutuhan pengembang demi akses komputasi awan yang mudah dimengerti dan terjangkau \cite{DigitalOcean}. Salah satu produk DigitalOcean yang sering digunakan adalah Droplet, \textit{easy-to-use} komputer virtual yang siap digunakan dalam hitungan menit. Pengguna dapat memilih lokasi dimana komputer akan dijalankan, bagaimana konfigurasi prosesor serta memori, serta memilih sistem operasi apa yang akan digunakan.

\section{\textit{Shell Script}}
\textit{Shell script} merupakan serangkaian perintah yang dieksekusi dalam lingkungan sistem operasi Unix atau \textit{Unix-like} \cite{newhamLearningBashShell2005}. \textit{Shell script} memungkinkan pengguna untuk mengotomatiskan tugas-tugas rutin, melakukan pemrosesan file, dan membangun aplikasi yang kompleks dengan menggunakan perintah-perintah shell. \textit{Shell script} umumnya ditulis menggunakan bahasa pemrograman shell, seperti Bash (Bourne Again Shell), yang merupakan shell standar pada sebagian besar sistem operasi Linux dan MacOS.
Sebagai contoh, dalam mengelola pencadangan sistem, seorang administrator dapat membuat \textit{shell script} sederhana yang menjalankan perintah-perintah untuk menyalin file-file penting ke lokasi penyimpanan cadangan secara berkala. Skrip tersebut dapat dijadwalkan untuk berjalan secara otomatis sehingga proses pencadangan dapat dilakukan tanpa campur tangan manusia secara berkala. 

\section{\textit{CPU Bound} dan \textit{I/O Bound}}

Terdapat dua jenis beban kerja pada pemrosesan data berdasarkan karakteristik dan kebutuhan sumber daya yang berbeda, yaitu \textit{CPU-bound} dan \textit{I/O-bound}. Proses \textit{CPU-bound} (Gambar \ref{fig:cpu-bound}, bagian atas) sangat bergantung pada kemampuan pemrosesan data oleh \textit{Central Processing Unit} (CPU). Proses ini menghabiskan sebagian besar waktunya dalam menjalankan instruksi CPU dan jarang berinteraksi dengan sistem I/O (\textit{Input/Output}). Sebaliknya, proses \textit{I/O-bound} (Gambar \ref{fig:cpu-bound}, bagian bawah) lebih banyak menghabiskan waktu dalam menunggu operasi I/O, seperti akses \textit{disk}, dan jaringan. CPU dalam proses ini hanya digunakan sesaat untuk memproses data yang baru saja diperoleh dari I/O. 

Proses \textit{CPU-bound} memiliki interval waktu yang panjang untuk menjalankan instruksi CPU, dan jarang melakukan operasi I/O, yang dikenal sebagai \textit{CPU bursts} yang panjang dan jarang. Sementara itu, proses \textit{I/O-bound} memiliki interval waktu yang pendek untuk menjalankan instruksi CPU, dan sering melakukan operasi I/O, sehingga memiliki \textit{CPU bursts} yang pendek dan sering. 

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/cpu-io-bound.jpg}
    \caption{\textit{CPU-I/O Bound} \cite{baeldungGuideCpuBoundBound2021}}
    \label{fig:cpu-bound}
\end{figure}


\section{MapReduce}
MapReduce adalah model pemrograman dan implementasi teknik pemrosesan data berukuran besar yang pertama kali dipopulerkan oleh Google pada tahun 2004\cite{kaliaAnalysisHadoopMapReduce2021}. MapReduce menawarkan pemrosesan data yang dapat diandalkan serta \textit{fault-tolerant manner} (tahan terhadap kesalahan).  MapReduce berjalan secara paralel dan berada pada lingkungan komputasi terdistribusi \cite{cTaskFailureResilience2020}. Model ini mengadopsi arsitektur tersentraliasi, yaitu satu \textit{node} berperan sebagai \textit{master} dan \textit{node} yang lain berperan sebagai \textit{workers} atau \textit{slave} \cite{herodotouHadoopPerformanceModels2011, bakratsasHadoopMapReducePerformance2018}. \textit{Master node} bertanggung jawab untuk melakukan penjadwalan kerja, dan \textit{slave node} berperan untuk menjalankan eksekusi kerja. 

MapReduce terdiri dari fungsi \textit{Map} dan fungsi \textit{Reduce}, sesuai dengan Gambar \ref{fig:mapreduce-flow} \cite{gandomiHybSMRPHybridScheduling2019}. Kedua fungsi ini tersebar di seluruh \textit{slave node} yang terhubung dalam klaster dan berjalan secara paralel. Fungsi \textit{Map} berperan untuk membagi masalah besar menjadi masalah yang lebih kecil dan mendistribusikannya ke \textit{slave node}. Hasil pemrosesan dari \textit{slave node} akan dikumpulkan oleh \textit{master node} melalui fungsi \textit{Reduce}. Hasil dari proses \textit{Reduce} yang akan dikirimkan sebagai keluaran akhir proses MapReduce.  

\begin{figure}[h!]
    \centering
    \includegraphics[width=1\textwidth]{figures/ch02/mapreduce-scheme.png}
    \caption{Cara Kerja MapReduce}
    \label{fig:mapreduce-flow}
\end{figure}
                                                                                                                                                                                                                                                                                                                                                       \subsection{Apache Hadoop}
Apache Hadoop adalah perangkat lunak sumber terbuka yang ditulis dengan bahasa pemrograman Java untuk pemrosesan dan penyimpanan data menggunakan komputasi terdistribusi \cite{ApacheHadoop}. Hadoop dapat diinstalasi pada satu \textit{node} komputer, atau ratusan \textit{node} komputer yang digabungkan dalam sebuah klaster \cite{maneasEvolutionHadoopDistributed2018}. Berkaitan dengan pemrosesan data, Hadoop mengimplementasikan model MapReduce untuk pemrosesan data secara paralel dan cepat. Selain itu, Hadoop menyediakan sistem penyimpanan data terdistribusi yang dikenal sebagai Hadoop Distributed File System (HDFS) untuk akses data, pemrosesan, dan komputasi \cite{dabasAnalysisCommentsYoutube2019}. Arsitektur Hadoop secara umum dapat dilihat pada Gambar \ref{fig:hadoop-str}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/ch02/hadoop-str}
    \caption{Arsitektur Hadoop}
    \label{fig:hadoop-str}
\end{figure}

\subsection{Mode Kerja Hadoop}
Hadoop dapat dijalankan dalam tiga mode operasi yang berbeda yaitu \textit{standalone, pseudo-distributed}, dan \textit{fully distributed} \cite{johnDataLakeEnterprises2017}. Dalam \textit{standalone mode}, semua proses Hadoop berjalan pada satu \textit{node} tunggal menggunakan sistem berkas lokal tanpa memerlukan konfigurasi tambahan pada Hadoop seperti pada Gambar \ref{fig:hadoop-modes}. 

\textit{Pseudo-distributed mode} menjalankan semua komponen Hadoop pada satu \textit{node} tunggal tetapi menyimulasikan klaster dengan komunikasi antar proses melalui socket jaringan, sehingga memerlukan konfigurasi pada berkas \textit{core-site, mapred-site}, dan \textit{hdfs-site}. Sedangkan \textit{fully distributed mode} menyebarkan proses Hadoop ke beberapa \textit{node} dalam klaster sebenarnya yang biasanya digunakan untuk tahap produksi. \textit{Fully distributed mode} mendukung skalabilitas, ketersediaan tinggi, dan keamanan dengan memerlukan instalasi Hadoop dan konfigurasi klaster pada setiap \textit{node}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/hadoop-modes}
    \caption{Mode Kerja Hadoop \cite{khataiImplementationTextMining2021}}
    \label{fig:hadoop-modes}
\end{figure}

\subsection{Hadoop Distributed File System (HDFS)}
\textit{Hadoop Distributed File System} adalah sistem file terdistribusi yang dikembangkan sebagai bagian dari Hadoop \cite{abhishekIntegratedHadoopCloud2017}. HDFS dirancang khusus untuk menyimpan data dalam jumlah besar dan memungkinkan pemrosesan data secara paralel. Beberapa fitur utama dari HDFS antara lain skalabilitas, toleransi kesalahan, \textit{streaming access}, dan cocok untuk aplikasi \textit{batch} seperti MapReduce.

Secara struktur, HDFS terdiri dari \textit{Name Node} sebagai \textit{node master} yang mengelola \textit{metadata} dan \textit{namespace}, serta \textit{Data Node} sebagai \textit{node slave} yang bertugas menyimpan data sebenarnya dalam bentuk blok seperti pada Gambar \ref{fig:hdfs-arch}. Berkas pada HDFS dipartisi menjadi satu atau lebih blok berukuran 64 MB atau 128 MB, kemudian didistribusikan dan disimpan di beberapa \textit{Data Node}. Setiap blok direplikasi di \textit{Data Node} yang berbeda untuk toleransi kesalahan. Replikasi blok di \textit{node/rack} yang berbeda juga meningkatkan ketersediaan HDFS.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/hdfsarchitecture}
    \caption{Arsitektur HDFS \cite{ApacheHadoopHDFS}}
    \label{fig:hdfs-arch}
\end{figure}

\subsection{Hadoop YARN}
\textit{Hadoop YARN} (\textit{Yet Another Resource Negotiator}) adalah manajer sumber daya dan sistem penjadwalan untuk klaster Hadoop. Komponen ini diperkenalkan dalam Hadoop 2.x sebagai evolusi dari Hadoop MapReduce 1.x, yang mengintegrasikan manajemen sumber daya dan pemrosesan data dalam satu sistem. YARN memungkinkan klaster untuk menjalankan berbagai aplikasi secara bersamaan dengan efisiensi yang lebih baik. YARN memisahkan fungsi manajemen sumber daya dari mekanisme pemrosesan data, yang sebelumnya keduanya tertanam dalam MapReduce. Dengan demikian, YARN dapat mendukung berbagai paradigma pemrosesan data di atas Hadoop, selain MapReduce, seperti \textit{real-time processing} dan \textit{graph processing}.

Struktur utama YARN terdiri dari \textit{Resource Manager} yang bertugas mengoordinasikan alokasi sumber daya di seluruh klaster, dan \textit{Node Manager} yang berjalan di setiap \textit{node} untuk mengawasi penggunaan sumber daya dan mengelola \textit{container} tempat aplikasi dijalankan. \textit{Application Master} adalah komponen khusus yang bertanggung jawab untuk negosiasi sumber daya dengan \textit{Resource Manager} dan bekerja dengan \textit{Node Manager} untuk menjalankan dan memantau \textit{tasks} seperti pada Gambar \ref{fig:yarn_arch}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/yarn-arch}
    \caption{Arsitektur YARN \cite{ApacheHadoopApache}}
    \label{fig:yarn_arch}
\end{figure}

\section{Apache Spark}
Apache Spark diperkenalkan oleh \textit{Apache Software Foundation} sebagai \textit{framework} pemrosesan data paralel \textit{open-source} yang dirancang untuk mempercepat pemrosesan \textit{big data} dibandingkan dengan  Hadoop MapReduce \cite{ApacheSparkUnified}. Meskipun sama-sama menggunakan model pemrosesan MapReduce, Spark bukanlah hasil modifikasi dari Hadoop MapReduce. Hal ini dikarenakan Spark menggunakan teknologi tersendiri yaitu \textit{Resilient Distributed Datasets} (RDDs) yang memungkinkan Spark memproses data secara \textit{in-memory} sehingga lebih cepat. Selain itu, Spark memiliki klaster pengolahan data tersendiri sehingga dapat berjalan independen tanpa Hadoop. Dengan performa tinggi serta dukungan untuk pemrosesan data secara interaktif, Spark banyak digunakan untuk pemrosesan data skala besar. 

\subsection{Arsitektur Spark}

Arsitektur Spark dirancang untuk pemrosesan data terdistribusi yang efisien dan cepat seperti pada Gambar \ref{fig:spark-arch}. Komponen utamanya meliputi \textit{Spark Driver, Cluster Manager}, dan \textit{Spark Executor}. \textit{Spark Driver} berperan sebagai otak operasi, bertanggung jawab untuk mengonversi program pengguna menjadi tugas-tugas, menjadwalkan tugas pada \textit{executor}, dan mengelola keseluruhan alur kerja. \textit{Cluster Manager}, yang dapat berupa YARN, Mesos, atau mode \textit{standalone} Spark, menangani alokasi sumber daya dan peluncuran \textit{executor} pada \textit{node-node cluster}. \textit{Spark Executor}, yang berjalan pada \textit{node-node cluster}, menjalankan tugas-tugas pemrosesan data yang diberikan oleh \textit{driver} dan menyediakan penyimpanan dalam memori untuk data yang di-\textit{cache}. Interaksi antara komponen-komponen ini memungkinkan pemrosesan data paralel yang cepat dan toleransi kesalahan yang tinggi. Arsitektur Spark yang fleksibel mendukung berbagai bahasa pemrograman dan sistem penyimpanan data, menjadikannya solusi ideal untuk berbagai kasus penggunaan data besar.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/ch02/spark-arch.jpeg}
    \caption{Arsitektur Spark}
    \label{fig:spark-arch}
\end{figure}

\subsection{Keterbatasan \textit{Data Sharing} pada MapReduce}

MapReduce, sebagai kerangka kerja pemrosesan data terdistribusi, mengandalkan sistem penyimpanan eksternal yang stabil, seperti HDFS, untuk berbagi data antar tugas (\textit{job}). Hal ini mengakibatkan inefisiensi karena beberapa alasan, yaitu:

\begin{enumerate}
    \item Replikasi Data. Data perlu direplikasi ke beberapa \textit{node} untuk toleransi kesalahan dan paralelisme. Replikasi ini memakan waktu dan \textit{bandwidth} jaringan.
    \item Serialisasi/Deserialisasi. Data perlu diubah formatnya (serialisasi) sebelum dikirim melalui jaringan dan diubah kembali (deserialisasi) di simpul tujuan. Proses ini menambah beban komputasi.
    \item \textit{Disk} I/O. Akses data dari dan ke \textit{disk} cenderung lambat dibandingkan dengan akses memori. Pada MapReduce, setiap operasi baca-tulis data melibatkan interaksi dengan \textit{disk}, yang memperlambat performa.
\end{enumerate}

Keterbatasan ini terlihat jelas pada aplikasi yang membutuhkan operasi iteratif, dimana hasil antara satu tugas perlu digunakan kembali oleh tugas berikutnya. Pada MapReduce, setiap iterasi memerlukan pembacaan dan penulisan data ke HDFS, seperti yang diilustrasikan pada Gambar \ref{fig:iterative_operations_on_mapreduce}. Akibatnya, aplikasi iteratif pada MapReduce cenderung lambat dan tidak efisien.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/ch02/iterative_operations_on_mapreduce}
    \caption{\textit{Data Sharing} pada MapReduce \cite{ApacheSparkIntroduction}}
    \label{fig:iterative_operations_on_mapreduce}
\end{figure}

\subsection{Solusi \textit{Data Sharing} dengan Spark RDD}

Spark mengatasi keterbatasan MapReduce dengan memperkenalkan RDD, yaitu koleksi data terdistribusi yang disimpan dalam memori. RDD bersifat \textit{immutable}, artinya data tidak dapat diubah setelah dibuat, dan \textit{fault-tolerant}, artinya data dapat dipulihkan jika terjadi kegagalan \textit{node}.

Dengan menyimpan data dalam memori, RDD memungkinkan akses data yang jauh lebih cepat dibandingkan dengan akses disk pada MapReduce. Selain itu, RDD mendukung \textit{lazy evaluation}, dimana operasi pada RDD tidak dieksekusi langsung, melainkan disimpan sebagai \textit{lineage} atau urutan operasi yang perlu dilakukan. Hal ini memungkinkan Spark untuk mengoptimalkan eksekusi tugas dan mengurangi overhead komputasi.

Pada aplikasi iteratif, RDD dapat menyimpan hasil antara dalam memori dan membagikannya antar tugas tanpa perlu mengakses \textit{disk}, seperti yang ditunjukkan pada Gambar \ref{fig:iterative_operations_on_spark_rdd}. Dengan demikian, Spark RDD memungkinkan eksekusi aplikasi iteratif yang jauh lebih cepat dan efisien dibandingkan dengan MapReduce.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/ch02/iterative_operations_on_spark_rdd}
    \caption{\textit{Data Sharing} pada RDD \cite{ApacheSparkIntroduction}}
    \label{fig:iterative_operations_on_spark_rdd}
\end{figure}

\section{HiBench}
HiBench memudahkan dalam eksekusi pengukuran berbagai beban kerja karena HiBench sudah membungkus sekumpulan perintah dalam bentuk \textit{shell script}\cite{samadiPerformanceComparisonHadoop2018}. Pengguna hanya perlu menjalankan perintah untuk HiBench melakukan persiapan data. Selanjutnya, pengguna bisa langsung melakukan pengukuran beban kerja. Hasilnya dapat terlihat langsung pada laporan HiBench. Secara umum, alur kerja HiBench terlihat seperti pada Gambar \ref{fig:hibench-process-flow}.

HiBench terdiri dari 3 proses utama. Proses pertama, pengguna melakukan konfigurasi parameter \textit{Data Generation}. Selanjutnya, \textit{Data Generation} akan melakukan pembentukan data yang nantinya akan disimpan pada \textit{Distributed File System} (DFS). Data ini yang akan digunakan pada proses selanjutnya. Proses kedua adalah proses eksekusi. Pengguna akan memicu salah satu beban kerja pada HiBench. Selanjutnya, HiBench akan memberi perintah kepada perangkat lunak (Hadoop/Spark) untuk menjalankan beban kerja tersebut. Setiap melakukan pengukuran, data yang digunakan adalah data dari \textit{Distributed File System} yang sebelumnya sudah dibentuk. Hasil dari eksekusi ini akan disimpan kembali di DFS. Proses terakhir adalah proses pembentukan laporan. Hasil dari proses sebelumnya akan diambil serta akan dibuatkan laporan secara otomatis. Dalam laporan otomatis yang diberikan oleh HiBench, terdapat beberapa metriks yang tersedia, meliputi \textit{Execution Time} dan \textit{Throughput}. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{figures/ch02/hibench-flow}
    \caption{Proses yang Terjadi di HiBench \cite{barosenAnalysisComparisonInterfacing2018}}
    \label{fig:hibench-process-flow}
\end{figure}

\subsection{\textit{Data Generation} pada \textit{Sort} dan \textit{Word Count}}
\textit{Data generation} merupakan tahapan krusial dalam \textit{benchmark} menggunakan HiBench, khususnya untuk beban kerja \textit{sort} dan \textit{word count}. Tahapan ini bertanggung jawab untuk membentuk data acak yang akan diproses oleh kedua beban kerja tersebut. Tujuannya adalah untuk menyimulasikan skenario nyata dengan data masukan yang bervolume besar dan beragam.

Pada HiBench, skrip persiapan seperti pada Algoritma \ref{algo:data-gen} berperan penting dalam menyiapkan data beban kerja. Skrip ini mengeksekusi program \textit{Random Text Writer} yang terdapat dalam paket Hadoop. \textit{Random Text Writer} menghasilkan sekumpulan data acak yang terdiri dari kata-kata yang diambil dari daftar kata yang telah ditentukan. Jumlah data yang dihasilkan, jumlah \textit{map}, dan jumlah \textit{reduce} dapat dikonfigurasi melalui parameter-parameter yang diberikan kepada skrip tersebut.

\begin{lstlisting}[caption=Skrip HiBench pada Tahap \textit{Data Generation}, label=algo:data-gen]
#!/bin/bash

START_TIME=`timestamp`
run_hadoop_job ${HADOOP_EXAMPLES_JAR} randomtextwriter \
    -D mapreduce.randomtextwriter.totalbytes=${DATASIZE} \
    -D mapreduce.randomtextwriter.bytespermap=$(( ${DATASIZE} /\
    	 ${NUM_MAPS} )) \
    -D mapreduce.job.maps=${NUM_MAPS} \
    -D mapreduce.job.reduces=${NUM_REDS} \
    ${INPUT_HDFS}
END_TIME=`timestamp`
\end{lstlisting}

Dalam penerapannya, \textit{Random Text Writer} akan menghitung jumlah total \textit{map task} yang diperlukan berdasarkan konfigurasi dan status klaster Hadoop. Setiap \textit{map task} akan menulis data hingga mencapai jumlah bita yang telah dikonfigurasi, menghasilkan kata acak sesuai dengan panjang yang telah ditentukan. Kata acak tersebut berasal dari kamus (\textit{dictionary}) yang sebelumnya sudah didefinisikan seperti pada Gambar \ref{fig:contoh-data}.

Jika diberikan input yang pasti, misalnya 100 KB, \textit{Random Text Writer} menghasilkan keluaran dengan ukuran tersebut tanpa pemotongan kata karena \textit{mapper} menghasilkan kata acak dalam kata lengkap dan menghitung panjang kata-kata tersebut sebelum menulisnya.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/contoh-data}
    \caption{Contoh Kamus pada \textit{Random Text Writer}}
    \label{fig:contoh-data}
\end{figure}

\subsection{Beban Kerja \textit{Sort}}
\textit{Sort} adalah algoritma yang umum digunakan untuk mengurutkan data berdasarkan kriteria tertentu. Algoritma ini menerima data dalam bentuk acak sebagai masukan, dan menghasilkan data yang terurut sebagai keluaran seperti pada Gambar \ref{fig:sample-sort}. Data masukan dan keluaran memiliki ukuran yang sama, sehingga beban kerja sort tidak menghasilkan pengurangan data.
Kompleksitas algoritma \textit{sort} bervariasi, tetapi umumnya membutuhkan perbandingan dan pertukaran elemen data yang intensif. Oleh karena itu, beban kerja \textit{sort} cenderung bersifat I/O \textit{bound}, dengan pemanfaatan CPU yang rendah dan penggunaan I/O yang tinggi. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ch02/sample-sort}
    \caption{Contoh Input dan Output \textit{Sort}}
    \label{fig:sample-sort}
\end{figure}

\subsection{Beban Kerja \textit{Word Count}}
\textit{Word count} adalah algoritma sederhana untuk membaca berkas teks, dan menghitung jumlah kemunculan kata-kata pada file tersebut. Pada algoritma ini, masukannya berupa berkas teks dan keluarannya berupa pasangan kata-kata dan jumlah kemunculannya. Beban kerja \textit{word count} akan menghasilkan data keluaran yang lebih kecil dari pada data masukan seperti pada Gambar \ref{fig:sample-wordcount}.
Karena itu, \textit{word count} memiliki sifat \textit{CPU Bound} yang nantinya akan ditandai dengan tingkat penggunaan CPU yang tinggi dan penggunaan I/O ringan. Selain itu, perilakunya akan tetap sama bahkan pada klaster yang lebih besar.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ch02/sample-wordcount.png}
    \caption{Contoh Input dan Output \textit{Word Count}}
    \label{fig:sample-wordcount}
\end{figure}

Implementasi MapReduce pada \textit{word count} dapat dilihat pada Gambar \ref{fig:mapreduce-wordcount}. Pada proses MapReduce, data masukan akan melalui beberapa tahapan pemrosesan. Pertama, data akan dipecah menjadi bagian-bagian yang lebih kecil pada proses pemecahan data masukan (\textit{splitting}). Dalam kasus Hadoop MapReduce, data idealnya akan dipecah menjadi beberapa blok berukuran maksimal 128 MB.

Kemudian, blok data tersebut akan diproses lebih lanjut pada tahap pemetaan (\textit{mapping}). Pemetaan merupakan salah satu tahapan terpenting dalam MapReduce. Pada tahap ini, blok data yang sudah dipecah akan diproses untuk menghasilkan pasangan kunci-nilai (\textit{key-value pairs}) sementara, seperti pada contoh kasus \textit{word count} yang menghasilkan pasangan kunci-nilai \textit{Dear:1, Bear:1, dan River:1}. Pemetaan dapat melibatkan satu atau beberapa mesin pekerja (\textit{worker}) yang memproses blok data secara paralel.

Selanjutnya adalah tahap pengocokan (\textit{shuffling}) dimana pasangan kunci-nilai hasil pemetaan yang tersebar di beberapa mesin akan dikumpulkan berdasarkan kesamaan kuncinya agar bisa diproses lebih lanjut. Misalnya semua pasangan dengan kunci \textit{Bear} dikumpulkan dalam satu mesin.

Pada tahap terakhir yaitu pengurangan (\textit{reducing}), dilakukan agregasi terhadap pasangan kunci-nilai dengan kunci yang sama untuk menghasilkan keluaran akhir. Seperti pada contoh kasus \textit{word count}, pasangan \textit{Bear:1} dan \textit{Bear:1} akan dijumlahkan menjadi \textit{Bear:2} oleh proses pengurangan.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/ch02/map-reduce-word-count-oreilly.png}
    \caption{Implementasi MapReduce pada Word Count \cite{MapReduceDistributedComputing}}
    \label{fig:mapreduce-wordcount}
\end{figure}


%\subsection{Word Count - Hadoop}
%\begin{algorithm}[H]
%\caption{Word Count MapReduce}
%\SetKwInOut{Masukan}{Masukan}
%\SetKwInOut{Keluaran}{Keluaran}
%
%\Masukan{Berkas teks dengan setiap baris sebagai kalimat.}
%\Keluaran{Berkas sequence dengan setiap baris sebagai (kata, jumlah).}
%\BlankLine
%\textbf{Tahap Map:}\\
%\For{setiap baris $baris$ dalam teks masukan}{
%    $kata \gets tokenisasi(baris)$\;
%    \For{setiap kata $k$ dalam $kata$}{
%        Keluarkan($k$, 1)\;
%    }
%}
%\BlankLine
%\textbf{Tahap Combine (Opsional):}\\
%\For{setiap pasangan kunci-nilai ($k$, [$j_1, j_2, ..., j_n$])}{
%    $jumlah \gets \sum_{i=1}^{n} j_i$\;
%    Keluarkan($k$, $jumlah$)\;
%}
%\BlankLine
%\textbf{Tahap Reduce:}\\
%\For{setiap pasangan kunci-nilai ($k$, [$j_1, j_2, ..., j_n$])}{
%    $jumlah\_total \gets \sum_{i=1}^{n} j_i$\;
%    Keluarkan($k$, $jumlah\_total$)\;
%}
%\end{algorithm}
%
%\subsection{Word Count - Spark}
%\begin{algorithm}[H]
%\caption{Scala Word Count}
%\SetKwInOut{Masukan}{Masukan}
%\SetKwInOut{Keluaran}{Keluaran}
%
%\Masukan{Berkas teks pada HDFS dengan setiap baris sebagai kalimat.}
%\Keluaran{Berkas teks pada HDFS dengan setiap baris sebagai (kata, jumlah).}
%\BlankLine
%1. Inisialisasi SparkContext\;
%2. $data \gets$ Muat data teks dari HDFS\;
%3. $kata \gets$ Pisahkan setiap baris dalam $data$ menjadi kata-kata individual\;
%4. $pasangan \gets$ Ubah setiap kata menjadi pasangan (kata, 1)\;
%5. $jumlah \gets$ Jumlahkan nilai untuk setiap kata menggunakan `reduceByKey`\;
%6. Simpan $jumlah$ ke HDFS\;
%7. Hentikan SparkContext\;
%\end{algorithm}


%\subsection{Beban Kerja Sort}
%\blindtext

%\subsection{Sort - Hadoop}
%\begin{algorithm}[H]
%\caption{Sort Hadoop MapReduce}
%\SetKwInOut{Masukan}{Masukan}
%\SetKwInOut{Keluaran}{Keluaran}
%
%\Masukan{Data pada HDFS (format dapat ditentukan)}
%\Keluaran{Data terurut pada HDFS (format dapat ditentukan)}
%\BlankLine
%\textbf{Inisialisasi:}\\
%1. Inisialisasi konfigurasi dan JobClient\;
%2. Tentukan jumlah reducer berdasarkan konfigurasi dan kapasitas cluster\;
%3. Tentukan format input, format output, kelas kunci output, dan kelas nilai output (dapat ditentukan pengguna)\;
%4. Buat objek Job dan atur konfigurasi dasar (nama, jar, mapper, reducer, jumlah reducer, format input/output, kelas kunci/nilai output)\;
%5. Atur lokasi input dan output pada HDFS\;
%6. \If{pengguna meminta total-order sort}{
%    Lakukan sampling data input\;
%    Buat berkas partisi untuk total-order sort\;
%    Atur konfigurasi total-order sort pada job\;
%}
%\BlankLine
%\textbf{Tahap Map:}\\
%\For{setiap masukan (kunci, nilai)}{
%    Emit (keluarkan) pasangan (kunci, nilai)\;
%}
%\BlankLine
%\textbf{Tahap Reduce:}\\
%\For{setiap (kunci, [nilai1, nilai2, ...])}{
%    \For{setiap nilai $v$ dalam daftar nilai}{
%        Emit (keluarkan) pasangan (kunci, $v$)\;
%    }
%}
%\BlankLine
%\textbf{Eksekusi:}\\
%7. Cetak informasi job (node, lokasi input/output, jumlah reducer)\;
%8. Jalankan job dan tunggu hingga selesai\;
%9. Cetak waktu eksekusi job\;
%\end{algorithm}

%\subsection{Sort - Spark}
%\begin{algorithm}[H]
%\caption{Scala Sort}
%\SetKwInOut{Masukan}{Masukan}
%\SetKwInOut{Keluaran}{Keluaran}
%
%\Masukan{Berkas teks pada HDFS}
%\Keluaran{Berkas teks terurut pada HDFS}
%\BlankLine
%1. Inisialisasi SparkContext\;
%2. Tentukan jumlah partisi untuk pengurutan\;
%3. $data \gets$ Muat data teks dari HDFS dan ubah setiap baris menjadi pasangan (baris, 1)\;
%4. Buat objek HashPartitioner dengan jumlah partisi yang ditentukan\;
%5. $terurut \gets$ Urutkan $data$ berdasarkan kunci menggunakan `sortByKeyWithPartitioner` dengan HashPartitioner\;
%6. Ambil nilai dari setiap pasangan terurut (yaitu, hanya teks baris)\;
%7. Simpan data terurut $terurut$ ke HDFS\;
%8. Hentikan SparkContext\;
%\end{algorithm}

\section{Data Keluaran HiBench dan Dool}
Diagram pada Gambar \ref{fig:output-hibench-dool} mengilustrasikan data keluaran dari dua alat, yaitu HiBench dan Dool, yang digunakan untuk mengevaluasi performa sistem. HiBench berfokus pada pengukuran kinerja keseluruhan dari suatu beban kerja (\textit{benchmark}), sementara Dool memberikan pemantauan sistem yang terperinci secara \textit{real-time}.

\subsection{HiBench \textit{Report}}
HiBench menghasilkan laporan yang mencakup dua metrik utama, yaitu
\begin{enumerate}
	\item \textbf{Waktu Eksekusi (\textit{Execution Time})}: Waktu eksekusi suatu proses dapat dihitung dengan mengambil perbedaan antara waktu akhir dan waktu awal dari proses tersebut dalam format \textit{timestamp}. \textit{Timestamp} yang digunakan adalah dalam bentuk \textit{Unix time}, yaitu representasi waktu sebagai jumlah detik yang telah berlalu sejak 1 Januari 1970 UTC. Untuk mendapatkan waktu eksekusi, pertama-tama catat waktu awal proses menggunakan \textit{Timestamp Unix}. Setelah proses selesai, catat waktu akhirnya dengan cara yang sama. Selanjutnya, kurangi waktu awal dari waktu akhir untuk memperoleh durasi eksekusi dalam satuan detik.
	\item \textbf{\textit{Throughput}}: Mengukur jumlah data yang diproses per satuan waktu, biasanya dalam bita (\textit{byte}) per detik. Metrik ini mencerminkan efisiensi sistem dalam menangani beban kerja. Ilustrasi \textit{Throughput} terlihat seperti pada Gambar \ref{fig:ilustrasi-throughput}
\end{enumerate}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/ch02/ilustrasi-lalu-lintas-jaringan.png}
    \caption{Ilustrasi \textit{Throughput} \cite{PenjelasanApaItu2023}}
    \label{fig:ilustrasi-throughput}
\end{figure}



\subsection{Dool: \textit{System Monitoring}}
Dool menyediakan pemantauan sistem yang mendetail dan diperbarui setiap detik. Data keluaran Dool meliputi berbagai aspek kinerja sistem, antara lain:
\begin{enumerate}
	\item \textbf{\textit{Time}}: \textit{Timestamp} yang menunjukkan waktu pengambilan data.
	\item \textbf{\textit{Date}}: Tanggal pengambilan data. 
	\item \textbf{\textit{Type}}: Jenis pengukuran yang dilakukan, misalnya CPU, memori, disk, atau jaringan. 
	\item \textbf{io/total}: Total aktivitas input/output (I/O) pada \textit{disk}, diukur dalam jumlah operasi I/O per detik.
	\item \textbf{\textit{total cpu usage}}: Persentase penggunaan CPU secara keseluruhan. 
		\begin{enumerate}
		\item \textbf{\textit{cpu/user}}: Persentase waktu CPU yang digunakan oleh proses-proses di ruang pengguna (\textit{user space}). Ini mencerminkan aktivitas dari aplikasi-aplikasi dan proses-proses yang dijalankan oleh pengguna.
		\item \textbf{\textit{cpu/wait}}: Persentase waktu CPU yang dihabiskan untuk menunggu operasi I/O selesai. Angka yang tinggi pada metrik ini bisa menunjukkan adanya \textit{bottleneck} pada disk atau jaringan yang menyebabkan CPU harus menunggu data tersedia.
		\end{enumerate}
	\item \textbf{\textit{disk/total}}: Total aktivitas \textit{disk}, mencakup baca dan tulis, diukur dalam bita per detik.
		\begin{enumerate}
		\item \textbf{\textit{disk/read}}: Jumlah data yang dibaca dari \textit{disk}, diukur dalam bita per detik. Metrik ini penting untuk mengidentifikasi seberapa banyak beban pembacaan yang diterima oleh sistem penyimpanan.
		\item \textbf{\textit{disk/write}}: Jumlah data yang ditulis ke \textit{disk}, diukur dalam bita per detik. Metrik ini menunjukkan seberapa banyak data yang sedang ditulis ke penyimpanan, yang bisa mempengaruhi kinerja sistem jika terlalu tinggi.
		\end{enumerate}
	\item \textbf{\textit{memory usage}}: Jumlah memori yang sedang digunakan oleh sistem.
	\item \textbf{net/total}: Total aktivitas jaringan, mencakup data yang dikirim dan diterima, diukur dalam bita per detik.
\end{enumerate}

HiBench memberikan gambaran umum tentang efisiensi sistem dalam menangani beban kerja tertentu, sementara Dool memungkinkan untuk memantau berbagai komponen sistem secara\textit{ real-time} dan mengidentifikasi potensi \textit{bottleneck} atau masalah kinerja.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{figures/ch02/output-hibench-dool.png}
    \caption{Data Keluaran HiBench dan Dool}
    \label{fig:output-hibench-dool}
\end{figure}
