@article{ahmadvandGapproxUsingGallup2019,
  title = {Gapprox: Using {{Gallup}} Approach for Approximation in {{Big Data}} Processing},
  shorttitle = {Gapprox},
  author = {Ahmadvand, Hossein and Goudarzi, Maziar and Foroutan, Fouzhan},
  date = {2019-02-26},
  journaltitle = {Journal of Big Data},
  shortjournal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {20},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0185-4},
  urldate = {2023-09-29},
  abstract = {As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various parts of data to have different impact on the final result. To address this problem, we develop a data variety aware approximation approach called Gapprox. Our idea is to use a kind of cluster sampling to improve the accuracy of estimation. Our approach can decrease the amount of data to be processed to achieve the desired Quality of Result with acceptable error bound and confidence interval. We divide the input data into some blocks considering the intra/inter cluster variance. The size of the block and the sample size are determined in such a way that by processing small amount of input data, an acceptable confidence interval and error bound is achieved. We compared our work with two well-known state of the art. The experimental results show that our result surpasses the state of the art and improve processing time up to 17× compared to ApproxHadoop and 8× compared to Sapprox when the user can tolerate an error of 5\% with 95\% confidence.},
  keywords = {Approximation,Cluster sampling,Data variety,Quality of Result},
  file = {/Users/dimasws/Zotero/storage/XEUXFUIQ/Ahmadvand et al. - 2019 - Gapprox using Gallup approach for approximation i.pdf;/Users/dimasws/Zotero/storage/J5W3YNWV/s40537-019-0185-4.html}
}

@article{ahnPerformanceStudySpark2018,
  title = {Performance {{Study}} of {{Spark}} on {{YARN Cluster Using HiBench}}},
  author = {Ahn, HooYoung and Kim, Hyunjae and You, Woongshik},
  date = {2018},
  journaltitle = {IEEE International Conference on Consumer Electronics},
  abstract = {Recently, various kinds of Internet-of-Things (IoT) solutions and services are provided such as smart industry, smart city, smart factory, smart agriculture and etc. Those solutions and services generate large amount of data from various devices which are connected through networks while they communicate with each other. However, it is a difficult problem to process the fast and massively produced data efficiently. To solve the problems in the framework level, there are many open-source big data processing and analysis frameworks. To process large-scale data in a fast manner, those frameworks use a cluster consisting of multiple computing machines. However, to set the framework running on large-scale cluster properly is not simple and it is difficult to verify its performance in the distributed environment. In this paper, we evaluate the performance of Apache Spark which is one of the most popular big data processing and analysis frameworks. Especially, we conduct experiments by using a representative benchmark tool, called HiBench, and large-scale data in the cluster environment. From the experimental results, we can conclude that Spark is highly scalable for distributed machine learning as well as big data processing.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/LPBF89UW/Ahn et al. - 2018 - Performance Study of Spark on YARN Cluster Using H.pdf}
}

@article{aminudinPengukuranPerformaApache2019,
  title = {Pengukuran Performa Apache Spark dengan Library H2O Menggunakan Benchmark Hibench Berbasis Cloud Computing},
  author = {Aminudin, Aminudin and Cahyono, Eko Budi},
  date = {2019-10-08},
  journaltitle = {Jurnal Teknologi Informasi dan Ilmu Komputer},
  volume = {6},
  number = {5},
  pages = {519--526},
  issn = {2528-6579},
  doi = {10.25126/jtiik.2019651520},
  urldate = {2023-09-21},
  abstract = {Apache Spark merupakan platform yang dapat digunakan untuk memproses data dengan ukuran data yang relatif~ besar (big data) dengan kemampuan untuk membagi data tersebut ke masing-masing cluster yang telah ditentukan konsep ini disebut dengan parallel komputing. Apache Spark mempunyai kelebihan dibandingkan dengan framework lain yang serupa misalnya Apache Hadoop dll, di mana Apache Spark mampu memproses data secara streaming artinya data yang masuk ke dalam lingkungan Apache Spark dapat langsung diproses tanpa menunggu data lain terkumpul. Agar di dalam Apache Spark mampu melakukan proses machine learning, maka di dalam paper ini akan dilakukan eksperimen yaitu dengan mengintegrasikan Apache Spark yang bertindak sebagai lingkungan pemrosesan data yang besar dan konsep parallel komputing akan dikombinasikan dengan library H2O yang khusus untuk menangani pemrosesan data menggunakan algoritme machine learning. Berdasarkan hasil pengujian Apache Spark di dalam lingkungan cloud computing, Apache Spark mampu memproses data cuaca yang didapatkan dari arsip data cuaca terbesar yaitu yaitu data NCDC dengan ukuran data sampai dengan 6GB. Data tersebut diproses menggunakan salah satu model machine learning yaitu deep learning dengan membagi beberapa node yang telah terbentuk di lingkungan cloud computing dengan memanfaatkan library H2O. Keberhasilan tersebut dapat dilihat dari parameter pengujian yang telah diujikan meliputi nilai running time, throughput, Avarege Memory dan Average CPU yang didapatkan dari Benchmark Hibench. Semua nilai tersebut ~dipengaruhi oleh banyaknya data dan jumlah node.~AbstractApache Spark is a platform that can be used to process data with relatively large data sizes (big data) with the ability to divide the data into each cluster that has been determined. This concept is called parallel computing. Apache Spark has advantages compared to other similar frameworks such as Apache Hadoop, etc., where Apache Spark is able to process data in streaming, meaning that the data entered into the Apache Spark environment can be directly processed without waiting for other data to be collected. In order for Apache Spark to be able to do machine learning processes, in this paper an experiment will be conducted that integrates Apache Spark which acts as a large data processing environment and the concept of parallel computing will be combined with H2O libraries specifically for handling data processing using machine learning algorithms . Based on the results of testing Apache Spark in a cloud computing environment, Apache Spark is able to process weather data obtained from the largest weather data archive, namely NCDC data with data sizes up to 6GB. The data is processed using one of the machine learning models namely deep learning by dividing several nodes that have been formed in the cloud computing environment by utilizing the H2O library. The success can be seen from the test parameters that have been tested including the value of running time, throughput, Avarege Memory and CPU Average obtained from the Hibench Benchmark. All these values are influenced by the amount of data and number of nodes.},
  issue = {5},
  langid = {indonesian},
  file = {/Users/dimasws/Zotero/storage/WJ5EJWQA/Aminudin and Cahyono - 2019 - Pengukuran Performa Apache Spark dengan Library H2.pdf}
}

@online{ComprehensivePerformanceAnalysis,
  title = {A Comprehensive Performance Analysis of {{Apache Hadoop}} and {{Apache Spark}} for Large Scale Data Sets Using {{HiBench}} | {{Journal}} of {{Big Data}} | {{Full Text}}},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00388-5#Sec9},
  urldate = {2023-09-21},
  keywords = {perbandingan},
  file = {/Users/dimasws/Zotero/storage/JP28856C/A comprehensive performance analysis of Apache Had.pdf;/Users/dimasws/Zotero/storage/GXRZ28AG/s40537-020-00388-5.html}
}

@inproceedings{deanMapReduceSimplifiedData2004,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  booktitle = {Proceedings of the 6th Conference on {{Symposium}} on {{Operating Systems Design}} \& {{Implementation}} - {{Volume}} 6},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  date = {2004-12-06},
  series = {{{OSDI}}'04},
  pages = {10},
  publisher = {{USENIX Association}},
  location = {{USA}},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.}
}

@article{enesBigDataOrientedPaaS2018,
  title = {Big {{Data-Oriented PaaS Architecture}} with {{Disk-as-a-Resource Capability}} and {{Container-Based Virtualization}}},
  author = {Enes, Jonatan and Cacheiro, Javier López and Expósito, Roberto R. and Touriño, Juan},
  date = {2018-12},
  journaltitle = {Journal of Grid Computing},
  shortjournal = {J Grid Computing},
  volume = {16},
  number = {4},
  pages = {587--605},
  issn = {1570-7873, 1572-9184},
  doi = {10.1007/s10723-018-9460-4},
  urldate = {2023-09-08},
  langid = {english},
  file = {/Users/dimasws/Downloads/enes2018.pdf}
}

@article{huangHiBenchBenchmarkSuite,
  title = {The {{HiBench Benchmark Suite}}: {{Characterization}} of the {{MapReduce-Based Data Analysis}}},
  author = {Huang, Shengsheng and Huang, Jie and Dai, Jinquan and Xie, Tao and Huang, Bo},
  abstract = {The MapReduce model is becoming prominent for the large-scale data analysis in the cloud. In this paper, we present the benchmarking, evaluation and characterization of Hadoop, an open-source implementation of MapReduce. We first introduce HiBench, a new benchmark suite for Hadoop. It consists of a set of Hadoop programs, including both synthetic micro-benchmarks and real-world Hadoop applications. We then evaluate and characterize the Hadoop framework using HiBench, in terms of speed (i.e., job running time), throughput (i.e., the number of tasks completed per minute), HDFS bandwidth, system resource (e.g., CPU, memory and I/O) utilizations, and data access patterns.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/PVZNPQJD/Huang et al. - The HiBench Benchmark Suite Characterization of t.pdf}
}

@inproceedings{imashevaPracticeMovingBig2020,
  title = {The {{Practice}} of {{Moving}} to {{Big Data}} on the {{Case}} of the {{NoSQL Database}}, {{Clickhouse}}},
  booktitle = {Optimization of {{Complex Systems}}: {{Theory}}, {{Models}}, {{Algorithms}} and {{Applications}}},
  author = {Imasheva, Baktagul and Azamat, Nakispekov and Sidelkovskiy, Andrey and Sidelkovskaya, Ainur},
  editor = {Le Thi, Hoai An and Le, Hoai Minh and Pham Dinh, Tao},
  date = {2020},
  series = {Advances in {{Intelligent Systems}} and {{Computing}}},
  pages = {820--828},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-21803-4_82},
  abstract = {In the modern world, every technology and user generate a large amount of data. Each data carries value to some degree. Therefore, the concept of big data is actively developing because the idea of big data is to generate a new value. Addressing big data is an invocation and time-demanding job that needs a large computational infrastructure to ensure successful data processing, storage, and analysis. This report is intended to compare how one of the big data storage, Clickhouse, can replace the relational database, Oracle. This paper motivation is to obtain an understanding of the benefit and drawbacks of NoSQL database, in the case of Clickhouse to supporting a huge amount of data.},
  isbn = {978-3-030-21803-4},
  langid = {english},
  keywords = {Big data,Big data value chain,Clickhouse,Column database,Data storage,NoSQL}
}

@article{kimDesignImplementationCloud2022,
  title = {Design and {{Implementation}} of {{Cloud Docker Application Architecture Based}} on {{Machine Learning}} in {{Container Management}} for {{Smart Manufacturing}}},
  author = {Kim, Byoung Soo and Lee, Sang Hyeop and Lee, Ye Rim and Park, Yong Hyun and Jeong, Jongpil},
  date = {2022-01},
  journaltitle = {Applied Sciences},
  volume = {12},
  number = {13},
  pages = {6737},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12136737},
  urldate = {2023-09-08},
  abstract = {Manufacturers are expanding their business-process innovation and customized manufacturing to reduce their information technology costs and increase their operational efficiency. Large companies are building enterprise-wide hybrid cloud platforms to further accelerate their digital transformation. Many companies are also introducing container virtualization technology to maximize their cloud transition and cloud benefits. However, small- and mid-sized manufacturers are struggling with their digital transformation owing to technological barriers. Herein, for small- and medium-sized manufacturing enterprises transitioning onto the cloud, we introduce a Docker Container application architecture, a customized container-based defect inspection machine-learning model for the AWS cloud environment developed for use in small manufacturing plants. By linking with open-source software, the development was improved and a datadog-based container monitoring system, built to enable real-time anomaly detection, was implemented.},
  issue = {13},
  langid = {english},
  keywords = {cloud docker,container management,docker container,machine learning,monitoring,smart manufacturing},
  file = {/Users/dimasws/Zotero/storage/3U7ES6WF/Kim et al. - 2022 - Design and Implementation of Cloud Docker Applicat.pdf}
}

@online{KOMPARASIKECEPATANHADOOP,
  title = {{{KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SPARK DALAM MENGOLAH DATA TEKS}} | {{Jurnal Ilmiah Matrik}}},
  url = {https://journal.binadarma.ac.id/index.php/jurnalmatrik/article/view/1649},
  urldate = {2023-09-21},
  file = {/Users/dimasws/Zotero/storage/VRYVYL6K/KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SP.pdf;/Users/dimasws/Zotero/storage/IZYFSQ25/1649.html}
}

@article{kozlovRealtimeDataStream,
  title = {Real-Time {{Data Stream Processing System}}},
  author = {Kozlov, Vitalij},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/KUFJH34Q/Kozlov - Real-time Data Stream Processing System.pdf}
}

@inproceedings{linLargescaleDataSet2020,
  title = {A {{Large-scale Data Set}} and an {{Empirical Study}} of {{Docker Images Hosted}} on {{Docker Hub}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Lin, Changyuan and Nadi, Sarah and Khazaei, Hamzeh},
  date = {2020-09},
  pages = {371--381},
  publisher = {{IEEE}},
  location = {{Adelaide, SA,  Australia}},
  doi = {10.1109/ICSME46990.2020.00043},
  urldate = {2023-09-13},
  abstract = {Docker is currently one of the most popular containerization solutions. Previous work investigated various characteristics of the Docker ecosystem, but has mainly focused on Dockerfiles from GitHub, limiting the type of questions that can be asked, and did not investigate evolution aspects. In this paper, we create a recent and more comprehensive data set by collecting data from Docker Hub, GitHub, and Bitbucket. Our data set contains information about 3,364,529 Docker images and 378,615 git repositories behind them. Using this data set, we conduct a large-scale empirical study with four research questions where we reproduce previously explored characteristics (e.g., popular languages and base images), investigate new characteristics such as image tagging practices, and study evolution trends. Our results demonstrate the maturity of the Docker ecosystem: we find more reliance on ready-to-use language and application base images as opposed to yet-to-be-configured OS images, a downward trend of Docker image sizes demonstrating the adoption of best practices of keeping images small, and a declining trend in the number of smells in Dockerfiles suggesting a general improvement in quality. On the downside, we find an upward trend in using obsolete OS base images, posing security risks, and find problematic usages of the latest tag, including version lagging. Overall, our results bring good news such as more developers following best practices, but they also indicate the need to build tools and infrastructure embracing new trends and addressing potential issues.},
  eventtitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  isbn = {978-1-72815-619-4},
  langid = {english},
  file = {/Users/dimasws/Downloads/lin2020.pdf}
}

@online{MathematicsFreeFullText,
  title = {Mathematics | {{Free Full-Text}} | {{Performance Evaluation}} of {{Query Plan Recommendation}} with {{Apache Hadoop}} and {{Apache Spark}}},
  url = {https://www.mdpi.com/2227-7390/10/19/3517},
  urldate = {2023-09-21}
}

@inproceedings{moravcikOverviewDockerContainer2020,
  title = {Overview of {{Docker}} Container Orchestration Tools},
  booktitle = {2020 18th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  author = {Moravcik, Marek and Kontsek, Martin},
  date = {2020-11-12},
  pages = {475--480},
  publisher = {{IEEE}},
  location = {{Košice, Slovenia}},
  doi = {10.1109/ICETA51985.2020.9379236},
  urldate = {2023-09-13},
  abstract = {The main goal of this paper is to analyze current options of Docker container orchestration. Currently, there are three widely used orchestrators - Docker Swarm, Kubernetes and OpenShift. In the paper, we analyzed all three of them, we highlighted their advantages and disadvantages and compared them.},
  eventtitle = {2020 18th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  isbn = {978-1-66542-226-0},
  langid = {english},
  file = {/Users/dimasws/Downloads/moravcik2020.pdf}
}

@article{mousaviScalableStreamProcessing,
  title = {Scalable {{Stream Processing}} and {{Management}} for {{Time Series Data}}},
  author = {Mousavi, Bamdad},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6CHBA5HA/Mousavi - Scalable Stream Processing and Management for Time.pdf}
}

@article{muthiahPerformanceEvaluationHadoopa,
  title = {Performance {{Evaluation}} of {{Hadoop}} Based {{Big Data Applications}} with {{HiBench Benchmarking}} Tool on {{IaaS Cloud Platforms}}},
  author = {Muthiah, Karthika},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/XIUJADB4/Muthiah - Performance Evaluation of Hadoop based Big Data Ap.pdf}
}

@inproceedings{naikDockerContainerbasedBig2017a,
  title = {Docker Container-Based Big Data Processing System in Multiple Clouds for Everyone},
  booktitle = {2017 {{IEEE International Systems Engineering Symposium}} ({{ISSE}})},
  author = {Naik, Nitin},
  date = {2017-10},
  pages = {1--7},
  publisher = {{IEEE}},
  location = {{Vienna, Austria}},
  doi = {10.1109/SysEng.2017.8088294},
  urldate = {2023-09-08},
  abstract = {Big data processing is progressively becoming essential for everyone to extract the meaningful information from their large volume of data irrespective of types of users and their application areas. Big data processing is a broad term and includes several operations such as the storage, cleaning, organization, modelling, analysis and presentation of data at a scale and efficiency. For ordinary users, the significant challenges are the requirement of the powerful data processing system and its provisioning, installation of complex big data analytics and difficulty in their usage. Docker is a container-based virtualization technology and it has recently introduced Docker Swarm for the development of various types of multi-cloud distributed systems, which can be helpful in solving all above problems for ordinary users. However, Docker is predominantly used in the software development industry, and less focus is given to the data processing aspect of this container-based technology. Therefore, this paper proposes the Docker container-based big data processing system in multiple clouds for everyone, which explores another potential dimension of Docker for big data analysis. This Docker container-based system is an inexpensive and user-friendly framework for everyone who has the knowledge of basic IT skills. Additionally, it can be easily developed on a single machine, multiple machines or multiple clouds. This paper demonstrates the architectural design and simulated development of the proposed Docker container-based big data processing system in multiple clouds. Subsequently, it illustrates the automated provisioning of big data clusters using two popular big data analytics, Hadoop and Pachyderm (without Hadoop) including the Web-based GUI interface Hue for easy data processing in Hadoop.},
  eventtitle = {2017 {{IEEE International Systems Engineering Symposium}} ({{ISSE}})},
  isbn = {978-1-5386-3403-5},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6XLSEFN6/Naik - 2017 - Docker container-based big data processing system .pdf}
}

@article{ruijunLightweightExperimentalPlatform2020,
  title = {A {{Lightweight Experimental Platform}} for {{Big Data Based}} on {{Docker Containers}}},
  author = {Ruijun, Gu},
  date = {2020-01},
  journaltitle = {Journal of Physics: Conference Series},
  shortjournal = {J. Phys.: Conf. Ser.},
  volume = {1437},
  number = {1},
  pages = {012104},
  publisher = {{IOP Publishing}},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1437/1/012104},
  urldate = {2023-09-08},
  abstract = {In recent years, many colleges and universities have set up the major of big data. The biggest problem in teaching is that there is no supporting basic experimental environment, and it is difficult to deploy and configure the big data environment at the same time. In addition, the lack of experimental data, experimental teaching plans and experimental manuals in the experimental process makes it difficult to carry out relevant teaching. In order to reduce the cost of laboratory construction and the difficulty of learning big data, a lightweight big data experimental platform was constructed based on virtualized container technology. Through this platform, we can create a big data cluster, provide various suitable experimental environments, focus on the technology itself, and greatly improve the learning efficiency.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/WLGIH7LQ/Ruijun - 2020 - A Lightweight Experimental Platform for Big Data B.pdf}
}

@inproceedings{samadiComparativeStudyHadoop2016,
  title = {Comparative Study between {{Hadoop}} and {{Spark}} Based on {{Hibench}} Benchmarks},
  booktitle = {2016 2nd {{International Conference}} on {{Cloud Computing Technologies}} and {{Applications}} ({{CloudTech}})},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  date = {2016-05},
  pages = {267--275},
  publisher = {{IEEE}},
  location = {{Marrakech, Morocco}},
  doi = {10.1109/CloudTech.2016.7847709},
  urldate = {2023-09-24},
  abstract = {Big Data is currently a hot topic for companies and scientists around the world, due to the emergence of new technologies, devices and communication means like social network sites, which led to a noticeable increase of the amount of data produced every year, even every day. In addition, traditional algorithms and technologies are inefficient to process, analyze and store this vast amount of data. So, to solve this problem, Big Data frameworks are needed. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks. Hadoop and Spark, which are used to efficiently process vast amount of data in parallel and distributed mode on a large clusters. Hibench benchmark suite is used to compare the performance of these two frameworks based on the criteria as execution time, throughput and speedup. Our experimental results show that Spark is more efficient than Hadoop to deal with large amount of data. However, spark requires higher memory allocation, since it loads processes into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  eventtitle = {2016 2nd {{International Conference}} on {{Cloud Computing Technologies}} and {{Applications}} ({{CloudTech}})},
  isbn = {978-1-4673-8894-8},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/VVTPYL4Z/Samadi et al. - 2016 - Comparative study between Hadoop and Spark based o.pdf}
}

@article{samadiPerformanceComparisonHadoop2018,
  title = {Performance Comparison between {{Hadoop}} and {{Spark}} Frameworks Using {{HiBench}} Benchmarks},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  date = {2018},
  journaltitle = {Concurrency and Computation: Practice and Experience},
  volume = {30},
  number = {12},
  pages = {e4367},
  issn = {1532-0634},
  doi = {10.1002/cpe.4367},
  urldate = {2023-09-21},
  abstract = {Big Data has become one of the major areas of research for cloud service providers due to a large amount of data produced every day and the inefficiency of traditional algorithms and technologies to handle these large amounts of data. Big Data with its characteristics such as volume, variety, and veracity (3V) requires efficient technologies to process in real time. To solve this problem and to process and analyze this vast amount of data, there are many powerful tools like Hadoop and Spark, which are mainly used in the context of Big Data. They work following the principles of parallel computing. The challenge is to specify which Big Data's tool is better depending on the processing context. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks deployed on virtual machines. Hadoop MapReduce and Apache Spark are used to efficiently process a vast amount of data in parallel and distributed mode on large clusters, and both of them suit for Big Data processing. We also present the execution results of Apache Hadoop in Amazon EC2, a major cloud computing environment. To compare the performance of these two frameworks, we use HiBench benchmark suite, which is an experimental approach for measuring the effectiveness of any computer system. The comparison is made based on three criteria: execution time, throughput, and speedup. We test Wordcount workload with different data sizes for more accurate results. Our experimental results show that the performance of these frameworks varies significantly based on the use case implementation. Furthermore, from our results we draw the conclusion that Spark is more efficient than Hadoop to deal with a large amount of data in major cases. However, Spark requires higher memory allocation, since it loads the data to be processed into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  langid = {english},
  keywords = {Amazon EC2,Big Data,cloud computing,Hadoop,HiBench,parallel and distributed processing,Spark},
  file = {/Users/dimasws/Zotero/storage/XGWSZMA2/Samadi et al. - 2018 - Performance comparison between Hadoop and Spark fr.pdf;/Users/dimasws/Zotero/storage/J7R8LXTP/cpe.html}
}

@article{saputroPerbandinganKinerjaKomputasi2020,
  title = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}} : {{Storm Event Database}})},
  shorttitle = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}}},
  author = {Saputro, Rendiyono and Aminuddin, Aminuddin and Munarko, Yuda},
  date = {2020-03-05},
  journaltitle = {Jurnal Repositor},
  shortjournal = {Jurnal Repositor},
  volume = {2},
  pages = {463},
  doi = {10.22219/repositor.v2i4.93},
  abstract = {Perkembangan teknologi telah mengakibatkan pertumbuhan data yang semakin cepat dan besar setiap waktunya. Hal tersebut disebabkan oleh banyaknya sumber data seperti mesin pencari, RFID, catatan transaksi digital, arsip video dan foto, user generated content, internet of things, penelitian ilmiah di berbagai bidang seperti genomika, meteorologi, astronomi, fisika, dll. Selain itu, data - data tersebut memiliki karakteristik yang unik antara satu dengan lainnya, hal ini yang menyebabkan tidak dapat diproses oleh teknologi basis data konvensional. Oleh karena itu, dikembangkan beragam framework komputasi terdistribusi seperti Apache Hadoop dan Apache Spark yang memungkinkan untuk memproses data secara terdistribusi dengan menggunakan gugus komputer.Adanya ragam framework komputasi terdistribusi, sehingga diperlukan sebuah pengujian untuk mengetahui kinerja komputasi keduanya. Pengujian dilakukan dengan memproses dataset dengan beragam ukuran dan dalam gugus komputer dengan jumlah node yang berbeda. Dari semua hasil pengujian, Apache Hadoop memerlukan waktu yang lebih sedikit dibandingkan dengan Apache Spark. Hal tersebut terjadi karena nilai throughput dan throughput/node Apache Hadoop lebih tinggi daripada Apache Spark.},
  file = {/Users/dimasws/Zotero/storage/JYJE3NEW/Saputro et al. - 2020 - Perbandingan Kinerja Komputasi Hadoop dan Spark un.pdf}
}

@article{shiClashTitansMapReduce2015,
  title = {Clash of the Titans: {{MapReduce}} vs. {{Spark}} for Large Scale Data Analytics},
  shorttitle = {Clash of the Titans},
  author = {Shi, Juwei and Qiu, Yunjie and Minhas, Umar Farooq and Jiao, Limei and Wang, Chen and Reinwald, Berthold and Özcan, Fatma},
  date = {2015-09},
  journaltitle = {Proceedings of the VLDB Endowment},
  shortjournal = {Proc. VLDB Endow.},
  volume = {8},
  number = {13},
  pages = {2110--2121},
  issn = {2150-8097},
  doi = {10.14778/2831360.2831365},
  urldate = {2023-09-28},
  abstract = {MapReduce and Spark are two very popular open source cluster computing frameworks for large scale data analytics. These frameworks hide the complexity of task parallelism and fault-tolerance, by exposing a simple programming API to users. In this paper, we evaluate the major architectural components in MapReduce and Spark frameworks including: shuffle, execution model, and caching, by using a set of important analytic workloads. To conduct a detailed analysis, we developed two profiling tools: (1) We correlate the task execution plan with the resource utilization for both MapReduce and Spark, and visually present this correlation; (2) We provide a break-down of the task execution time for in-depth analysis. Through detailed experiments, we quantify the performance differences between MapReduce and Spark. Furthermore, we attribute these performance differences to different components which are architected differently in the two frameworks. We further expose the source of these performance differences by using a set of micro-benchmark experiments. Overall, our experiments show that Spark is about 2.5x, 5x, and 5x faster than MapReduce, for Word Count, k-means, and PageRank, respectively. The main causes of these speedups are the efficiency of the hash-based aggregation component for combine, as well as reduced CPU and disk overheads due to RDD caching in Spark. An exception to this is the Sort workload, for which MapReduce is 2x faster than Spark. We show that MapReduce’s execution model is more efficient for shuffling data than Spark, thus making Sort run faster on MapReduce.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/MALM7QFA/Shi et al. - 2015 - Clash of the titans MapReduce vs. Spark for large.pdf}
}

@article{xuDeployingResearchingHadoop,
  title = {Deploying and {{Researching Hadoop}} in {{Virtual Machines}}},
  author = {Xu, Guanghui and Xu, Feng and Ma, Hongxu},
  abstract = {Hadoop's emerging and the maturity of virtualization make it feasible to combine them together to process immense data set. To do research on Hadoop in virtual environment, an experimental environment is needed. This paper firstly introduces some technologies used such as CloudStack, MapReduce and Hadoop. Based on that, a method to deploy CloudStack is given. Then we discuss how to deploy Hadoop in virtual machines which can be obtained from CloudStack by some means, then an algorithm to solve the problem that all the virtual machines which are created by CloudStack using same template have a same hostname. After that we run some Hadoop programs under the virtual cluster, which shows that it is feasible to deploying Hadoop in this way. Then some methods to optimize Hadoop in virtual machines are discussed. From this paper, readers can follow it to set up their own Hadoop experimental environment and capture the current status and trend of optimizing Hadoop in virtual environment.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6EBEWYQQ/Xu et al. - Deploying and Researching Hadoop in Virtual Machin.pdf}
}

@inproceedings{zahariaSparkClusterComputing2010,
  title = {Spark: Cluster Computing with Working Sets},
  shorttitle = {Spark},
  booktitle = {Proceedings of the 2nd {{USENIX}} Conference on {{Hot}} Topics in Cloud Computing},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  date = {2010-06-22},
  series = {{{HotCloud}}'10},
  pages = {10},
  publisher = {{USENIX Association}},
  location = {{USA}},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}
