@article{ahmadvandGapproxUsingGallup2019,
  title = {Gapprox: Using {{Gallup}} Approach for Approximation in {{Big Data}} Processing},
  shorttitle = {Gapprox},
  author = {Ahmadvand, Hossein and Goudarzi, Maziar and Foroutan, Fouzhan},
  year = {2019},
  month = feb,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {20},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0185-4},
  urldate = {2023-09-29},
  abstract = {As Big Data processing often takes a long time and needs a lot of resources, sampling and approximate computing techniques may be used to generate a desired Quality of Result. On the other hand, due to not considering data variety, available sample-based approximation approaches suffer from poor accuracy. Data variety is one of the key features of Big Data which causes various parts of data to have different impact on the final result. To address this problem, we develop a data variety aware approximation approach called Gapprox. Our idea is to use a kind of cluster sampling to improve the accuracy of estimation. Our approach can decrease the amount of data to be processed to achieve the desired Quality of Result with acceptable error bound and confidence interval. We divide the input data into some blocks considering the intra/inter cluster variance. The size of the block and the sample size are determined in such a way that by processing small amount of input data, an acceptable confidence interval and error bound is achieved. We compared our work with two well-known state of the art. The experimental results show that our result surpasses the state of the art and improve processing time up to 17\texttimes{} compared to ApproxHadoop and 8\texttimes{} compared to Sapprox when the user can tolerate an error of 5\% with 95\% confidence.},
  keywords = {Approximation,Cluster sampling,Data variety,Quality of Result},
  file = {/Users/dimasws/Zotero/storage/XEUXFUIQ/Ahmadvand et al. - 2019 - Gapprox using Gallup approach for approximation i.pdf;/Users/dimasws/Zotero/storage/J5W3YNWV/s40537-019-0185-4.html}
}

@article{ahmedComprehensivePerformanceAnalysis,
  title = {A {{Comprehensive Performance Analysis}} of {{Apache Hadoop}} and {{Apache Spark}} for {{Large Scale Data Sets Using HiBench}}},
  author = {Ahmed, N and Barczak, Andre L C and Susnjak, Teo and Rashid, Mohammed A},
  abstract = {Big Data analytics for storing, processing, and analyzing large-scale datasets has become an essential tool for the industry. The advent of distributed computing frameworks such as Hadoop and Spark offers efficient solutions to analyze vast amounts of data. Due to the application programming interface (API) availability and its performance, Spark becomes very popular, even more popular than the MapReduce framework. Both these frameworks have more than 150 parameters, and the combination of these parameters has a massive impact on cluster performance. The default system parameters help the system administrator deploy their system applications without much effort, and they can measure their specific cluster performance with factory-set parameters. However, an open question remains: can new parameter selection improve cluster performance for large datasets? In this regard, this study investigates the most impacting parameters, under resource utilization, input splits, and shuffle, to compare the performance between Hadoop and Spark, using an implemented cluster in our laboratory. We used a trial-and-error approach for tuning these parameters based on a large number of experiments. In order to evaluate the frameworks of comparative analysis, we select two workloads: WordCount and TeraSort. The performance metrics are carried out based on three criteria: execution time, throughput, and speedup. Our experimental results revealed that both system performances heavily depends on input data size and correct parameter selection. The analysis of the results shows that Spark has better performance as compared to Hadoop when data sets are small, achieving up to 2 times speedup in WordCount workloads and up to 14 times in TeraSort workloads when default parameter values are reconfigured.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/HVRJVSHR/Ahmed et al. - A Comprehensive Performance Analysis of Apache Had.pdf}
}

@article{ahnPerformanceStudySpark2018,
  title = {Performance {{Study}} of {{Spark}} on {{YARN Cluster Using HiBench}}},
  author = {Ahn, HooYoung and Kim, Hyunjae and You, Woongshik},
  year = {2018},
  journal = {IEEE International Conference on Consumer Electronics},
  abstract = {Recently, various kinds of Internet-of-Things (IoT) solutions and services are provided such as smart industry, smart city, smart factory, smart agriculture and etc. Those solutions and services generate large amount of data from various devices which are connected through networks while they communicate with each other. However, it is a difficult problem to process the fast and massively produced data efficiently. To solve the problems in the framework level, there are many open-source big data processing and analysis frameworks. To process large-scale data in a fast manner, those frameworks use a cluster consisting of multiple computing machines. However, to set the framework running on large-scale cluster properly is not simple and it is difficult to verify its performance in the distributed environment. In this paper, we evaluate the performance of Apache Spark which is one of the most popular big data processing and analysis frameworks. Especially, we conduct experiments by using a representative benchmark tool, called HiBench, and large-scale data in the cluster environment. From the experimental results, we can conclude that Spark is highly scalable for distributed machine learning as well as big data processing.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/LPBF89UW/Ahn et al. - 2018 - Performance Study of Spark on YARN Cluster Using H.pdf}
}

@inproceedings{almansouriHadoopDistributedFile2019,
  title = {Hadoop {{Distributed File System}} for {{Big}} Data Analysis},
  booktitle = {2019 4th {{World Conference}} on {{Complex Systems}} ({{WCCS}})},
  author = {Almansouri, Hatim Talal and Masmoudi, Youssef},
  year = {2019},
  month = apr,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Ouarzazate, Morocco}},
  doi = {10.1109/ICoCS.2019.8930804},
  urldate = {2023-11-08},
  isbn = {978-1-72811-232-9},
  file = {/Users/dimasws/Zotero/storage/WN3AGZUX/Almansouri and Masmoudi - 2019 - Hadoop Distributed File System for Big data analys.pdf}
}

@article{aminudinPengukuranPerformaApache2019,
  title = {{Pengukuran Performa Apache Spark dengan Library H2O Menggunakan Benchmark Hibench Berbasis Cloud Computing}},
  author = {Aminudin, Aminudin and Cahyono, Eko Budi},
  year = {2019},
  month = oct,
  journal = {Jurnal Teknologi Informasi dan Ilmu Komputer},
  volume = {6},
  number = {5},
  pages = {519--526},
  issn = {2528-6579},
  doi = {10.25126/jtiik.2019651520},
  urldate = {2023-09-21},
  abstract = {Apache Spark merupakan platform yang dapat digunakan untuk memproses data dengan ukuran data yang relatif~ besar (big data) dengan kemampuan untuk membagi data tersebut ke masing-masing cluster yang telah ditentukan konsep ini disebut dengan parallel komputing. Apache Spark mempunyai kelebihan dibandingkan dengan framework lain yang serupa misalnya Apache Hadoop dll, di mana Apache Spark mampu memproses data secara streaming artinya data yang masuk ke dalam lingkungan Apache Spark dapat langsung diproses tanpa menunggu data lain terkumpul. Agar di dalam Apache Spark mampu melakukan proses machine learning, maka di dalam paper ini akan dilakukan eksperimen yaitu dengan mengintegrasikan Apache Spark yang bertindak sebagai lingkungan pemrosesan data yang besar dan konsep parallel komputing akan dikombinasikan dengan library H2O yang khusus untuk menangani pemrosesan data menggunakan algoritme machine learning. Berdasarkan hasil pengujian Apache Spark di dalam lingkungan cloud computing, Apache Spark mampu memproses data cuaca yang didapatkan dari arsip data cuaca terbesar yaitu yaitu data NCDC dengan ukuran data sampai dengan 6GB. Data tersebut diproses menggunakan salah satu model machine learning yaitu deep learning dengan membagi beberapa node yang telah terbentuk di lingkungan cloud computing dengan memanfaatkan library H2O. Keberhasilan tersebut dapat dilihat dari parameter pengujian yang telah diujikan meliputi nilai running time, throughput, Avarege Memory dan Average CPU yang didapatkan dari Benchmark Hibench. Semua nilai tersebut ~dipengaruhi oleh banyaknya data dan jumlah node.~AbstractApache Spark is a platform that can be used to process data with relatively large data sizes (big data) with the ability to divide the data into each cluster that has been determined. This concept is called parallel computing. Apache Spark has advantages compared to other similar frameworks such as Apache Hadoop, etc., where Apache Spark is able to process data in streaming, meaning that the data entered into the Apache Spark environment can be directly processed without waiting for other data to be collected. In order for Apache Spark to be able to do machine learning processes, in this paper an experiment will be conducted that integrates Apache Spark which acts as a large data processing environment and the concept of parallel computing will be combined with H2O libraries specifically for handling data processing using machine learning algorithms . Based on the results of testing Apache Spark in a cloud computing environment, Apache Spark is able to process weather data obtained from the largest weather data archive, namely NCDC data with data sizes up to 6GB. The data is processed using one of the machine learning models namely deep learning by dividing several nodes that have been formed in the cloud computing environment by utilizing the H2O library. The success can be seen from the test parameters that have been tested including the value of running time, throughput, Avarege Memory and CPU Average obtained from the Hibench Benchmark. All these values are influenced by the amount of data and number of nodes.},
  copyright = {Hak Cipta (c) 2019 Jurnal Teknologi Informasi dan Ilmu Komputer},
  langid = {indonesian},
  file = {/Users/dimasws/Zotero/storage/WJ5EJWQA/Aminudin and Cahyono - 2019 - Pengukuran Performa Apache Spark dengan Library H2.pdf}
}

@misc{ApacheHadoop,
  title = {Apache {{Hadoop}}},
  urldate = {2023-11-08},
  howpublished = {https://hadoop.apache.org/},
  file = {/Users/dimasws/Zotero/storage/ZKR8J5ML/hadoop.apache.org.html}
}

@article{azhirPerformanceEvaluationQuery2022,
  title = {Performance {{Evaluation}} of {{Query Plan Recommendation}} with {{Apache Hadoop}} and {{Apache Spark}}},
  author = {Azhir, Elham and Hosseinzadeh, Mehdi and Khan, Faheem and Mosavi, Amir},
  year = {2022},
  month = jan,
  journal = {Mathematics},
  volume = {10},
  number = {19},
  pages = {3517},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2227-7390},
  doi = {10.3390/math10193517},
  urldate = {2023-11-04},
  abstract = {Access plan recommendation is a query optimization approach that executes new queries using prior created query execution plans (QEPs). The query optimizer divides the query space into clusters in the mentioned method. However, traditional clustering algorithms take a significant amount of execution time for clustering such large datasets. The MapReduce distributed computing model provides efficient solutions for storing and processing vast quantities of data. Apache Spark and Apache Hadoop frameworks are used in the present investigation to cluster different sizes of query datasets in the MapReduce-based access plan recommendation method. The performance evaluation is performed based on execution time. The results of the experiments demonstrated the effectiveness of parallel query clustering in achieving high scalability. Furthermore, Apache Spark achieved better performance than Apache Hadoop, reaching an average speedup of 2x.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {access plan recommendation,Apache Hadoop,Apache Spark,artificial intelligence,big data,cloud computing,data science,MapReduce,parallel processing,soft computing},
  file = {/Users/dimasws/Zotero/storage/Q5UZ7XLP/Azhir et al. - 2022 - Performance Evaluation of Query Plan Recommendatio.pdf}
}

@article{bakratsasHadoopMapReducePerformance2018,
  title = {Hadoop {{MapReduce Performance}} on {{SSDs}} for {{Analyzing Social Networks}}},
  author = {Bakratsas, M. and Basaras, P. and Katsaros, D. and Tassiulas, L.},
  year = {2018},
  month = mar,
  journal = {Big Data Research},
  volume = {11},
  pages = {1--10},
  issn = {22145796},
  doi = {10.1016/j.bdr.2017.06.001},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/LTEKWTGT/Bakratsas et al. - 2018 - Hadoop MapReduce Performance on SSDs for Analyzing.pdf}
}

@article{bhattacharyaEvaluatingDistributedComputing2021,
  title = {Evaluating {{Distributed Computing Infrastructures}}: {{An Empirical Study Comparing Hadoop Deployments}} on {{Cloud}} and {{Local Systems}}},
  shorttitle = {Evaluating {{Distributed Computing Infrastructures}}},
  author = {Bhattacharya, Devipsita and Currim, Faiz and Ram, Sudha},
  year = {2021},
  month = jul,
  journal = {IEEE Transactions on Cloud Computing},
  volume = {9},
  number = {3},
  pages = {1075--1088},
  issn = {2168-7161},
  doi = {10.1109/TCC.2019.2902377},
  urldate = {2023-10-13},
  abstract = {The popularity of distributed computing platforms (e.g., Hadoop) is largely due to their ability to address scalability issues that arise from data storage and processing limitations of standard computing systems. However, the decision to dedicate organizational resources and capital for such systems needs a careful consideration of several factors including evaluation of cloud-based distributed computing options. We propose a framework of metrics which we used to conduct an in-depth performance and cost benefit analysis of two standard Hadoop infrastructural choices, i.e., a Platform as a Service (PaaS) on-demand cloud setup and a local organizational setup. We evaluated the framework by means of an exploratory data analysis use-case for a large-scale graph processing research problem. Our analysis considered highly granular aspects of distributed computing performance and studied how utilization rates and infrastructure amortization times affect break-even times. We identified that virtual memory management adversely affects the performance of a cloud cluster during the reduce phase with the magnitude of degradation dependent on the type of MapReduce operation. Our study is intended not only as an evaluation of infrastructural choices but also a development of a metric framework that can serve as a baseline for researchers examining distributed infrastructures.},
  file = {/Users/dimasws/Zotero/storage/HJ98ZIFQ/Bhattacharya et al. - 2021 - Evaluating Distributed Computing Infrastructures .pdf;/Users/dimasws/Zotero/storage/HT9ID6T6/8656524.html}
}

@inproceedings{ceesayPlugPlayBench2017,
  title = {Plug and Play Bench: {{Simplifying}} Big Data Benchmarking Using Containers},
  shorttitle = {Plug and Play Bench},
  booktitle = {2017 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Ceesay, Sheriffo and Barker, Adam and Varghese, Blesson},
  year = {2017},
  month = dec,
  pages = {2821--2828},
  publisher = {{IEEE}},
  address = {{Boston, MA}},
  doi = {10.1109/BigData.2017.8258249},
  urldate = {2023-10-27},
  abstract = {The recent boom of big data, coupled with the challenges of its processing and storage gave rise to the development of distributed data processing and storage paradigms like MapReduce, Spark, and NoSQL databases. With the advent of cloud computing, processing and storing such massive datasets on clusters of machines is now feasible with ease. However, there are limited tools and approaches, which users can rely on to gauge and comprehend the performance of their big data applications deployed locally on clusters, or in the cloud. Researchers have started exploring this area by providing benchmarking suites suitable for big data applications. However, many of these tools are fragmented, complex to deploy and manage, and do not provide transparency with respect to the monetary cost of benchmarking an application.},
  isbn = {978-1-5386-2715-0},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/8GQ8E9P2/Ceesay et al. - 2017 - Plug and play bench Simplifying big data benchmar.pdf}
}

@inproceedings{chebbiComparisonBigRemote2018,
  title = {A Comparison of Big Remote Sensing Data Processing with {{Hadoop MapReduce}} and {{Spark}}},
  booktitle = {2018 4th {{International Conference}} on {{Advanced Technologies}} for {{Signal}} and {{Image Processing}} ({{ATSIP}})},
  author = {Chebbi, I. and Boulila, W. and Mellouli, N. and Lamolle, M. and Farah, I.R.},
  year = {2018},
  month = mar,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Sousse}},
  doi = {10.1109/ATSIP.2018.8364497},
  urldate = {2023-11-08},
  isbn = {978-1-5386-5239-8},
  file = {/Users/dimasws/Zotero/storage/DGFLNQLR/Chebbi et al. - 2018 - A comparison of big remote sensing data processing.pdf}
}

@misc{ComprehensivePerformanceAnalysis,
  title = {A Comprehensive Performance Analysis of {{Apache Hadoop}} and {{Apache Spark}} for Large Scale Data Sets Using {{HiBench}} | {{Journal}} of {{Big Data}} | {{Full Text}}},
  urldate = {2023-09-21},
  howpublished = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00388-5\#Sec9},
  keywords = {perbandingan},
  file = {/Users/dimasws/Zotero/storage/JP28856C/A comprehensive performance analysis of Apache Had.pdf;/Users/dimasws/Zotero/storage/GXRZ28AG/s40537-020-00388-5.html}
}

@article{cTaskFailureResilience2020,
  title = {Task Failure Resilience Technique for Improving the Performance of {{MapReduce}} in {{Hadoop}}},
  author = {C, Kavitha and X, Anita},
  year = {2020},
  journal = {ETRI Journal},
  volume = {42},
  number = {5},
  pages = {748--760},
  issn = {2233-7326},
  doi = {10.4218/etrij.2018-0265},
  urldate = {2023-11-08},
  abstract = {MapReduce is a framework that can process huge datasets in parallel and distributed computing environments. However, a single machine failure during the runtime of MapReduce tasks can increase completion time by 50\%. MapReduce handles task failures by restarting the failed task and re-computing all input data from scratch, regardless of how much data had already been processed. To solve this issue, we need the computed key-value pairs to persist in a storage system to avoid re-computing them during the restarting process. In this paper, the task failure resilience (TFR) technique is proposed, which allows the execution of a failed task to continue from the point it was interrupted without having to redo all the work. Amazon ElastiCache for Redis is used as a non-volatile cache for the key-value pairs. We measured the performance of TFR by running different Hadoop benchmarking suites. TFR was implemented using the Hadoop software framework, and the experimental results showed significant performance improvements when compared with the performance of the default Hadoop implementation.},
  copyright = {\textcopyright{} 2020 ETRI},
  langid = {english},
  keywords = {Hadoop,in-memory,key-value pair,MapReduce,recovery,Redis cache,resilience,task failure},
  file = {/Users/dimasws/Zotero/storage/XD6GCPMV/C and X - 2020 - Task failure resilience technique for improving th.pdf}
}

@inproceedings{dabasAnalysisCommentsYoutube2019,
  title = {Analysis of {{Comments}} on {{Youtube Videos}} Using {{Hadoop}}},
  booktitle = {2019 {{Fifth International Conference}} on {{Image Information Processing}} ({{ICIIP}})},
  author = {Dabas, Chetna and Kaur, Parmeet and Gulati, Nimisha and Tilak, Manan},
  year = {2019},
  month = nov,
  pages = {353--358},
  issn = {2640-074X},
  doi = {10.1109/ICIIP47207.2019.8985907},
  urldate = {2023-11-08},
  abstract = {YouTube is a popular video sharing portal of today's time which also allows users to provide their feedback in form of comments, likes and subscriptions. Analysis of this feedback plays a crucial role in enhancement of video content to suit the need of the time. The comments, in specific, can highly benefit a new user in forming an informed and decisive opinion about a particular video. The work presents a system to perform a detailed analysis and classification of the comments on YouTube videos. The comments corresponding to videos have been ingested into the Hadoop Distributed File System (HDFS) and queried with the Hadoop analytical software, Hive. Further, sentiment analysis of these comments has been performed using Python. The proposed system has been evaluated by executing multiple self-designed queries on the YouTube data. Results of execution times of these queries have been tabulated and presented in the form of graphs. Further, results of sentiment analysis of comments that determine the reaction to the video, which are encouraging.},
  file = {/Users/dimasws/Zotero/storage/9KY8F2PC/Dabas et al. - 2019 - Analysis of Comments on Youtube Videos using Hadoo.pdf}
}

@inproceedings{deanMapReduceSimplifiedData2004,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  booktitle = {Proceedings of the 6th Conference on {{Symposium}} on {{Operating Systems Design}} \& {{Implementation}} - {{Volume}} 6},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2004},
  month = dec,
  series = {{{OSDI}}'04},
  pages = {10},
  publisher = {{USENIX Association}},
  address = {{USA}},
  urldate = {2023-09-28},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.}
}

@misc{dhamechaMapReduceFoundationBig2018,
  type = {{{SSRN Scholarly Paper}}},
  title = {{{MapReduce Foundation}} of {{Big Data With Hadoop Environment}}},
  author = {Dhamecha, Maulik and Patalia, Dr Tejas},
  year = {2018},
  month = nov,
  number = {3276536},
  address = {{Rochester, NY}},
  doi = {10.2139/ssrn.3276536},
  urldate = {2023-11-08},
  abstract = {Hadoop Distributed File System that is also known as HDFS and Hadoop MapReduce are good enough to point out about big data's consequences in market. This is very positive hints for data scientists. The opposite hints are that person actually be a digger to be able to find the particular from these results. For more accurate result we have to find more possibilities and produce larger number of pair to find greater accuracy. From our past and because of our great future hand, modules which are open source and also company programmers into the whole market have been created and try that modules to increase the adaptability and use of Hadoop environment.  And that's why for this hadoop environment, the adaptability of this map and reduce functionality is in larger cap or we can say that we can get more accurate result with this map and reduce functionality. In Apache project, many developers ware practicing on signals of the hadoop ecosystem. This constant development in working in sequel of static and incremental leads to handle the hadoop ecosystem ahead in a secure and controlled environment. In this paper we demonstrate all these fundamental criterion with example and prove the basic importance of mapreduce foundation with hadoop environment in big data.},
  langid = {english},
  keywords = {Hadoop,Hadoop Distributed File System (HDFS),Mapreduce},
  file = {/Users/dimasws/Zotero/storage/K4T3CQIP/Dhamecha and Patalia - 2018 - MapReduce Foundation of Big Data With Hadoop Envir.pdf}
}

@article{elkawkagyHighPerformanceHadoop2020,
  title = {High {{Performance Hadoop Distributed File System}}},
  author = {Elkawkagy, Mohamed and Elbeh, Heba},
  year = {2020},
  month = may,
  journal = {International Journal of Networked and Distributed Computing},
  volume = {8},
  number = {3},
  pages = {119--123},
  publisher = {{Atlantis Press}},
  issn = {2211-7946},
  doi = {10.2991/ijndc.k.200515.007},
  urldate = {2023-11-08},
  abstract = {Although by the end of 2020, most of companies will be running 1000 node Hadoop in the system, the Hadoop implementation is still accompanied by many challenges like security, fault tolerance, flexibility. Hadoop is a software paradigm that handles big data, and it has a distributed file systems so-called Hadoop Distributed File System (HDFS). HDFS...},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/NYZ8L3LZ/Elkawkagy and Elbeh - 2020 - High Performance Hadoop Distributed File System.pdf}
}

@article{enesBigDataOrientedPaaS2018,
  title = {Big {{Data-Oriented PaaS Architecture}} with {{Disk-as-a-Resource Capability}} and {{Container-Based Virtualization}}},
  author = {Enes, Jonatan and Cacheiro, Javier L{\'o}pez and Exp{\'o}sito, Roberto R. and Touri{\~n}o, Juan},
  year = {2018},
  month = dec,
  journal = {Journal of Grid Computing},
  volume = {16},
  number = {4},
  pages = {587--605},
  issn = {1570-7873, 1572-9184},
  doi = {10.1007/s10723-018-9460-4},
  urldate = {2023-09-08},
  langid = {english},
  file = {/Users/dimasws/Downloads/enes2018.pdf}
}

@article{gandomiHybSMRPHybridScheduling2019,
  title = {{{HybSMRP}}: A Hybrid Scheduling Algorithm in {{Hadoop MapReduce}} Framework},
  shorttitle = {{{HybSMRP}}},
  author = {Gandomi, Abolfazl and Reshadi, Midia and Movaghar, Ali and Khademzadeh, Ahmad},
  year = {2019},
  month = nov,
  journal = {Journal of Big Data},
  volume = {6},
  number = {1},
  pages = {106},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0253-9},
  urldate = {2023-11-08},
  abstract = {Due to the advent of new technologies, devices, and communication tools such as social networking sites, the amount of data produced by mankind is growing rapidly every year. Big data is a collection of large datasets that cannot be processed using traditional computing techniques. MapReduce has been introduced to solve large-data computational problems. It is specifically designed to run on commodity hardware, and it depends on dividing and conquering principles. Nowadays, the focus of researchers has shifted towards Hadoop MapReduce. One of the most outstanding characteristics of MapReduce is data locality-aware scheduling. Data locality-aware scheduler is a further efficient solution to optimize one or a set of performance metrics such as data locality, energy consumption and job completion time. Similar to all situations, time and scheduling are the most important aspects of the MapReduce framework. Therefore, many scheduling algorithms have been proposed in the past decades. The main ideas of these algorithms are increasing data locality rate and decreasing the response and completion time. In this paper, a new hybrid scheduling algorithm has been proposed, which uses dynamic priority and localization ID techniques and focuses on increasing data locality rate and decreasing completion time. The proposed algorithm was evaluated and compared with Hadoop default schedulers (FIFO, Fair), by running concurrent workloads consisting of Wordcount and Terasort benchmarks. The experimental results show that the proposed algorithm is faster than FIFO and Fair scheduling, achieves higher data locality rate and avoids wasting resources.},
  keywords = {Data Locality,Dynamic priority,Hybrid algorithm,MapReduce,Scheduling},
  file = {/Users/dimasws/Zotero/storage/V4NKM67T/Gandomi et al. - 2019 - HybSMRP a hybrid scheduling algorithm in Hadoop M.pdf}
}

@article{gaoBigDataBenchBigData2013,
  title = {{{BigDataBench}}: A {{Big Data Benchmark Suite}} from {{Web Search Engines}}},
  shorttitle = {{{BigDataBench}}},
  author = {Gao, Wanling and Zhu, Yuqing and Jia, Zhen and Luo, Chunjie and Wang, Lei and Li, Zhiguo and Zhan, Jianfeng and Qi, Yong and He, Yongqiang and Gong, Shiming and Li, Xiaona and Zhang, Shujie and Qiu, Bizhu},
  year = {2013},
  month = jul,
  abstract = {This paper presents our joint research efforts on big data benchmarking with several industrial partners. Considering the complexity, diversity, workload churns, and rapid evolution of big data systems, we take an incremental approach in big data benchmarking. For the first step, we pay attention to search engines, which are the most important domain in Internet services in terms of the number of page views and daily visitors. However, search engine service providers treat data, applications, and web access logs as business confidentiality, which prevents us from building benchmarks. To overcome those difficulties, with several industry partners, we widely investigated the open source solutions in search engines, and obtained the permission of using anonymous Web access logs. Moreover, with two years' great efforts, we created a sematic search engine named ProfSearch (available from http://prof.ict.ac.cn). These efforts pave the path for our big data benchmark suite from search engines---BigDataBench, which is released on the web page (http://prof.ict.ac.cn/BigDataBench). We report our detailed analysis of search engine workloads, and present our benchmarking methodology. An innovative data generation methodology and tool are proposed to generate scalable volumes of big data from a small seed of real data, preserving semantics and locality of data. Also, we preliminarily report two case studies using BigDataBench for both system and architecture researches.},
  file = {/Users/dimasws/Zotero/storage/765S9IN3/Gao et al. - 2013 - BigDataBench a Big Data Benchmark Suite from Web .pdf}
}

@article{gopalaniComparingApacheSpark2015,
  title = {Comparing {{Apache Spark}} and {{Map Reduce}} with {{Performance Analysis}} Using {{K-Means}}},
  author = {Gopalani, Satish and Arora, Rohan},
  year = {2015},
  month = mar,
  journal = {International Journal of Computer Applications},
  volume = {113},
  number = {1},
  pages = {8--11},
  issn = {09758887},
  doi = {10.5120/19788-0531},
  urldate = {2023-10-13},
  abstract = {Data has long been the topic of fascination for Computer Science enthusiasts around the world, and has gained even more prominence in the recent times with the continuous explosion of data resulting from the likes of social media and the quest for tech giants to gain access to deeper analysis of their data. This paper discusses two of the comparison of - Hadoop Map Reduce and the recently introduced Apache Spark - both of which provide a processing model for analyzing big data. Although both of these options are based on the concept of Big Data, their performance varies significantly based on the use case under implementation. This is what makes these two options worthy of analysis with respect to their variability and variety in the dynamic field of Big Data. In this paper we compare these two frameworks along with providing the performance analysis using a standard machine learning algorithm for clustering (K- Means).},
  file = {/Users/dimasws/Zotero/storage/PVDDSTVN/Gopalani and Arora - 2015 - Comparing Apache Spark and Map Reduce with Perform.pdf}
}

@misc{herodotouHadoopPerformanceModels2011,
  title = {Hadoop {{Performance Models}}},
  author = {Herodotou, Herodotos},
  year = {2011},
  month = jun,
  number = {arXiv:1106.0940},
  eprint = {1106.0940},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1106.0940},
  urldate = {2023-11-08},
  abstract = {Hadoop MapReduce is now a popular choice for performing large-scale data analytics. This technical report describes a detailed set of mathematical performance models for describing the execution of a MapReduce job on Hadoop. The models describe dataflow and cost information at the fine granularity of phases within the map and reduce tasks of a job execution. The models can be used to estimate the performance of MapReduce jobs as well as to find the optimal configuration settings to use when running the jobs.},
  archiveprefix = {arxiv},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/Users/dimasws/Zotero/storage/3LCQ7U89/Herodotou - 2011 - Hadoop Performance Models.pdf;/Users/dimasws/Zotero/storage/N924Q9MA/1106.html}
}

@article{huangHiBenchBenchmarkSuite,
  title = {The {{HiBench Benchmark Suite}}: {{Characterization}} of the {{MapReduce-Based Data Analysis}}},
  author = {Huang, Shengsheng and Huang, Jie and Dai, Jinquan and Xie, Tao and Huang, Bo},
  abstract = {The MapReduce model is becoming prominent for the large-scale data analysis in the cloud. In this paper, we present the benchmarking, evaluation and characterization of Hadoop, an open-source implementation of MapReduce. We first introduce HiBench, a new benchmark suite for Hadoop. It consists of a set of Hadoop programs, including both synthetic micro-benchmarks and real-world Hadoop applications. We then evaluate and characterize the Hadoop framework using HiBench, in terms of speed (i.e., job running time), throughput (i.e., the number of tasks completed per minute), HDFS bandwidth, system resource (e.g., CPU, memory and I/O) utilizations, and data access patterns.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/PVZNPQJD/Huang et al. - The HiBench Benchmark Suite Characterization of t.pdf}
}

@article{kaliaAnalysisHadoopMapReduce2021,
  title = {Analysis of Hadoop {{MapReduce}} Scheduling in Heterogeneous Environment},
  author = {Kalia, Khushboo and Gupta, Neeraj},
  year = {2021},
  month = mar,
  journal = {Ain Shams Engineering Journal},
  volume = {12},
  number = {1},
  pages = {1101--1110},
  issn = {20904479},
  doi = {10.1016/j.asej.2020.06.009},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/MFT5BUGG/Kalia and Gupta - 2021 - Analysis of hadoop MapReduce scheduling in heterog.pdf}
}

@article{kimDesignImplementationCloud2022,
  title = {Design and {{Implementation}} of {{Cloud Docker Application Architecture Based}} on {{Machine Learning}} in {{Container Management}} for {{Smart Manufacturing}}},
  author = {Kim, Byoung Soo and Lee, Sang Hyeop and Lee, Ye Rim and Park, Yong Hyun and Jeong, Jongpil},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {13},
  pages = {6737},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app12136737},
  urldate = {2023-09-08},
  abstract = {Manufacturers are expanding their business-process innovation and customized manufacturing to reduce their information technology costs and increase their operational efficiency. Large companies are building enterprise-wide hybrid cloud platforms to further accelerate their digital transformation. Many companies are also introducing container virtualization technology to maximize their cloud transition and cloud benefits. However, small- and mid-sized manufacturers are struggling with their digital transformation owing to technological barriers. Herein, for small- and medium-sized manufacturing enterprises transitioning onto the cloud, we introduce a Docker Container application architecture, a customized container-based defect inspection machine-learning model for the AWS cloud environment developed for use in small manufacturing plants. By linking with open-source software, the development was improved and a datadog-based container monitoring system, built to enable real-time anomaly detection, was implemented.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {cloud docker,container management,docker container,machine learning,monitoring,smart manufacturing},
  file = {/Users/dimasws/Zotero/storage/3U7ES6WF/Kim et al. - 2022 - Design and Implementation of Cloud Docker Applicat.pdf}
}

@misc{KOMPARASIKECEPATANHADOOP,
  title = {{{KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SPARK DALAM MENGOLAH DATA TEKS}} | {{Jurnal Ilmiah Matrik}}},
  urldate = {2023-09-21},
  howpublished = {https://journal.binadarma.ac.id/index.php/jurnalmatrik/article/view/1649},
  file = {/Users/dimasws/Zotero/storage/VRYVYL6K/KOMPARASI KECEPATAN HADOOP MAPREDUCE DAN APACHE SP.pdf;/Users/dimasws/Zotero/storage/IZYFSQ25/1649.html}
}

@article{kozlovRealtimeDataStream,
  title = {Real-Time {{Data Stream Processing System}}},
  author = {Kozlov, Vitalij},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/KUFJH34Q/Kozlov - Real-time Data Stream Processing System.pdf}
}

@inproceedings{linLargescaleDataSet2020,
  title = {A {{Large-scale Data Set}} and an {{Empirical Study}} of {{Docker Images Hosted}} on {{Docker Hub}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Software Maintenance}} and {{Evolution}} ({{ICSME}})},
  author = {Lin, Changyuan and Nadi, Sarah and Khazaei, Hamzeh},
  year = {2020},
  month = sep,
  pages = {371--381},
  publisher = {{IEEE}},
  address = {{Adelaide, SA,  Australia}},
  doi = {10.1109/ICSME46990.2020.00043},
  urldate = {2023-09-13},
  abstract = {Docker is currently one of the most popular containerization solutions. Previous work investigated various characteristics of the Docker ecosystem, but has mainly focused on Dockerfiles from GitHub, limiting the type of questions that can be asked, and did not investigate evolution aspects. In this paper, we create a recent and more comprehensive data set by collecting data from Docker Hub, GitHub, and Bitbucket. Our data set contains information about 3,364,529 Docker images and 378,615 git repositories behind them. Using this data set, we conduct a large-scale empirical study with four research questions where we reproduce previously explored characteristics (e.g., popular languages and base images), investigate new characteristics such as image tagging practices, and study evolution trends. Our results demonstrate the maturity of the Docker ecosystem: we find more reliance on ready-to-use language and application base images as opposed to yet-to-be-configured OS images, a downward trend of Docker image sizes demonstrating the adoption of best practices of keeping images small, and a declining trend in the number of smells in Dockerfiles suggesting a general improvement in quality. On the downside, we find an upward trend in using obsolete OS base images, posing security risks, and find problematic usages of the latest tag, including version lagging. Overall, our results bring good news such as more developers following best practices, but they also indicate the need to build tools and infrastructure embracing new trends and addressing potential issues.},
  isbn = {978-1-72815-619-4},
  langid = {english},
  file = {/Users/dimasws/Downloads/lin2020.pdf}
}

@inproceedings{maneasEvolutionHadoopDistributed2018,
  title = {The {{Evolution}} of the {{Hadoop Distributed File System}}},
  booktitle = {2018 32nd {{International Conference}} on {{Advanced Information Networking}} and {{Applications Workshops}} ({{WAINA}})},
  author = {Maneas, Stathis and Schroeder, Bianca},
  year = {2018},
  month = may,
  pages = {67--74},
  doi = {10.1109/WAINA.2018.00065},
  urldate = {2023-11-08},
  abstract = {Frameworks for large-scale distributed data processing, such as the Hadoop ecosystem, are at the core of the big data revolution we have experienced over the last decade. In this paper, we conduct an extensive study of the Hadoop Distributed File System (HDFS)'s code evolution. Our study is based on the reports and patch files (patches) available from the official Apache issue tracker (JIRA) and our goal was to make complete use of the entire history of HDFS at the time and the richness of the available data. The purpose of our study is to assist developers in improving the design of similar systems and implementing more solid systems in general. In contrast to prior work, our study covers all reports that have been submitted over HDFS's lifetime, rather than a sampled subset. Additionally, we include all associated patch files that have been verified by the developers of the system and classify the root causes of issues at a finer granularity than prior work, by manually inspecting all 3302 reports over the first nine years, based on a two-level classification scheme that we developed. This allows us to present a different perspective of HDFS, including a focus on the system's evolution over time, as well as a detailed analysis of characteristics that have not been previously studied in detail. These include, for example, the scope and complexity of issues in terms of the size of the patch that fixes it and number of files it affects, the time it takes before an issue is exposed, the time it takes to resolve an issue and how these vary over time. Our results indicate that bug reports constitute the most dominant type, having a continuously increasing rate over time. Moreover, the overall scope and complexity of reports and patch files remain surprisingly stable throughout HDFS' lifetime, despite the significant growth the code base experiences over time. Finally, as part of our work, we created a detailed database that includes all reports and patches, along with the key characteristics we extracted.},
  file = {/Users/dimasws/Zotero/storage/GYPNSFB3/Maneas and Schroeder - 2018 - The Evolution of the Hadoop Distributed File Syste.pdf;/Users/dimasws/Zotero/storage/RCADNN6Z/8418050.html}
}

@misc{MapReduceDistributedComputing,
  title = {{{MapReduce}} - {{Distributed Computing}} in {{Java}} 9 [{{Book}}]},
  urldate = {2023-11-08},
  abstract = {MapReduce MapReduce is a programming pattern used by Apache Hadoop. Hadoop MapReduce works in providing the systems that can store, process, and mine huge data with parallel multi node clusters \ldots{} - Selection from Distributed Computing in Java 9 [Book]},
  howpublished = {https://www.oreilly.com/library/view/distributed-computing-in/9781787126992/5fef6ce5-20d7-4d7c-93eb-7e669d48c2b4.xhtml},
  isbn = {9781787126992},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/25BHVL4L/5fef6ce5-20d7-4d7c-93eb-7e669d48c2b4.html}
}

@misc{MapReduceTutorial,
  title = {{{MapReduce Tutorial}}},
  urldate = {2023-11-08},
  howpublished = {https://hadoop.apache.org/docs/r1.2.1/mapred\_tutorial.html},
  file = {/Users/dimasws/Zotero/storage/WKBYLL97/mapred_tutorial.html}
}

@article{meenaReducedTimeCompression2019,
  title = {Reduced {{Time Compression}} in {{Big Data Using MapReduce Approach}} and {{Hadoop}}},
  author = {Meena, K. and Sujatha, J.},
  year = {2019},
  month = jun,
  journal = {Journal of Medical Systems},
  volume = {43},
  number = {8},
  pages = {239},
  issn = {1573-689X},
  doi = {10.1007/s10916-019-1369-3},
  urldate = {2023-11-08},
  abstract = {An exponential rise has been observed in the data volume over the time when considering a real time environment. A phenomenal feature termed as `Predictability' helps in predicting and portraying related data to the user according to their needs. Moreover, classification of Big Data is usually a tedious and lengthy task. The technique of MapReduce Framework performs the data processing that being paralleled by data distribution in small chunks through the clusters. This Map Reduce technique is being proposed which is employed to process heterogeneous data items. Few issues that are being targeted in the existing paper include associating climatologically and meteorological information with large variety of farming decisions. Using the well-known MapReduce framework the above issues and challenges can be resolved. The existing paper proposes empirical techniques of climate classification and prediction by adopting Co-EANFS (Co-Effective and Adaptive Neuro-Fuzzy System) approach for data handling. Furthermore, the paper examines association rule mining too, which is being implemented for examining the best crop production by relying upon the soil and weather condition. Lastly, a technique is proposed for managing various levels such as preprocessing, clustering, classification and prediction. First, the weather dataset is being collected which undergoes processing; thereafter the proposed model is implemented which results in formation of cluster data sets linked to each season. For evaluating the performance, accuracy predictions generated by Co-EANFS is used which being formulated with varying no: of inputs and variables. The proposed framework acquires least execution time.},
  langid = {english},
  keywords = {Association rule mining,Big data,Classification,Clustering,Co-effective and adaptive neuro-fuzzy system (Co-EANFS),Hadoop,MapReduce,Prediction}
}

@inproceedings{moravcikOverviewDockerContainer2020,
  title = {Overview of {{Docker}} Container Orchestration Tools},
  booktitle = {2020 18th {{International Conference}} on {{Emerging eLearning Technologies}} and {{Applications}} ({{ICETA}})},
  author = {Moravcik, Marek and Kontsek, Martin},
  year = {2020},
  month = nov,
  pages = {475--480},
  publisher = {{IEEE}},
  address = {{Ko\v{s}ice, Slovenia}},
  doi = {10.1109/ICETA51985.2020.9379236},
  urldate = {2023-09-13},
  abstract = {The main goal of this paper is to analyze current options of Docker container orchestration. Currently, there are three widely used orchestrators - Docker Swarm, Kubernetes and OpenShift. In the paper, we analyzed all three of them, we highlighted their advantages and disadvantages and compared them.},
  isbn = {978-1-66542-226-0},
  langid = {english},
  file = {/Users/dimasws/Downloads/moravcik2020.pdf}
}

@article{mousaviScalableStreamProcessing,
  title = {Scalable {{Stream Processing}} and {{Management}} for {{Time Series Data}}},
  author = {Mousavi, Bamdad},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6CHBA5HA/Mousavi - Scalable Stream Processing and Management for Time.pdf}
}

@article{muthiahPerformanceEvaluationHadoopa,
  title = {Performance {{Evaluation}} of {{Hadoop}} Based {{Big Data Applications}} with {{HiBench Benchmarking}} Tool on {{IaaS Cloud Platforms}}},
  author = {Muthiah, Karthika},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/XIUJADB4/Muthiah - Performance Evaluation of Hadoop based Big Data Ap.pdf}
}

@article{muthiahPerformanceEvaluationHadoopb,
  title = {Performance {{Evaluation}} of {{Hadoop}} Based {{Big Data Applications}} with {{HiBench Benchmarking}} Tool on {{IaaS Cloud Platforms}}},
  author = {Muthiah, Karthika},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/XAKHHG4D/Muthiah - Performance Evaluation of Hadoop based Big Data Ap.pdf}
}

@inproceedings{naikDockerContainerbasedBig2017a,
  title = {Docker Container-Based Big Data Processing System in Multiple Clouds for Everyone},
  booktitle = {2017 {{IEEE International Systems Engineering Symposium}} ({{ISSE}})},
  author = {Naik, Nitin},
  year = {2017},
  month = oct,
  pages = {1--7},
  publisher = {{IEEE}},
  address = {{Vienna, Austria}},
  doi = {10.1109/SysEng.2017.8088294},
  urldate = {2023-09-08},
  abstract = {Big data processing is progressively becoming essential for everyone to extract the meaningful information from their large volume of data irrespective of types of users and their application areas. Big data processing is a broad term and includes several operations such as the storage, cleaning, organization, modelling, analysis and presentation of data at a scale and efficiency. For ordinary users, the significant challenges are the requirement of the powerful data processing system and its provisioning, installation of complex big data analytics and difficulty in their usage. Docker is a container-based virtualization technology and it has recently introduced Docker Swarm for the development of various types of multi-cloud distributed systems, which can be helpful in solving all above problems for ordinary users. However, Docker is predominantly used in the software development industry, and less focus is given to the data processing aspect of this container-based technology. Therefore, this paper proposes the Docker container-based big data processing system in multiple clouds for everyone, which explores another potential dimension of Docker for big data analysis. This Docker container-based system is an inexpensive and user-friendly framework for everyone who has the knowledge of basic IT skills. Additionally, it can be easily developed on a single machine, multiple machines or multiple clouds. This paper demonstrates the architectural design and simulated development of the proposed Docker container-based big data processing system in multiple clouds. Subsequently, it illustrates the automated provisioning of big data clusters using two popular big data analytics, Hadoop and Pachyderm (without Hadoop) including the Web-based GUI interface Hue for easy data processing in Hadoop.},
  isbn = {978-1-5386-3403-5},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6XLSEFN6/Naik - 2017 - Docker container-based big data processing system .pdf}
}

@misc{ozdilExperimentalComparativeBenchmark2021,
  title = {An {{Experimental}} and {{Comparative Benchmark Study Examining Resource Utilization}} in {{Managed Hadoop Context}}},
  author = {Ozdil, Uluer Emre and Ayvaz, Serkan},
  year = {2021},
  month = dec,
  number = {arXiv:2112.10134},
  eprint = {2112.10134},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-10-27},
  abstract = {Transitioning cloud-based Hadoop from IaaS to PaaS, which are commercially conceptualized as pay-as-you-go or pay-per-use, often reduces the associated system costs. However, managed Hadoop systems do present a black-box behavior to the end-users who cannot be clear on the inner performance dynamics, hence, on the benefits of leveraging them. In the study, we aimed to understand managed Hadoop context in terms of resource utilization. We utilized three experimental Hadoopon-PaaS proposals as they come out-of-the-box and conducted Hadoopspecific workloads of the HiBench Benchmark Suite. During the benchmark executions, we collected system resource utilization data on the worker nodes. The results indicated that the same property specifications among cloud services do not guarantee nearby performance outputs, nor consistent results within themselves. We assume that the managed systems' architectures and pre-configurations play a significant role in the performance.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {{Computer Science - Distributed, Parallel, and Cluster Computing}},
  file = {/Users/dimasws/Zotero/storage/ZGH4JFBT/Ozdil and Ayvaz - 2021 - An Experimental and Comparative Benchmark Study Ex.pdf}
}

@inproceedings{pavloComparisonApproachesLargescale2009,
  title = {A Comparison of Approaches to Large-Scale Data Analysis},
  booktitle = {Proceedings of the 2009 {{ACM SIGMOD International Conference}} on {{Management}} of Data},
  author = {Pavlo, Andrew and Paulson, Erik and Rasin, Alexander and Abadi, Daniel J. and DeWitt, David J. and Madden, Samuel and Stonebraker, Michael},
  year = {2009},
  month = jun,
  series = {{{SIGMOD}} '09},
  pages = {165--178},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1559845.1559865},
  urldate = {2023-11-06},
  abstract = {There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.},
  isbn = {978-1-60558-551-2},
  keywords = {benchmarks,mapreduce,parallel database},
  file = {/Users/dimasws/Zotero/storage/E6VLEXR3/Pavlo et al. - 2009 - A comparison of approaches to large-scale data ana.pdf}
}

@misc{PDFIntegratedHadoop,
  title = {({{PDF}}) {{Integrated Hadoop Cloud Framework}} ({{IHCF}})},
  urldate = {2023-11-08},
  howpublished = {https://www.researchgate.net/publication/316241109\_Integrated\_Hadoop\_Cloud\_Framework\_IHCF},
  file = {/Users/dimasws/Zotero/storage/PH8ER7B8/(PDF) Integrated Hadoop Cloud Framework (IHCF).pdf;/Users/dimasws/Zotero/storage/WDKLRDQ3/316241109_Integrated_Hadoop_Cloud_Framework_IHCF.html}
}

@misc{ResilientDistributedComputing,
  title = {Resilient Distributed Computing Platforms for Big Data Analysis Using {{Spark}} and {{Hadoop}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  urldate = {2023-10-13},
  howpublished = {https://xplorestaging.ieee.org/document/7539859/;jsessionid=PRAnIaN-qIUI9KFpsvdeQ4sQLkwBcXV9bFIxgU2qADV5InHStw-J!-1449844779},
  file = {/Users/dimasws/Zotero/storage/P78XIXB3/;jsessionid=PRAnIaN-qIUI9KFpsvdeQ4sQLkwBcXV9bFIxgU2qADV5InHStw-J!-1449844779.html}
}

@article{ruijunLightweightExperimentalPlatform2020,
  title = {A {{Lightweight Experimental Platform}} for {{Big Data Based}} on {{Docker Containers}}},
  author = {Ruijun, Gu},
  year = {2020},
  month = jan,
  journal = {Journal of Physics: Conference Series},
  volume = {1437},
  number = {1},
  pages = {012104},
  publisher = {{IOP Publishing}},
  issn = {1742-6596},
  doi = {10.1088/1742-6596/1437/1/012104},
  urldate = {2023-09-08},
  abstract = {In recent years, many colleges and universities have set up the major of big data. The biggest problem in teaching is that there is no supporting basic experimental environment, and it is difficult to deploy and configure the big data environment at the same time. In addition, the lack of experimental data, experimental teaching plans and experimental manuals in the experimental process makes it difficult to carry out relevant teaching. In order to reduce the cost of laboratory construction and the difficulty of learning big data, a lightweight big data experimental platform was constructed based on virtualized container technology. Through this platform, we can create a big data cluster, provide various suitable experimental environments, focus on the technology itself, and greatly improve the learning efficiency.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/WLGIH7LQ/Ruijun - 2020 - A Lightweight Experimental Platform for Big Data B.pdf}
}

@inproceedings{samadiComparativeStudyHadoop2016,
  title = {Comparative Study between {{Hadoop}} and {{Spark}} Based on {{Hibench}} Benchmarks},
  booktitle = {2016 2nd {{International Conference}} on {{Cloud Computing Technologies}} and {{Applications}} ({{CloudTech}})},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  year = {2016},
  month = may,
  pages = {267--275},
  publisher = {{IEEE}},
  address = {{Marrakech, Morocco}},
  doi = {10.1109/CloudTech.2016.7847709},
  urldate = {2023-09-24},
  abstract = {Big Data is currently a hot topic for companies and scientists around the world, due to the emergence of new technologies, devices and communication means like social network sites, which led to a noticeable increase of the amount of data produced every year, even every day. In addition, traditional algorithms and technologies are inefficient to process, analyze and store this vast amount of data. So, to solve this problem, Big Data frameworks are needed. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks. Hadoop and Spark, which are used to efficiently process vast amount of data in parallel and distributed mode on a large clusters. Hibench benchmark suite is used to compare the performance of these two frameworks based on the criteria as execution time, throughput and speedup. Our experimental results show that Spark is more efficient than Hadoop to deal with large amount of data. However, spark requires higher memory allocation, since it loads processes into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  isbn = {978-1-4673-8894-8},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/VVTPYL4Z/Samadi et al. - 2016 - Comparative study between Hadoop and Spark based o.pdf}
}

@article{samadiPerformanceComparisonHadoop2018,
  title = {Performance Comparison between {{Hadoop}} and {{Spark}} Frameworks Using {{HiBench}} Benchmarks},
  author = {Samadi, Yassir and Zbakh, Mostapha and Tadonki, Claude},
  year = {2018},
  journal = {Concurrency and Computation: Practice and Experience},
  volume = {30},
  number = {12},
  pages = {e4367},
  issn = {1532-0634},
  doi = {10.1002/cpe.4367},
  urldate = {2023-09-21},
  abstract = {Big Data has become one of the major areas of research for cloud service providers due to a large amount of data produced every day and the inefficiency of traditional algorithms and technologies to handle these large amounts of data. Big Data with its characteristics such as volume, variety, and veracity (3V) requires efficient technologies to process in real time. To solve this problem and to process and analyze this vast amount of data, there are many powerful tools like Hadoop and Spark, which are mainly used in the context of Big Data. They work following the principles of parallel computing. The challenge is to specify which Big Data's tool is better depending on the processing context. In this paper, we present and discuss a performance comparison between two popular Big Data frameworks deployed on virtual machines. Hadoop MapReduce and Apache Spark are used to efficiently process a vast amount of data in parallel and distributed mode on large clusters, and both of them suit for Big Data processing. We also present the execution results of Apache Hadoop in Amazon EC2, a major cloud computing environment. To compare the performance of these two frameworks, we use HiBench benchmark suite, which is an experimental approach for measuring the effectiveness of any computer system. The comparison is made based on three criteria: execution time, throughput, and speedup. We test Wordcount workload with different data sizes for more accurate results. Our experimental results show that the performance of these frameworks varies significantly based on the use case implementation. Furthermore, from our results we draw the conclusion that Spark is more efficient than Hadoop to deal with a large amount of data in major cases. However, Spark requires higher memory allocation, since it loads the data to be processed into memory and keeps them in caches for a while, just like standard databases. So the choice depends on performance level and memory constraints.},
  copyright = {Copyright \textcopyright{} 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {Amazon EC2,Big Data,cloud computing,Hadoop,HiBench,parallel and distributed processing,Spark},
  file = {/Users/dimasws/Zotero/storage/XGWSZMA2/Samadi et al. - 2018 - Performance comparison between Hadoop and Spark fr.pdf;/Users/dimasws/Zotero/storage/J7R8LXTP/cpe.html}
}

@article{saputroPerbandinganKinerjaKomputasi2020,
  title = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}} : {{Storm Event Database}})},
  shorttitle = {Perbandingan {{Kinerja Komputasi Hadoop}} Dan {{Spark}} Untuk {{Memprediksi Cuaca}} ({{Studi Kasus}}},
  author = {Saputro, Rendiyono and Aminuddin, Aminuddin and Munarko, Yuda},
  year = {2020},
  month = mar,
  journal = {Jurnal Repositor},
  volume = {2},
  pages = {463},
  doi = {10.22219/repositor.v2i4.93},
  abstract = {Perkembangan teknologi telah mengakibatkan pertumbuhan data yang semakin cepat dan besar setiap waktunya. Hal tersebut disebabkan oleh banyaknya sumber data seperti mesin pencari, RFID, catatan transaksi digital, arsip video dan foto, user generated content, internet of things, penelitian ilmiah di berbagai bidang seperti genomika, meteorologi, astronomi, fisika, dll. Selain itu, data - data tersebut memiliki karakteristik yang unik antara satu dengan lainnya, hal ini yang menyebabkan tidak dapat diproses oleh teknologi basis data konvensional. Oleh karena itu, dikembangkan beragam framework komputasi terdistribusi seperti Apache Hadoop dan Apache Spark yang memungkinkan untuk memproses data secara terdistribusi dengan menggunakan gugus komputer.Adanya ragam framework komputasi terdistribusi, sehingga diperlukan sebuah pengujian untuk mengetahui kinerja komputasi keduanya. Pengujian dilakukan dengan memproses dataset dengan beragam ukuran dan dalam gugus komputer dengan jumlah node yang berbeda. Dari semua hasil pengujian, Apache Hadoop memerlukan waktu yang lebih sedikit dibandingkan dengan Apache Spark. Hal tersebut terjadi karena nilai throughput dan throughput/node Apache Hadoop lebih tinggi daripada Apache Spark.},
  file = {/Users/dimasws/Zotero/storage/JYJE3NEW/Saputro et al. - 2020 - Perbandingan Kinerja Komputasi Hadoop dan Spark un.pdf}
}

@article{shiClashTitansMapReduce2015,
  title = {Clash of the Titans: {{MapReduce}} vs. {{Spark}} for Large Scale Data Analytics},
  shorttitle = {Clash of the Titans},
  author = {Shi, Juwei and Qiu, Yunjie and Minhas, Umar Farooq and Jiao, Limei and Wang, Chen and Reinwald, Berthold and {\"O}zcan, Fatma},
  year = {2015},
  month = sep,
  journal = {Proceedings of the VLDB Endowment},
  volume = {8},
  number = {13},
  pages = {2110--2121},
  issn = {2150-8097},
  doi = {10.14778/2831360.2831365},
  urldate = {2023-09-28},
  abstract = {MapReduce and Spark are two very popular open source cluster computing frameworks for large scale data analytics. These frameworks hide the complexity of task parallelism and fault-tolerance, by exposing a simple programming API to users. In this paper, we evaluate the major architectural components in MapReduce and Spark frameworks including: shuffle, execution model, and caching, by using a set of important analytic workloads. To conduct a detailed analysis, we developed two profiling tools: (1) We correlate the task execution plan with the resource utilization for both MapReduce and Spark, and visually present this correlation; (2) We provide a break-down of the task execution time for in-depth analysis. Through detailed experiments, we quantify the performance differences between MapReduce and Spark. Furthermore, we attribute these performance differences to different components which are architected differently in the two frameworks. We further expose the source of these performance differences by using a set of micro-benchmark experiments. Overall, our experiments show that Spark is about 2.5x, 5x, and 5x faster than MapReduce, for Word Count, k-means, and PageRank, respectively. The main causes of these speedups are the efficiency of the hash-based aggregation component for combine, as well as reduced CPU and disk overheads due to RDD caching in Spark. An exception to this is the Sort workload, for which MapReduce is 2x faster than Spark. We show that MapReduce's execution model is more efficient for shuffling data than Spark, thus making Sort run faster on MapReduce.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/MALM7QFA/Shi et al. - 2015 - Clash of the titans MapReduce vs. Spark for large.pdf}
}

@misc{simpsonLargeAnnotatedMedical2019,
  title = {A Large Annotated Medical Image Dataset for the Development and Evaluation of Segmentation Algorithms},
  author = {Simpson, Amber L. and Antonelli, Michela and Bakas, Spyridon and Bilello, Michel and Farahani, Keyvan and {van Ginneken}, Bram and {Kopp-Schneider}, Annette and Landman, Bennett A. and Litjens, Geert and Menze, Bjoern and Ronneberger, Olaf and Summers, Ronald M. and Bilic, Patrick and Christ, Patrick F. and Do, Richard K. G. and Gollub, Marc and {Golia-Pernicka}, Jennifer and Heckers, Stephan H. and Jarnagin, William R. and McHugo, Maureen K. and Napel, Sandy and Vorontsov, Eugene and {Maier-Hein}, Lena and Cardoso, M. Jorge},
  year = {2019},
  month = feb,
  number = {arXiv:1902.09063},
  eprint = {1902.09063},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.09063},
  urldate = {2023-11-07},
  abstract = {Semantic segmentation of medical images aims to associate a pixel with a label in a medical image without human initialization. The success of semantic segmentation algorithms is contingent on the availability of high-quality imaging data with corresponding labels provided by experts. We sought to create a large collection of annotated medical image datasets of various clinically relevant anatomies available under open source license to facilitate the development of semantic segmentation algorithms. Such a resource would allow: 1) objective assessment of general-purpose segmentation methods through comprehensive benchmarking and 2) open and free access to medical image data for any researcher interested in the problem domain. Through a multi-institutional effort, we generated a large, curated dataset representative of several highly variable segmentation tasks that was used in a crowd-sourced challenge - the Medical Segmentation Decathlon held during the 2018 Medical Image Computing and Computer Aided Interventions Conference in Granada, Spain. Here, we describe these ten labeled image datasets so that these data may be effectively reused by the research community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/dimasws/Zotero/storage/6EMYBMJA/Simpson et al. - 2019 - A large annotated medical image dataset for the de.pdf;/Users/dimasws/Zotero/storage/Q5XCYJXC/1902.html}
}

@article{singhPerformanceOptimizationMapReducebased2018,
  title = {Performance Optimization of {{MapReduce-based Apriori}} Algorithm on {{Hadoop}} Cluster},
  author = {Singh, Sudhakar and Garg, Rakhi and Mishra, P K},
  year = {2018},
  month = apr,
  journal = {Computers \& Electrical Engineering},
  volume = {67},
  pages = {348--364},
  issn = {00457906},
  doi = {10.1016/j.compeleceng.2017.10.008},
  urldate = {2023-11-08},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/L6NAY2FL/Singh et al. - 2018 - Performance optimization of MapReduce-based Aprior.pdf}
}

@inproceedings{vermaBigDataManagement2016,
  title = {Big Data Management Processing with {{Hadoop MapReduce}} and Spark Technology: {{A}} Comparison},
  shorttitle = {Big Data Management Processing with {{Hadoop MapReduce}} and Spark Technology},
  booktitle = {2016 {{Symposium}} on {{Colossal Data Analysis}} and {{Networking}} ({{CDAN}})},
  author = {Verma, Ankush and Mansuri, Ashik Hussain and Jain, Neelesh},
  year = {2016},
  month = mar,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Indore, Madhya Pradesh, India}},
  doi = {10.1109/CDAN.2016.7570891},
  urldate = {2023-10-13},
  abstract = {Hadoop MapReduce is processed for analysis large volume of data through multiple nodes in parallel. However MapReduce has two function Map and Reduce, large data is stored through HDFS. Lack of facility involve in MapReduce so Spark is designed to run for real time stream data and for fast queries. Spark jobs perform work on Resilient Distributed Datasets and directed acyclic graph execution engine. In this paper, we extend Hadoop MapReduce working and Spark architecture with supporting kind of operation to perform. We also show the differences between Hadoop MapReduce and Spark through Map and Reduce phase individually.},
  isbn = {978-1-5090-0669-4},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/V9G87JKX/Verma et al. - 2016 - Big data management processing with Hadoop MapRedu.pdf}
}

@article{xuDeployingResearchingHadoop,
  title = {Deploying and {{Researching Hadoop}} in {{Virtual Machines}}},
  author = {Xu, Guanghui and Xu, Feng and Ma, Hongxu},
  abstract = {Hadoop's emerging and the maturity of virtualization make it feasible to combine them together to process immense data set. To do research on Hadoop in virtual environment, an experimental environment is needed. This paper firstly introduces some technologies used such as CloudStack, MapReduce and Hadoop. Based on that, a method to deploy CloudStack is given. Then we discuss how to deploy Hadoop in virtual machines which can be obtained from CloudStack by some means, then an algorithm to solve the problem that all the virtual machines which are created by CloudStack using same template have a same hostname. After that we run some Hadoop programs under the virtual cluster, which shows that it is feasible to deploying Hadoop in this way. Then some methods to optimize Hadoop in virtual machines are discussed. From this paper, readers can follow it to set up their own Hadoop experimental environment and capture the current status and trend of optimizing Hadoop in virtual environment.},
  langid = {english},
  file = {/Users/dimasws/Zotero/storage/6EBEWYQQ/Xu et al. - Deploying and Researching Hadoop in Virtual Machin.pdf}
}

@inproceedings{zahariaSparkClusterComputing2010,
  title = {Spark: Cluster Computing with Working Sets},
  shorttitle = {Spark},
  booktitle = {Proceedings of the 2nd {{USENIX}} Conference on {{Hot}} Topics in Cloud Computing},
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J. and Shenker, Scott and Stoica, Ion},
  year = {2010},
  month = jun,
  series = {{{HotCloud}}'10},
  pages = {10},
  publisher = {{USENIX Association}},
  address = {{USA}},
  urldate = {2023-09-28},
  abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.}
}
